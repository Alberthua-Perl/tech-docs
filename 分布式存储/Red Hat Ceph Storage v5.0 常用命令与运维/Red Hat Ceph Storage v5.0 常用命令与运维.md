# ğŸ™ Red Hat Ceph Storage v5.0 å¸¸ç”¨å‘½ä»¤ä¸è¿ç»´

## æ–‡æ¡£è¯´æ˜

- æ­¤æ–‡æ¡£æ¶‰åŠçš„å‘½ä»¤å·²åœ¨ `Red Hat Ceph Storage v5.0` é›†ç¾¤ä¸­è¿›è¡ŒéªŒè¯
- Red Hat Ceph Storage v5.0 å¯¹åº” `Ceph 16.2.0 pacific (stable)` å¼€æºç‰ˆæœ¬
- âœ æ­¤æ–‡æ¡£å°†æŒç»­æ›´æ–°
- ğŸ§ª æ–‡æ¡£ä¸­å¸¦æœ‰ `Lab` æ ‡è®°çš„å†…å®¹å¯è¿›è¡Œå®éªŒéªŒè¯

## æ–‡æ¡£ç›®å½•

- [Ceph é›†ç¾¤çŠ¶æ€](#ceph-é›†ç¾¤çŠ¶æ€)
  - [cephadm å‘½ä»¤](#cephadm-å‘½ä»¤)
  - [Lab: å¯¼å‡ºé›†ç¾¤å…¬é’¥å¹¶æ·»åŠ é›†ç¾¤ä¸»æœº](#lab-å¯¼å‡ºé›†ç¾¤å…¬é’¥å¹¶æ·»åŠ é›†ç¾¤ä¸»æœº)
  - [Ceph Orchestrator ç¼–æ’å™¨ä½¿ç”¨](#ceph-orchestrator-ç¼–æ’å™¨ä½¿ç”¨)
  - [Ceph é›†ç¾¤é…ç½®](#ceph-é›†ç¾¤é…ç½®)
- [Ceph é›†ç¾¤ç½‘ç»œ](#ceph-é›†ç¾¤ç½‘ç»œ)
  - [é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜](#é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜)
  - [Lab: é…ç½® Ceph é›†ç¾¤ç½‘ç»œ](#lab-é…ç½®-ceph-é›†ç¾¤ç½‘ç»œ)
- [Ceph Monitor ç›‘æ§å™¨](#ceph-monitor-ç›‘æ§å™¨)
- [Ceph Manager ç®¡ç†å™¨](#ceph-manager-ç®¡ç†å™¨)
  - [Ceph Mgr å‘½ä»¤](#ceph-mgr-å‘½ä»¤)
  - [Lab: é…ç½®ç®¡ç† Ceph Dashboard](#lab-é…ç½®ç®¡ç†-ceph-dashboard)
  - [Lab: ä¿®æ”¹ Dashboard ä»ªè¡¨æ¿å¯†ç ](#lab-ä¿®æ”¹-dashboard-ä»ªè¡¨æ¿å¯†ç )
  - [Ceph balancer å‡è¡¡å™¨](#ceph-balancer-å‡è¡¡å™¨)
- [Ceph Cluster Map é›†ç¾¤æ˜ å°„](#ceph-cluster-map-é›†ç¾¤æ˜ å°„)
- [Ceph CRUSH Map æ˜ å°„](#ceph-crush-map-æ˜ å°„)
- [Ceph PGï¼ˆPlacement Groupï¼‰æ”¾ç½®ç»„](#ceph-pgplacement-groupæ”¾ç½®ç»„)
  - [Ceph PG å¸¸ç”¨ç®¡ç†å‘½ä»¤](#ceph-pg-å¸¸ç”¨ç®¡ç†å‘½ä»¤)
  - [PG æ•°é‡çš„è®¡ç®—ä¸è§„åˆ’](#pg-æ•°é‡çš„è®¡ç®—ä¸è§„åˆ’)
  - [PG çŠ¶æ€æ±‡æ€»](#pg-çŠ¶æ€æ±‡æ€»)
  - [ä¿®å¤ unknown çŠ¶æ€çš„ PG](#ä¿®å¤-unknown-çŠ¶æ€çš„-pg)
- [Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡](#ceph-osd-å¯¹è±¡å­˜å‚¨è®¾å¤‡)
  - [Lab: å®šä½å¯¹è±¡ä¸ OSDã€PG çš„æ˜ å°„å…³ç³»](#lab-å®šä½å¯¹è±¡ä¸-osdpg-çš„æ˜ å°„å…³ç³»)
  - [Lab: æ ‡è®° OSD åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€](#lab-æ ‡è®°-osd-åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€)
  - [Lab: ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„ OSD è®¾å¤‡ï¼ˆscale upï¼‰](#lab-ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„-osd-è®¾å¤‡scale-up)
  - [Lab: ä½¿ç”¨ osd specification file éƒ¨ç½² OSD è®¾å¤‡](#lab-ä½¿ç”¨-osd-specification-file-éƒ¨ç½²-osd-è®¾å¤‡)
  - [Lab: åˆ é™¤ OSD è®¾å¤‡](#lab-åˆ é™¤-osd-è®¾å¤‡)
  - [Lab: é…ç½® ceph è½¯ä»¶æº](#lab-é…ç½®-ceph-è½¯ä»¶æº)
  - [Lab: RHCS5 ä¸­ä½¿ç”¨ ceph-volume æ‰‹åŠ¨éƒ¨ç½² OSD](#lab-rhcs5-ä¸­ä½¿ç”¨-ceph-volume-æ‰‹åŠ¨éƒ¨ç½²-osd)
- [RADOS å¯¹è±¡æ“ä½œ](#rados-å¯¹è±¡æ“ä½œ)
- [Ceph Pool å­˜å‚¨æ± ](#ceph-pool-å­˜å‚¨æ± )
  - [Ceph å­˜å‚¨æ± çŠ¶æ€](#ceph-å­˜å‚¨æ± çŠ¶æ€)
  - [Ceph å¤åˆ¶æ± å‘½ä»¤](#ceph-å¤åˆ¶æ± å‘½ä»¤)
  - [Ceph çº åˆ ä»£ç æ± å‘½ä»¤](#ceph-çº åˆ ä»£ç æ± å‘½ä»¤)
- [CephX è®¤è¯ä¸ç”¨æˆ·](#cephx-è®¤è¯ä¸ç”¨æˆ·)
  - [CephX è®¤è¯æœºåˆ¶](#cephx-è®¤è¯æœºåˆ¶)
  - [CephX ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤](#cephx-ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤)
- [Ceph RBD é•œåƒ](#ceph-rbd-é•œåƒ)
  - [RBD é•œåƒçš„ç‰¹æ€§ï¼ˆfeatureï¼‰](#rbd-é•œåƒçš„ç‰¹æ€§feature)
  - [ç®¡ç† RBD é•œåƒï¼šåˆ›å»ºã€åˆ é™¤ã€æŸ¥è¯¢ã€æ˜ å°„ã€å¿«ç…§ã€å…‹éš†ã€æ‰å¹³åŒ–](#ç®¡ç†-rbd-é•œåƒåˆ›å»ºåˆ é™¤æŸ¥è¯¢æ˜ å°„å¿«ç…§å…‹éš†æ‰å¹³åŒ–)
  - [Ceph RBD Mirror é›†ç¾¤æ¨¡å¼](#ceph-rbd-mirror-é›†ç¾¤æ¨¡å¼)
  - [Lab: å®ç° RBD one-way mirroring](#lab-å®ç°-rbd-one-way-mirroring)
  - [Lab: rbd-nbd æ˜ å°„ä½¿ç”¨å·² mirroring çš„ RBD é•œåƒ](#lab-rbd-nbd-æ˜ å°„ä½¿ç”¨å·²-mirroring-çš„-rbd-é•œåƒ)
  - [å…¶ä»– RBD Mirror ç›¸å…³å‘½ä»¤](#å…¶ä»–-rbd-mirror-ç›¸å…³å‘½ä»¤)
  - [RBD Mirror çš„æ•…éšœè½¬ç§»](#rbd-mirror-çš„æ•…éšœè½¬ç§»)
  - [Lab: éƒ¨ç½² Ceph iSCSI Gateway ä¸åˆ›å»º target](#lab-éƒ¨ç½²-ceph-iscsi-gateway-ä¸åˆ›å»º-target)
  - [Lab: iscsi-initiator å®¢æˆ·ç«¯ç™»å½•è®¤è¯ iscsi-gateway å®ç°å¤šè·¯å¾„è®¿é—®](#lab-iscsi-initiator-å®¢æˆ·ç«¯ç™»å½•è®¤è¯-iscsi-gateway-å®ç°å¤šè·¯å¾„è®¿é—®)
- [CephFS æ–‡ä»¶ç³»ç»Ÿ](#cephfs-æ–‡ä»¶ç³»ç»Ÿ)
  - [Lab: éƒ¨ç½²å¤šä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿä¸ standby åˆ‡æ¢éªŒè¯](#lab-éƒ¨ç½²å¤šä¸ª-cephfs-æ–‡ä»¶ç³»ç»Ÿä¸-standby-åˆ‡æ¢éªŒè¯)
  - [Lab: åˆ é™¤ CephFS](#lab-åˆ é™¤-cephfs)
  - [Lab: è®¾ç½® CephFS å…·å¤‡å¤šä¸ªæ´»è·ƒçš„ MDS æœåŠ¡å™¨](#lab-è®¾ç½®-cephfs-å…·å¤‡å¤šä¸ªæ´»è·ƒçš„-mds-æœåŠ¡å™¨)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## Ceph é›†ç¾¤çŠ¶æ€

Red Hat Ceph Storage v5.0 ä¸­å·²ä¸å†ä½¿ç”¨æ—§å¼çš„ systemd ç®¡ç†å®ˆæŠ¤è¿›ç¨‹çš„æ–¹å¼ç®¡ç† Ceph è¿›ç¨‹ï¼Œè€Œä½¿ç”¨ `Podman rootfull` å®¹å™¨çš„æ–¹å¼å°è£…è¿è¡Œ Ceph è¿›ç¨‹ï¼Œå› æ­¤ï¼Œå¯ä½¿ç”¨ `cephadm` å‘½ä»¤è¡Œå·¥å…·å–ä»£ä»¥å¾€çš„ `ceph-ansible` æ¥å®ç°é›†ç¾¤çš„éƒ¨ç½²ä¸ç®¡ç†ã€‚

ç®¡ç† Ceph é›†ç¾¤çš„ä¸¤ç§æ–¹å¼ï¼š

- 1ï¸âƒ£ åœ¨ç®¡ç†èŠ‚ç‚¹ä¸Šç›´æ¥ä½¿ç”¨ ceph å‘½ä»¤ã€‚
- 2ï¸âƒ£ è¿›å…¥ç®¡ç†èŠ‚ç‚¹çš„ cephadm shell ä¸´æ—¶å®¹å™¨ä¸­ï¼Œä½¿ç”¨äº¤äº’å¼å‘½ä»¤ç®¡ç†ã€‚

### cephadm å‘½ä»¤

cephadm å·¥å…·ä¸€èˆ¬å®‰è£…äºéƒ¨ç½²èŠ‚ç‚¹æˆ–ç®¡ç†èŠ‚ç‚¹ä¸­ï¼Œå› æ­¤è¿è¡Œä»¥ä¸‹å‘½ä»¤éœ€åœ¨å¯¹åº”èŠ‚ç‚¹ä¸­è¿è¡Œã€‚
  
```bash
$ cephadm --help
$ cephadm version
$ cephadm inspect-image
# æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ Ceph å®¹å™¨é•œåƒä»“åº“ä¸ç‰ˆæœ¬
$ cephadm list-networks
# æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ç½‘æ®µä¿¡æ¯
  
$ cephadm shell -- <command>
$ cephadm shell -- ceph status  #ç¤ºä¾‹
# åœ¨é›†ç¾¤ bootstrap èŠ‚ç‚¹ä½¿ç”¨ cephadm shell ä»¥ç¡®è®¤é›†ç¾¤å¥åº·çŠ¶æ€
# æ­¤å‘½ä»¤è¿è¡Œè¿‡ç¨‹ä¸­å°†å¯åŠ¨ä¸€ä¸ªä¸´æ—¶å®¹å™¨ä»¥è¿è¡ŒæŒ‡å®šçš„å‘½ä»¤
$ cephadm shell --mount /root/<mount_point>
# å¯åŠ¨ä¸´æ—¶å®¹å™¨å¹¶å°†æŒ‡å®šç›®å½•æ˜ å°„è‡³å®¹å™¨ /mnt ç›®å½•ä¸Š
  
$ cephadm bootstrap --config /etc/ceph/ceph.conf
# é›†ç¾¤å¼•å¯¼è¿‡ç¨‹ä¸­æ›´æ”¹é›†ç¾¤é…ç½®æ–‡ä»¶ä»¥ä¼ é€’ä¿®æ”¹çš„é…ç½®ä½¿å…¶ç”Ÿæ•ˆ
  
$ ceph versions
{
    "mon": {
        "ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)": 4
    },
    "mgr": {
        "ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)": 4
    },
    "osd": {
        "ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)": 6,
        "ceph version 16.2.15 (618f440892089921c3e944a991122ddc44e60516) pacific (stable)": 3
    },
    "mds": {},
    "rgw": {
        "ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)": 2
    },
    "overall": {
        "ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)": 16,
        "ceph version 16.2.15 (618f440892089921c3e944a991122ddc44e60516) pacific (stable)": 3
    }
}
# æŸ¥çœ‹é›†ç¾¤æ‰€æœ‰æœåŠ¡ç»„ä»¶çš„ç‰ˆæœ¬

$ ceph tell osd.* version
osd.0: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
osd.1: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
...
# æŸ¥çœ‹æ‰€æœ‰ osd çš„ç‰ˆæœ¬

$ ceph tell mon.* version
mon.serverc.lab.example.com: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
mon.serverd: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
mon.clienta: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
mon.servere: {
    "version": "16.2.0-117.el8cp",
    "release": "pacific",
    "release_type": "stable"
}
# æŸ¥çœ‹æ‰€æœ‰ mon çš„ç‰ˆæœ¬
```
  
![cephadm-demo](images/cephadm-demo.png)
  
![cephadm-shell-demo](images/cephadm-shell-demo.png)

### Lab: å¯¼å‡ºé›†ç¾¤å…¬é’¥å¹¶æ·»åŠ é›†ç¾¤ä¸»æœº

âœ¨ æ³¨æ„ï¼šä½¿ç”¨ ceph orch host add æ·»åŠ é¢å¤–çš„èŠ‚ç‚¹æ—¶ï¼Œéœ€å…ˆå°†é›†ç¾¤å…¬é’¥å¯¼å‡ºå¹¶åŒæ­¥è‡³èŠ‚ç‚¹ã€‚

```bash
$ ceph cephadm get-pub-key > /path/to/ceph.pub
# ç”Ÿæˆé›†ç¾¤å…¬é’¥
# ä½¿ç”¨åœºæ™¯ï¼šä½¿ç”¨ cephadm æ·»åŠ ä¸»æœºéœ€ä½¿ç”¨é›†ç¾¤å…¬é’¥è¿›è¡Œ SSH ç™»å½•è®¤è¯
$ ssh-copy-id -f -i /path/to/ceph.pub root@<host_fqdn>
# å°†é›†ç¾¤å…¬é’¥åŒæ­¥è‡³èŠ‚ç‚¹ï¼Œå³å¯æ·»åŠ æ­¤èŠ‚ç‚¹ã€‚
$ ceph orch host add <host_fqdn>
# æ·»åŠ èŠ‚ç‚¹ä¸ºé›†ç¾¤èŠ‚ç‚¹
$ ceph orch host rm <host_fqdn>
# åˆ é™¤ç¼–æ’å™¨ç®¡ç†çš„é›†ç¾¤èŠ‚ç‚¹
```

### Ceph Orchestrator ç¼–æ’å™¨ä½¿ç”¨
  
```bash
$ ceph orch status
  Backend: cephadm
  Available: Yes
  Paused: No
# æŸ¥çœ‹é›†ç¾¤æ•´ä½“çŠ¶æ€
  
$ ceph orch ls [--service_type=<name>]
  NAME                     RUNNING  REFRESHED  AGE  PLACEMENT                                                                
  alertmanager                 1/1  9m ago     2y   count:1                                                                  
  crash                        4/4  9m ago     2y   *                                                                        
  grafana                      1/1  9m ago     2y   count:1                                                                  
  mgr                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
  mon                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
  node-exporter                4/4  9m ago     2y   *                                                                        
  osd.default_drive_group     9/12  9m ago     2y   server*                                                                  
  prometheus                   1/1  9m ago     2y   count:1                                                                  
  rgw.realm.zone               2/2  9m ago     23M  serverc.lab.example.com;serverd.lab.example.com
# æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡ï¼ˆserviceï¼‰çŠ¶æ€
# ä½¿ç”¨ --service_type é€‰é¡¹æŒ‡å®š Ceph æœåŠ¡ç±»å‹
$ ceph orch ls --service_type=mon
  NAME  RUNNING  REFRESHED  AGE  PLACEMENT                                                                                        
  mon       4/4  3m ago     2y   clienta.lab.example.com;serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com
# æŸ¥çœ‹ mon æœåŠ¡çš„çŠ¶æ€

$ ceph orch restart osd.default_drive_group
# é‡å¯åç§°ä¸º osd.default_drive_group çš„æ‰€æœ‰ OSD å®ˆæŠ¤è¿›ç¨‹
  
$ ceph orch ps [--daemon_type=<name>]
  NAME                         HOST                     STATUS         REFRESHED  AGE  PORTS  VERSION           IMAGE ID      CONTAINER ID  
  mon.clienta                  clienta.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  6c85145c2b22  
  mon.serverc.lab.example.com  serverc.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  f93b366a9f74  
  mon.serverd                  serverd.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  d35695b5833e  
  mon.servere                  servere.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  97319d05259c
# æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡å®ä¾‹çŠ¶æ€
# ä½¿ç”¨ --daemon_type é€‰é¡¹å¯æŒ‡å®šæœåŠ¡å®ä¾‹çš„åç§°è¿›è¡Œè¿‡æ»¤
$ ceph orch ps <host_fqdn>
# æŸ¥çœ‹æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœåŠ¡å®ä¾‹çŠ¶æ€
  
$ ceph orch host ls
  HOST                     ADDR           LABELS  STATUS  
  clienta.lab.example.com  172.25.250.10  _admin          
  serverc.lab.example.com  172.25.250.12  _admin          
  serverd.lab.example.com  172.25.250.13                  
  servere.lab.example.com  172.25.250.14
# ä½¿ç”¨ç¼–æ’å™¨æŸ¥çœ‹å„èŠ‚ç‚¹æ¦‚è¦ä¸ label æ ‡ç­¾
# æ³¨æ„ï¼šlabel æ ‡ç­¾çš„ä½œç”¨åœ¨äºåŒºåˆ†å„ä¸ªèŠ‚ç‚¹ï¼Œå¯¹èŠ‚ç‚¹è¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œå¹¶ä¸”ä¸€ä¸ªèŠ‚ç‚¹å¯å…·æœ‰å¤šä¸ªæ ‡ç­¾ã€‚
$ ceph orch host label add <node> _admin
# ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  admin ç®¡ç†å‘˜ label æ ‡ç­¾
$ ceph orch host label add <node> prometheus
# ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  prometheus æ ‡ç­¾
$ ceph orch apply prometheus --placement="label:prometheus"
# æ ¹æ®é›†ç¾¤èŠ‚ç‚¹æ ‡ç­¾éƒ¨ç½² prometheus
  
$ ceph orch device ls [--wide]
# æŸ¥çœ‹é›†ç¾¤å„èŠ‚ç‚¹å¯ç”¨ä¸ä¸å¯ç”¨çš„è®¾å¤‡ä¿¡æ¯ï¼Œ--wide é€‰é¡¹å°†æŒ‡å®šå…¨éƒ¨çš„è¯¦ç»†è®¾å¤‡ä¿¡æ¯ã€‚
  
$ sudo systemctl list-units --all "ceph*"
# ç™»å½• Ceph é›†ç¾¤èŠ‚ç‚¹æŸ¥çœ‹æ‰€æœ‰çš„ ceph æœåŠ¡å•å…ƒ
  
$ ceph orch host maintenance enter <fqdn> [--force]
# å°†æŒ‡å®šèŠ‚ç‚¹è®¾ç½®ä¸ºç»´æŠ¤æ¨¡å¼
$ ceph orch host maintenance exit <fqdn>
# å°†æŒ‡å®šèŠ‚ç‚¹é€€å‡ºç»´æŠ¤æ¨¡å¼
```

### Ceph é›†ç¾¤é…ç½®
  
æ¯ä¸ª `ceph monitor` èŠ‚ç‚¹ç®¡ç†ä¸€ä¸ªé›†ä¸­å¼é…ç½®æ•°æ®åº“ï¼Œä½äº `/var/lib/ceph/$fsid/mon.$host/store.db/` ä¸­ã€‚åœ¨é›†ç¾¤å¯åŠ¨æ—¶ï¼ŒCeph å®ˆæŠ¤è¿›ç¨‹è§£æç”±å‘½ä»¤è¡Œé€‰é¡¹ã€ç¯å¢ƒå˜é‡ä¸æœ¬åœ°é›†ç¾¤é…ç½®çš„é…ç½®é€‰é¡¹ã€‚Ceph å®ˆæŠ¤è¿›ç¨‹è¿æ¥åˆ°é›†ç¾¤ä»¥è·å–å­˜å‚¨åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®è®¾å®šã€‚ä» RHCS 4 å¼€å§‹å¼ƒç”¨ `/etc/ceph/ceph.conf` é›†ç¾¤é…ç½®æ–‡ä»¶ï¼Œè€Œå°†é›†ä¸­å¼é…ç½®æ•°æ®åº“ä½œä¸ºé…ç½®å­˜å‚¨çš„é¦–é€‰æ–¹å¼ã€‚`ceph config set` å‘½ä»¤å¯ç”¨äºæ›´æ”¹é›†ç¾¤å„ç±»é…ç½®ã€‚
  
```bash
$ sudo systemctl list-units "ceph*"
# æŸ¥çœ‹ ceph èŠ‚ç‚¹ä¸Šçš„ ceph å•å…ƒæ–‡ä»¶
$ sudo journalctl -uef ceph-<cluster_id>@<service_name>.service
# æŸ¥çœ‹æŒ‡å®š ceph æœåŠ¡å•å…ƒæ–‡ä»¶çš„å®æ—¶æ—¥å¿—

$ ceph health detail
  HEALTH_OK
$ ceph -s
$ ceph status [-f json-pretty]
$ ceph -w
# å®æ—¶åˆ·æ–°æ—¥å¿—
```
  
```bash
$ ceph config ls
# åˆ—å‡ºé›†ç¾¤æ‰€æœ‰çš„å¯é…ç½®å‚æ•°ï¼ˆRHCS5 åŒ…å« 2026 ä¸ªé…ç½®å‚æ•°ï¼‰
$ ceph config help <config_setting>
# æŸ¥çœ‹æŒ‡å®šé…ç½®å‚æ•°çš„å…·ä½“ä¿¡æ¯
$ ceph config dump
# æŸ¥çœ‹é›†ä¸­å¼é…ç½®æ•°æ®åº“çš„é…ç½®ï¼ˆåŒºåˆ«äºé›†ç¾¤é…ç½®æ–‡ä»¶ï¼‰
$ ceph config show $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é…ç½®
$ ceph config show-with-defaults $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é»˜è®¤é…ç½®
  
$ ceph config get $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
$ ceph config set $type.$id <config_setting>
# è®¾ç½®ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
$ ceph config set mon.serverd mon_max_pool_pg_num 65536  #ç¤ºä¾‹
# è®¾ç½® mon.serverd çš„ mon_max_pool_pg_num ä¸º 65536
  
$ ceph config assimilate-conf -i /path/to/config_file
# å°†æŒ‡å®šé…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°èå…¥å½“å‰çš„é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­
  
$ ceph config get mon mon_allow_pool_delete
# æŸ¥çœ‹é›†ç¾¤æ˜¯å¦å…è®¸åˆ é™¤å­˜å‚¨æ± 
$ ceph config get mon mon_compact_on_start
  false
# æŸ¥çœ‹ mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­æ˜¯å¦å¯æ”¶ç¼©æ•°æ®åº“ï¼Œä»¥æé«˜æ•°æ®åº“æ€§èƒ½ã€‚
$ ceph config set mon mon_compact_on_start true
# è®¾ç½® mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®åº“å‹ç¼©
  
$ ceph config get mon auth_service_required
$ ceph config get mon auth_cluster_required
$ ceph config get mon auth_client_required
# æŸ¥çœ‹é›†ç¾¤çš„å„è®¤è¯æ–¹å¼
  
### é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®è®¾ç½® ###
$ ceph tell $type.$id config get <config_setting>
$ ceph tell mon.serverc config get mon_max_pool_pg_num  #ç¤ºä¾‹
# æŸ¥çœ‹ serverc æ§åˆ¶èŠ‚ç‚¹ä¸Šæ”¯æŒçš„æ¯ä¸ªå­˜å‚¨æ± å¯åŒ…å«çš„æœ€å¤§ pg æ•°
# æ³¨æ„ï¼š
#   1. ä»¥ä¸Šå‘½ä»¤å¯åœ¨é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®ï¼Œæ­¤æ–¹å¼ä½œä¸ºä¸´æ—¶é…ç½®ï¼Œå½“å®ˆæŠ¤è¿›ç¨‹é‡å¯åå°†å¤±æ•ˆã€‚
#   2. ceph tell å­å‘½ä»¤ä¾ç„¶ä½¿ç”¨ monã€‚
```

## Ceph é›†ç¾¤ç½‘ç»œ

### é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜

- `public_network`ï¼šå…¬å…±ç½‘ç»œï¼Œä¸»è¦ç”¨äºå®¢æˆ·ç«¯ä¸ Monã€Mgrã€Mdsã€osd ä»¥åŠå…¶ä»–ç›¸å…³ç»„ä»¶ä¹‹é—´çš„æ•°æ®è¯·æ±‚ä¸äº¤æ¢ã€‚
- `cluster_network`ï¼šé›†ç¾¤ç½‘ç»œï¼Œä¸»è¦ç”¨äº osd ä¹‹é—´çš„æ•°æ®åŒæ­¥ã€æ•°æ®é‡å¹³è¡¡ã€æ•°æ®æ¢å¤ã€æ•°æ®å›å¡«ã€å¿ƒè·³æ£€æµ‹ç­‰ã€‚
  
<center><img src="images/ceph-arch-network.png" style="width:60%"></center>

<center><img src="images/osd-network-community.jpg" style="width:60%"></center>
  
<center>Ceph OSD èŠ‚ç‚¹çš„ç½‘ç»œè¿æ¥ç¤ºæ„</center>

- æ¯ä¸ª OSD ä½¿ç”¨ä¸€ä¸ªç«¯å£é€šè¿‡ public ç½‘ç»œä¸å®¢æˆ·ç«¯åŠ Mon é€šä¿¡
- ä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œä¸å…¶ä»– OSD é—´ä¼ è¾“æ•°æ®
- ä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œäº¤æ¢ heatbeat å¿ƒè·³åŒ…ï¼‰

<center><img src="images/default-ports-rhcs5.jpg" style="width:60%"></center>

<center>RHCS5 ä¸­çš„é»˜è®¤ç«¯å£èŒƒå›´</center>

### Lab: é…ç½® Ceph é›†ç¾¤ç½‘ç»œ

Ceph é›†ç¾¤ç½‘ç»œé…ç½®çš„ä¸¤ç§æ–¹å¼ï¼š

```bash
### æ–¹å¼1ï¼šé…ç½®æ–‡ä»¶æ–¹å¼
$ vim ./cluster-public-network.conf
[osd]  #é…ç½® osd ä½¿ç”¨çš„ä¸¤å¤§ç±»ç½‘ç»œï¼Œä¹Ÿå¯å®šä¹‰ mon ä½¿ç”¨çš„ç½‘ç»œã€‚
  public network = 172.25.250.0/24
  cluster network = 172.25.249.0/24

$ ceph config assimilate-conf -i ./cluster-public-network.conf
$ ceph config get osd public_network
$ ceph config get osd cluster_network
$ ceph orch restart osd  #é‡å¯æ‰€æœ‰ osd æœåŠ¡ç¡®ä¿é…ç½®ç”Ÿæ•ˆ

### æ–¹å¼2ï¼šå‘½ä»¤è¡Œé…ç½®é›†ä¸­å¼æ•°æ®åº“
$ ceph config set mon cluster_network 172.25.249.0/24
$ ceph config get mon cluster_network
$ ceph orch restart mon  #é‡å¯æ‰€æœ‰ mon æœåŠ¡ç¡®ä¿é…ç½®ç”Ÿæ•ˆ
```

## Ceph Monitor ç›‘æ§å™¨

```bash
$ ceph mon stat
  e5: 3 mons at {serverc.lab.example.com=[v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0],
  serverd=[v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0],servere=[v2:172.25.250.14:3300/0,
  v1:172.25.250.14:6789/0]}, election epoch 42, leader 0 serverc.lab.example.com, 
  quorum 0,1,2 serverc.lab.example.com,serverd,servere
# æŸ¥çœ‹ mon çŠ¶æ€ï¼ŒåŒ…æ‹¬ mon çš„ IPv4 åœ°å€ä¸ leader monã€‚

$ ceph quorum_status -f json-pretty
{
  "election_epoch": 42,
  "quorum": [
    0,
    1,
    2
  ],
  "quorum_names": [
    "serverc.lab.example.com",
    "serverd",
    "servere"
  ],
  "quorum_leader_name": "serverc.lab.example.com",
  ...
# æŸ¥çœ‹ mon çš„é€‰ä¸¾çŠ¶æ€
```

## Ceph Manager ç®¡ç†å™¨

### Ceph Mgr å‘½ä»¤

```bash
$ ceph mgr stat
{
    "epoch": 19,
    "available": true,
    "active_name": "serverc.lab.example.com.weqbtr",
    "num_standby": 2
}
# ç¡®è®¤ mgr çš„æ•´ä½“çŠ¶æ€

$ ceph mgr services
{
    "dashboard": "https://172.25.250.12:8443/",
    "prometheus": "http://172.25.250.12:9283/"
}
# æŸ¥çœ‹ mgr ç®¡ç†çš„æ¨¡å—å‘å¸ƒåœ°å€

$ ceph mgr module ls
{
    "always_on_modules": [
        "balancer",
        "crash",
        "devicehealth",
        "orchestrator",
        "pg_autoscaler",
        "progress",
        "rbd_support",
        "status",
        "telemetry",
        "volumes"
    ],
    ...
}
# æŸ¥çœ‹ mgr ç®¡ç†çš„æ¨¡å—è¯¦ç»†ä¿¡æ¯
$ ceph mgr module enable pg_autoscaler
# å¯ç”¨ pg_autoscaler æ¨¡å—

$ ceph config get mgr mgr/cephadm/warn_on_stray_daemons
# æŸ¥çœ‹ cephadm æ¨¡å— warn_on_stray_daemons å‚æ•°çš„è®¾ç½®ï¼Œæ­¤å‚æ•°ç”¨äºå¯ç”¨ ceph ç¼–æ’å™¨ç®¡ç†çš„å®ˆæŠ¤è¿›ç¨‹çš„å‘Šè­¦ã€‚
$ ceph config set mgr mgr/cephadm/warn_on_stray_daemons false
# å…³é—­ ceph ç¼–æ’å™¨ç®¡ç†çš„å®ˆæŠ¤è¿›ç¨‹çš„å‘Šè­¦
```

### Lab: é…ç½®ç®¡ç† Ceph Dashboard

```bash
$ ceph mgr module enable dashboard --force
# å¯ç”¨ dashboard æ¨¡å—
$ ceph mgr module ls | \
  perl -MJSON -MYAML -0777 -wnl -e 'print YAML::Dump(decode_json($_))' -
# æŸ¥çœ‹ mgr ç®¡ç†çš„æ¨¡å—å¹¶ä½¿ç”¨ perl å°†è¾“å‡ºçš„ JSON æ ¼å¼è½¬æ¢ä¸º YAML æ ¼å¼
# æ³¨æ„ï¼šä½¿ç”¨ perl æ—¶éœ€æå‰å®‰è£… perl-JSON ä¸ perl-YAML è½¯ä»¶åŒ…ä»¥æä¾›å¯¹åº”æ¨¡å—
$ ceph dashboard create-self-signed-cert
# é…ç½® dashboard ç”Ÿæˆè¯ä¹¦
$ ceph config set mgr mgr/dashboard/server_addr <dashboard_server_address>
# é…ç½® dashboard çš„ä¸»æœº IP åœ°å€
$ ceph config set mgr mgr/dashboard/server_port 8080
# é…ç½® dashboard ç›‘å¬çš„é SSL ç«¯å£
$ ceph config set mgr mgr/dashboard/ssl_server_port 8443
# é…ç½® dashboard ç›‘å¬çš„ SSL ç«¯å£
$ ceph mgr services
{
    "dashboard": "https://172.25.250.12:8443/",
    "prometheus": "http://172.25.250.12:9283/"
}
# è·å– mgr ç®¡ç†çš„ dashboard URL
$ ceph dashboard ac-user-create \
<dashboard_user> <dashboard_password> administrator
# åˆ›å»ºç™»å½• dashboard çš„ç”¨æˆ·åä¸å¯†ç ï¼Œä»¥ç®¡ç†å‘˜ç”¨æˆ·èº«ä»½ç™»å½•ã€‚
```

### Lab: ä¿®æ”¹ Dashboard ä»ªè¡¨æ¿å¯†ç 

```bash
$ echo "<password>" > /path/to/dashboard_password.yml
# æ³¨å…¥ä¿®æ”¹çš„æ–°å¯†ç 
$ ceph dashboard ac-user-set-password <dashboard_user> -i /path/to/dashboard_password.yml 
# è®¾ç½®æŒ‡å®šç”¨æˆ·ä¸å¯†ç 
```

### Ceph balancer å‡è¡¡å™¨

```bash
$ ceph balancer status
{
    "active": true,
    "last_optimize_duration": "0:00:00.000462",
    "last_optimize_started": "Thu Jan 11 05:51:41 2024",
    "mode": "upmap",
    "optimize_result": "Unable to find further optimization, or pool(s) pg_num is decreasing, or distribution is already perfect",
    "plans": []
} 
# æŸ¥çœ‹ balancer å‡è¡¡å™¨çŠ¶æ€
```

## Ceph Cluster Map é›†ç¾¤æ˜ å°„

Ceph Cluster Map åŒ…å« `MON Map`ã€`OSD Map`ã€`PG Map`ã€`MDS Map` å’Œ `CRUSH Map`ã€‚

```bash
$ ceph mon dump
  epoch 55
  fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
  last_changed 2023-09-17T23:46:59.748723+0000
  created 2021-10-01T09:30:30.146231+0000
  min_mon_release 16 (pacific)
  election_strategy: 1
  0: [v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0] mon.serverc.lab.example.com
  1: [v2:172.25.250.14:3300/0,v1:172.25.250.14:6789/0] mon.servere
  2: [v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0] mon.serverd
  dumped monmap epoch 55
# æŸ¥çœ‹é›†ç¾¤ monmap
$ ceph osd dump
# æŸ¥çœ‹é›†ç¾¤ osdmap
$ ceph pg dump
# æŸ¥çœ‹é›†ç¾¤ pgmap
$ ceph pg dump pgs_brief
# æŸ¥çœ‹é›†ç¾¤ç®€è¦ pgmap
$ ceph osd crush dump
# æŸ¥çœ‹é›†ç¾¤ crush mapï¼ˆå¯æŸ¥çœ‹é›†ç¾¤æ•´ä½“çš„ CRUSH å±‚æ¬¡ç»“æ„ä¸æ•…éšœæ¢å¤åŸŸï¼‰
$ ceph fs dump
# æŸ¥çœ‹é›†ç¾¤ cephfs map
```

## Ceph CRUSH Map æ˜ å°„

ğŸš€ å…³äº `CRUSH map` çš„è¯´æ˜å¯å‚è€ƒ [Ceph CRUSH map æ¦‚è¿°ä¸å®ç°](https://github.com/Alberthua-Perl/tech-docs/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph%20CRUSH%20map%20%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0.md#-ceph-crush-map-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0)

## Ceph PGï¼ˆPlacement Groupï¼‰æ”¾ç½®ç»„

### Ceph PG å¸¸ç”¨ç®¡ç†å‘½ä»¤

Ceph PGã€CRUSH æ”¾ç½®è§„åˆ™ä¸ OSD ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<center><img src="images/ceph-pg-crush-osd-mapping.png" style="width:80%"></center>

```bash
$ ceph pg stat
  105 pgs: 105 active+clean; 4.9 KiB data, 181 MiB used, 90 GiB / 90 GiB avail
# æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰ pg çš„çŠ¶æ€

$ ceph pg dump [all|summary|sum|delta|pools|osds|pgs|pgs_brief...]
# show human-readable versions of pg map (only 'all' valid with plain)
$ ceph pg dump --format json | jq .
# ä»¥ JSON æ ¼å¼æŸ¥çœ‹ pg map
$ ceph pg dump_json [all|summary|sum|pools|osds|pgs...]
# show human-readable version of pg map in json only
$ pg dump_pools_json
# show pg pools info in json only
$ ceph pg dump_stuck [inactive|unclean|stale|undersized|degraded...] [<threshold:int>]
# show information about stuck pgs

$ ceph pg map <pg.id>
# ğŸ©º æ•…éšœæ’é™¤ï¼šæ ¹æ® pg id æŸ¥æ‰¾æŒ‡å®š pg ä¸ osd çš„å¯¹åº”å…³ç³»
$ ceph pg map 9.1b  #ç¤ºä¾‹
  osdmap e325 pg 9.1b (9.1b) -> up [8,1,0] acting [8,1,0]
$ ceph pg <pg.id> query
# ğŸ©º æ•…éšœæ’é™¤ï¼šæ ¹æ® pg id æŸ¥çœ‹å…¶è¯¦ç»†çŠ¶æ€ä¿¡æ¯
```

### PG æ•°é‡çš„è®¡ç®—ä¸è§„åˆ’

å¢åŠ  PG çš„æ•°é‡å¯ä»¥ä½¿è´Ÿè½½æ›´åŠ å‡åŒ€åœ°åˆ†æ•£åˆ°é›†ç¾¤ä¸­çš„ OSDã€‚ä¸è¿‡ï¼Œå¦‚æœè®¾ç½®çš„ PG æ•°é‡è¿‡é«˜ï¼Œå®ƒä¼šå¤§å¹…å¢åŠ  CPU å’Œå†…å­˜ä½¿ç”¨é‡ã€‚RedHat å»ºè®®æ¯ä¸ª OSD å¤§çº¦ 100 åˆ° 200 ä¸ª PGã€‚

è®¡ç®—é›†ç¾¤ä¸­ PG çš„æ•°é‡ï¼š

$$
  \begin{align*}
    \text{Total PGs} = \frac{\text{Total number of OSDs * 100}}{\text{Number of replicas}}
  \end{align*}
$$

å…¶ä¸­ï¼ŒOSDs ä¸ºé›†ç¾¤ä¸­çš„ OSD è®¾å¤‡æ€»æ•°ï¼Œ100 ä¸ºæ¯ä¸ª OSD ä¸­çš„ PG æ•°ï¼Œ$Number\;of\;replicas$ ä¸ºå¯¹è±¡å‰¯æœ¬çš„æ•°é‡ã€‚

ğŸ’¥ ä½†æ¯ä¸ªå­˜å‚¨æ± ä¸­ PG æ•°é‡æœ€å¥½æ¥è¿‘æˆ–ç­‰äº $2^n$

ä¾‹å¦‚ï¼šæœ‰ 100 ä¸ª OSDï¼Œ2 å‰¯æœ¬ï¼Œ5 ä¸ª poolï¼Œåˆ™

$$\begin{align*}
  \text{Total PGs} = \frac{100 * 100 }{2} = 5000
\end{align*}$$

æ¯ä¸ª pool çš„ $PG = \frac{5000}{5} = 1000$ï¼Œé‚£ä¹ˆåˆ›å»º pool çš„æ—¶å€™å°±æŒ‡å®š PG ä¸º 1024ï¼Œå³ä½¿ç”¨å‘½ä»¤ ceph osd pool create <pool_name> 1024ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå­˜å‚¨æ± ä¸­çš„ PG æ•°é‡åœ¨é›†ç¾¤è§„æ¨¡æ‰©å®¹çš„æ—¶å€™ï¼ŒåŠ¨æ€è¿›è¡Œè°ƒæ•´ä»¥æ»¡è¶³ç›¸å¯¹è¾ƒä½³çš„ PG æ•°é‡æä¾›å­˜å‚¨æ€§èƒ½ã€‚

### PG çŠ¶æ€æ±‡æ€»

| çŠ¶æ€ | æè¿° |
|-----|-----|
| creating | PG æ­£åœ¨è¢«åˆ›å»º |
| peering | PG æ­£åœ¨æ‰§è¡ŒåŒæ­¥å¤„ç†ã€‚PG å¤„äº peering è¿‡ç¨‹ä¸­ï¼Œpeering ç”±ä¸» OSD å‘èµ·ï¼Œä½¿å­˜æ”¾ PG å‰¯æœ¬çš„æ‰€æœ‰ OSD å°± PG çš„æ‰€æœ‰å¯¹è±¡å’Œå…ƒæ•°æ®çš„çŠ¶æ€è¾¾æˆä¸€è‡´çš„è¿‡ç¨‹ï¼Œpeering è¿‡ç¨‹å®Œæˆåï¼Œä¸» OSD å°±å¯ä»¥æ¥å—å®¢æˆ·ç«¯å†™è¯·æ±‚ã€‚ |
| peered | peering å·²ç»å®Œæˆï¼Œä½†è¿˜ä¸èƒ½å¤„ç†å®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚ï¼ŒPG å½“å‰ acting set æ•°é‡å°äºå­˜å‚¨æ± çš„æœ€å°å‰¯æœ¬æ•°ï¼ˆmin_sizeï¼‰ã€‚recovery å¯èƒ½å‘ç”Ÿè¿™ç§çŠ¶æ€ã€‚ |
| activating | peering å·²ç»å®Œæˆï¼ŒPG æ­£åœ¨ç­‰å¾…æ‰€æœ‰ PG å®ä¾‹åŒæ­¥å¹¶å›ºåŒ– peered çš„ç»“æœï¼ˆinfoã€log ç­‰ï¼‰ã€‚ |
| active | æ¿€æ´»çŠ¶æ€ã€‚PG å¯ä»¥æ­£å¸¸å¤„ç†æ¥è‡ªå®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚ã€‚ |
| unactive | éæ¿€æ´»çŠ¶æ€ã€‚PG ä¸èƒ½å¤„ç†æ¥è‡ªå®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚ã€‚ |
| clean | å¹²å‡€çŠ¶æ€ã€‚PG å½“å‰ä¸å­˜åœ¨å¾…ä¿®å¤çš„å¯¹è±¡ï¼Œacting set å’Œ up set å†…å®¹ä¸€è‡´ï¼ˆå…·æœ‰æ­£ç¡®æ•°é‡çš„å‰¯æœ¬ï¼‰ï¼Œå¹¶ä¸”å¤§å°ç­‰äºå­˜å‚¨æ± çš„å‰¯æœ¬æ•°ã€‚ |
| unclean | éå¹²å‡€çŠ¶æ€ã€‚PG ä¸èƒ½ä»ä¸Šä¸€ä¸ªå¤±è´¥ä¸­æ¢å¤ã€‚ |
| degraded | é™çº§çŠ¶æ€ã€‚peering å®Œæˆåï¼ŒPG æ£€æµ‹åˆ°ä»»æ„ä¸€ä¸ª PG å®ä¾‹å­˜åœ¨ä¸ä¸€è‡´ï¼ˆéœ€è¦è¢«åŒæ­¥/ä¿®å¤ï¼‰çš„å¯¹è±¡ï¼ˆprimary OSD ä¸­çš„å¯¹è±¡å‰¯æœ¬è¿˜æœªå®Œå…¨åŒæ­¥è‡³ secondary OSD ä¸­ï¼‰ï¼Œæˆ–è€…å½“å‰ acting set å°äºå­˜å‚¨æ± å‰¯æœ¬æ•°ã€‚ |
| undersized | å·²è¢«åˆ†é…åˆ° OSD ä¸Šçš„ PG æ•°é‡å°äºå­˜å‚¨æ± çš„å‰¯æœ¬æ•°ï¼›æˆ–è€…é¢„æœŸåˆ†é…çš„ PG æ•°é‡å¤§äºå½“å‰å¯ç”¨çš„ OSD æ•°é‡ï¼Œè¿™ç§æƒ…å†µæå¤§å¯èƒ½æ˜¯ç”±äº CRUSH map æœªè®¾ç½®æ­£ç¡®ï¼Œå¯¼è‡´æ— æ³•æ‰¾åˆ°æ»¡è¶³ CRUSH è§„åˆ™çš„ OSDï¼Œè‡´ä½¿ PG å¤„äº undersized çŠ¶æ€ã€‚ |
| remapped | PG é‡æ–°æ˜ å°„çŠ¶æ€ã€‚PG ä¸­çš„å¯¹è±¡æ•°æ®ä»æ—§çš„æ´»åŠ¨çš„ OSD é›†åˆ°æ–°çš„æ´»åŠ¨çš„ OSD é›†çš„è¿ç§»ã€‚åœ¨è¿ç§»æœŸé—´è¿˜æ˜¯ç”¨åŸå…ˆçš„ primary OSD å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ï¼Œä¸€æ—¦è¿ç§»å®Œæˆåæ–°çš„æ´»åŠ¨çš„ OSD é›†ä¸­ primary OSD å¼€å§‹å¤„ç†ã€‚ |
| down | PG å¤„äºå¤±æ•ˆã€ç¦»çº¿çŠ¶æ€ã€‚peering è¿‡ç¨‹ä¸­ï¼ŒPG æ£€æµ‹åˆ°æŸä¸ªä¸èƒ½è¢«è·³è¿‡çš„ interval ä¸­ï¼ˆå¦‚è¯¥ interval æœŸé—´ï¼ŒPG å®Œæˆäº† peeringï¼Œå¹¶ä¸”æˆåŠŸåˆ‡æ¢è‡³ active çŠ¶æ€ï¼Œä»è€Œæœ‰å¯èƒ½æ­£å¸¸å¤„ç†äº†æ¥è‡ªå®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚ï¼‰ï¼Œå½“å‰å‰©ä½™åœ¨çº¿çš„ OSD ä¸è¶³ä»¥å®Œæˆæ•°æ®ä¿®å¤ã€‚ |
| recovery_wait | PG æ­£åœ¨ç­‰å¾…é˜Ÿåˆ—ä¸­ç­‰å¾…æ¢å¤ã€‚ |
| recovering | PG æ­£åœ¨æ¢å¤ã€‚PG ä¸­çš„å¯¹è±¡æ­£åœ¨è¿ç§»æˆ–å‰¯æœ¬æ­£åœ¨åŒæ­¥ã€‚ |
| incomplete | Ceph æ£€æµ‹åˆ° PG ç¼ºå°‘å…³äºå¯èƒ½å‘ç”Ÿçš„å†™å…¥çš„ä¿¡æ¯ï¼Œæˆ–è€…æ²¡æœ‰ä»»ä½•å¥åº·çš„å¯¹è±¡å‰¯æœ¬ã€‚å¦‚æœçœ‹åˆ°è¿™ç§çŠ¶æ€ï¼Œè¯·å°è¯•å¯åŠ¨ä»»ä½•å¯èƒ½åŒ…å«æ‰€éœ€ä¿¡æ¯çš„å¤±è´¥ OSDã€‚peering è¿‡ç¨‹ä¸­ï¼Œç”±äºæ— æ³•é€‰å‡ºæƒå¨æ—¥å¿—ï¼Œæˆ–é€šè¿‡ choose_acting é€‰å‡ºçš„ acting set åç»­ä¸è¶³ä»¥å®Œæˆæ•°æ®ä¿®å¤ï¼Œå¯¼è‡´ peering æ— æ³•æ­£å¸¸å®Œæˆã€‚ |
| inconsistent | PG ä¸­çš„å¤šä¸ªå¯¹è±¡å‰¯æœ¬ä¸ä¸€è‡´ã€‚é›†ç¾¤æ¸…ç†ï¼ˆscrubï¼‰æˆ–æ·±åº¦æ¸…ç†ï¼ˆdeep-scrubï¼‰åæ£€æµ‹åˆ° PG ä¸­çš„å¯¹è±¡å‰¯æœ¬å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¦‚å¯¹è±¡çš„å¤§å°ä¸ä¸€è‡´æˆ– recovery åçš„æŸä¸ªå¯¹è±¡å‰¯æœ¬ä¸¢å¤±ã€‚ |
| repair | PG åœ¨æ‰§è¡Œ scrub è¿‡ç¨‹ä¸­ï¼Œå¦‚æœå‘ç°å­˜åœ¨ä¸ä¸€è‡´çš„å¯¹è±¡å‰¯æœ¬ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿®å¤ï¼Œåˆ™è‡ªåŠ¨è¿›è¡Œä¿®å¤çŠ¶æ€ã€‚ |
| backfill_wait | PG æ­£åœ¨å¼€å§‹å›å¡«çš„ç­‰å¾…é˜Ÿåˆ—ä¸­ã€‚ |
| backfilling | PG æ­£åœ¨æ‰§è¡Œæ•°æ®å›å¡«ã€‚backfill æ˜¯ recovery çš„ä¸€ç§ç‰¹æ®Šåœºæ™¯ã€‚å½“ peering ç»“æŸåï¼Œå¦‚æœåŸºäºå½“å‰æƒå¨æ—¥å¿—æ— æ³•å¯¹ up set ä¸­çš„ PG å®ä¾‹å®æ–½å¢é‡åŒæ­¥ï¼ˆå¦‚æ‰¿è½½è¿™äº› PG å®ä¾‹çš„ OSD ç¦»çº¿å¤ªä¹…ï¼Œæˆ–è€…æ–°çš„ OSD åŠ å…¥é›†ç¾¤å¯¼è‡´çš„ PG å®ä¾‹æ•´ä½“è¿ç§»ï¼‰ï¼Œåˆ™é€šè¿‡å®Œå…¨æ‹·è´å½“å‰ primary OSD ä¸­çš„æ‰€æœ‰å¯¹è±¡çš„æ–¹å¼è¿›è¡Œå…¨é‡åŒæ­¥ã€‚ |
| backfill_toofull | æŸä¸ªéœ€è¦è¢« backfill çš„ PG å®ä¾‹ï¼Œå…¶æ‰€åœ¨çš„ OSD è¶…è¿‡äº† backfillfull-ratioï¼Œbackfill æµç¨‹è¢«æŒ‚èµ·ã€‚ |
| scrubbing | PG æ­£åœ¨æˆ–å³å°†è¿›è¡Œå¯¹è±¡å‰¯æœ¬å…ƒæ•°æ®çš„ä¸ä¸€è‡´æ€§æ£€æµ‹ |
| deep | PG æ­£åœ¨æˆ–å³å°†è¿›è¡Œå¯¹è±¡å‰¯æœ¬æ•°æ®å·²ä¿å­˜çš„æ ¡éªŒå’Œï¼ˆchecksumsï¼‰çš„æ£€æµ‹ |
| stale | PG å¤„äºæœªçŸ¥çš„çŠ¶æ€ï¼ˆæœªåˆ·æ–°çŠ¶æ€ï¼‰ã€‚PG çŠ¶æ€æ²¡æœ‰è¢«ä»»ä½• OSD æ›´æ–°ï¼Œè¯´æ˜æ‰€æœ‰å­˜å‚¨è¯¥ PG çš„ OSD å¯èƒ½æŒ‚æ‰ï¼Œæˆ–è€… mon æ²¡æœ‰æ£€æµ‹åˆ° primary OSD çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯èƒ½å­˜åœ¨ç½‘ç»œæŠ–åŠ¨ï¼‰ã€‚ |
| unknown | è‡ª Mgr å¯åŠ¨ï¼Œceph-magr è¿˜æœªæ¥æ”¶åˆ°æ¥è‡ªäº PG çš„ä»»ä½•æ¶ˆæ¯ã€‚ |

### ä¿®å¤ unknown çŠ¶æ€çš„ PG

PG å‡ºç° stale çŠ¶æ€ï¼Œä¹Ÿå°±æ˜¯å¤„äºåƒµæ­»çŠ¶æ€ï¼ˆé•¿æ—¶é—´å¤„äºé”™è¯¯çŠ¶æ€ï¼‰ï¼Œè¯¥çŠ¶æ€æ— æ³•å¤„ç†å®¢æˆ·ç«¯æ–°çš„è¯·æ±‚ï¼Œæ–°çš„è¯·æ±‚è¿‡æ¥åªä¼šé˜»å¡ï¼Œè¿™ç§æƒ…å†µä¸€èˆ¬æ˜¯ç”±äºæ‰€æœ‰ PG å‰¯æœ¬æ‰€åœ¨çš„ OSD éƒ½æŒ‚äº†ã€‚è¦æ¨¡æ‹Ÿå…¶å®ä¹Ÿå¾ˆç®€å•ï¼Œæ¯”å¦‚è®¾ç½®å­˜å‚¨æ± ä¸º 2 å‰¯æœ¬ï¼Œç„¶å PG æ˜ å°„çš„ 2 ä¸ª OSD æŒ‚æ‰å³å¯å‡ºç°ã€‚æœ€å¥½çš„æ¢å¤æ–¹æ³•å½“ç„¶æ˜¯é‡æ–°æ‹‰èµ·è¿™ä¸¤ä¸ª OSDï¼Œä½†æœ‰æ—¶å¯èƒ½å‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä¸¤ä¸ª OSD æ°¸è¿œä¹Ÿæ‹‰ä¸èµ·æ¥ï¼Œç„¶åå°†è¿™ä¸¤ä¸ª OSD è¸¢å‡ºé›†ç¾¤ï¼Œç„¶åè¿™äº› PG å°±æ˜¯ stale çš„çŠ¶æ€ï¼Œè¿™æ—¶çš„æ¢å¤æ–¹æ³•åªèƒ½æ˜¯ä¸¢æ‰æ­¤ PG ä¸­çš„å¯¹è±¡æ•°æ®ï¼Œé‡æ–°åˆ›å»º PGã€‚

PG å‡ºç° unknown çŠ¶æ€ï¼Œè¿™é€šå¸¸è¡¨æ˜è¯¥ PG çš„æ•°æ®å¯èƒ½å·²ç»ä¸¢å¤±æˆ–æ— æ³•æ­£å¸¸è®¿é—®ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œå¯å…ˆå°è¯•ä»å…¶ä»– PG å‰¯æœ¬æ¢å¤æ•°æ®ï¼Œè‹¥æ­¤æ–¹æ³•ä¾ç„¶ä¸èƒ½æ¢å¤ PGï¼Œé‚£ä¹ˆå°è¯•å¼ºåˆ¶é‡å»º PGã€‚ä½†è¯·æ³¨æ„ï¼Œæ­¤æ“ä½œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œå› æ­¤ä»…åœ¨ç¡®å®š PG æ•°æ®å·²ç»æ— æ³•æ¢å¤æ—¶ä½¿ç”¨ã€‚

```bash
### åœºæ™¯1ï¼šå°è¯•ä»å…¶ä»– PG ä¸­æ¢å¤å¯¹è±¡å‰¯æœ¬è¿›è¡Œä¿®å¤
$ ceph pg dump_stuck [stale|inactive|unclean]
$ ceph pg <pg.id> query | jq .recovery_state
# æŸ¥çœ‹ PG å½“å‰å¯èƒ½å‡ºç°çš„é—®é¢˜
# è‹¥å¯è¿”å›æŒ‡å®š pg.id çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ç»§ç»­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼›è‹¥æ— æ³•æ‰¾åˆ°è¯¥ pg.idï¼Œå¯ç›´æ¥ä½¿ç”¨åœºæ™¯2ä¿®å¤ã€‚
$ ceph pg deep-scrub <pg.id>
# è‹¥èƒ½æŸ¥è¯¢åˆ° pg.idï¼Œæ‰§è¡ŒæŒ‡å®š PG çš„æ·±åº¦æ¸…ç†ï¼Œæ£€æŸ¥ PG çš„å®é™…çŠ¶æ€ã€‚

$ ceph pg <pg.id> mark_unfound_lost revert
# æ ‡è®° PG ä¸ºä¸¢å¤±ï¼Œå¹¶ä»å…¶ä»– PG ä¸­å°è¯•æ¢å¤æ•°æ®ï¼ˆå³å›æ»šå¯¹è±¡æ•°æ®è‡³ä¸Šä¸€ä¸ªç‰ˆæœ¬ï¼‰ã€‚
$ ceph pg <pg.id> mark_unfound_lost delete  #å¯é€‰æ­¥éª¤
# è‹¥ä»¥ä¸Šå‘½ä»¤ä¾ç„¶ä¸èƒ½æ¢å¤ pg çŠ¶æ€ï¼Œé‚£ä¹ˆåˆ é™¤ pg ä¸­çš„å¯¹è±¡å‰¯æœ¬ï¼Œè¿™å°†é€ æˆæ•°æ®çš„ä¸¢å¤±ï¼
$ ceph pg repair <pg.id>
# è§¦å‘ PG ä¿®å¤
# è‹¥ä¿®å¤æˆåŠŸï¼Œé‚£ä¹ˆ PG è¿›å…¥æ­£å¸¸çŠ¶æ€ï¼ˆactive+cleanï¼‰ã€‚

### åœºæ™¯2ï¼šæ— æ³•æŸ¥è¯¢ pg.id é‡å»º PG
$ ceph osd force-create-pg <pg.id> --yes-i-really-mean-it
# æ ¹æ® pg.id å¼ºåˆ¶é‡å»ºæŒ‡å®šçš„ PG
```

## Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡

```bash
$ ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE    RAW USE  DATA     OMAP     META     AVAIL    %USE   VAR   PGS  STATUS
 1    hdd  0.00980   1.00000  10 GiB   63 MiB  3.9 MiB      0 B   59 MiB  9.9 GiB   0.62  0.09   74      up
 3    hdd  0.00980   1.00000  10 GiB   72 MiB  3.9 MiB      0 B   68 MiB  9.9 GiB   0.70  0.11   65      up
 5    ssd  0.00980   1.00000  10 GiB   46 MiB  3.9 MiB      0 B   42 MiB   10 GiB   0.45  0.07   62      up
 0    hdd  0.00980   1.00000  10 GiB   67 MiB  3.9 MiB      0 B   63 MiB  9.9 GiB   0.65  0.10   67      up
 2    hdd  0.00980   1.00000  10 GiB   54 MiB  3.9 MiB      0 B   50 MiB  9.9 GiB   0.53  0.08   67      up
 4    ssd  0.00980   1.00000  10 GiB   68 MiB  3.9 MiB      0 B   64 MiB  9.9 GiB   0.67  0.10   67      up
 7    hdd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  324 MiB   10 GiB  16.70  2.52   63      up
 8    hdd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  324 MiB   10 GiB  16.70  2.52   68      up
 6    ssd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  318 MiB   10 GiB  16.70  2.52   70      up
                       TOTAL  96 GiB  6.4 GiB   35 MiB  8.5 KiB  1.3 GiB   90 GiB   6.64
MIN/MAX VAR: 0.07/2.52  STDDEV: 7.62
# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
$ ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE    RAW USE  DATA     OMAP     META     AVAIL    %USE   VAR   PGS  STATUS  TYPE NAME
-1         0.09384         -  96 GiB  6.4 GiB   35 MiB      0 B  1.3 GiB   90 GiB   6.64  1.00    -          root default
-3         0.02939         -  30 GiB  181 MiB   12 MiB      0 B  169 MiB   30 GiB   0.59  0.09    -              host serverc
 1    hdd  0.00980   1.00000  10 GiB   63 MiB  3.9 MiB      0 B   59 MiB  9.9 GiB   0.62  0.09   74      up          osd.1
 3    hdd  0.00980   1.00000  10 GiB   72 MiB  3.9 MiB      0 B   68 MiB  9.9 GiB   0.70  0.11   65      up          osd.3
 5    ssd  0.00980   1.00000  10 GiB   46 MiB  3.9 MiB      0 B   42 MiB   10 GiB   0.45  0.07   62      up          osd.5
-5         0.02939         -  30 GiB  189 MiB   12 MiB      0 B  178 MiB   30 GiB   0.62  0.09    -              host serverd
 0    hdd  0.00980   1.00000  10 GiB   67 MiB  3.9 MiB      0 B   63 MiB  9.9 GiB   0.65  0.10   67      up          osd.0
 2    hdd  0.00980   1.00000  10 GiB   54 MiB  3.9 MiB      0 B   50 MiB  9.9 GiB   0.53  0.08   67      up          osd.2
 4    ssd  0.00980   1.00000  10 GiB   68 MiB  3.9 MiB      0 B   64 MiB  9.9 GiB   0.67  0.10   67      up          osd.4
-9         0.03506         -  36 GiB  6.0 GiB   12 MiB      0 B  966 MiB   30 GiB  16.70  2.52    -              host servere
 7    hdd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  324 MiB   10 GiB  16.70  2.52   63      up          osd.7
 8    hdd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  324 MiB   10 GiB  16.70  2.52   68      up          osd.8
 6    ssd  0.01169   1.00000  12 GiB  2.0 GiB  3.9 MiB      0 B  318 MiB   10 GiB  16.70  2.52   70      up          osd.6
                       TOTAL  96 GiB  6.4 GiB   35 MiB  8.5 KiB  1.3 GiB   90 GiB   6.64
MIN/MAX VAR: 0.07/2.52  STDDEV: 7.62

# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µå¹¶æ˜¾ç¤º osd åœ¨ CRUSH map ä¸­çš„ä½ç½®

$ ceph osd tree
# osd ç£ç›˜åœ¨é›†ç¾¤ä¸­çš„åˆ†å¸ƒæƒ…å†µ
$ ceph osd stat
  9 osds: 9 up (since 13h), 9 in (since 2y); epoch: e199
# æŸ¥çœ‹æ‰€æœ‰ osd çš„çŠ¶æ€

$ ceph osd find <osd.id>
# ğŸ©º æ•…éšœæ’é™¤ï¼šæ ¹æ® osd id æŸ¥æ‰¾ osd çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹
$ ceph osd find 2  #ç¤ºä¾‹
{
    "osd": 2,
    "addrs": {
        "addrvec": [
            {
                "type": "v2",
                "addr": "172.25.250.12:6816",
                "nonce": 3631084615
            },
            {
                "type": "v1",
                "addr": "172.25.250.12:6817",
                "nonce": 3631084615
            }
        ]
    },
    "osd_fsid": "a28ad912-a54a-423a-a2d1-4889b0788a47",
    "host": "serverc.lab.example.com",
    "crush_location": {
        "host": "serverc",
        "root": "default"
    }
}
# æŸ¥æ‰¾ osd.2 çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹

$ ceph osd metadata <osd.id>
# ğŸ©º æ•…éšœæ’é™¤ï¼šæ ¹æ® osd id æŸ¥çœ‹ osd çš„å…ƒæ•°æ®ä¿¡æ¯

$ ceph osd perf
osd  commit_latency(ms)  apply_latency(ms)
  8                   0                  0
  7                   0                  0
  6                   0                  0
  1                   0                  0
  0                   0                  0
  2                   0                  0
  3                   0                  0
  4                   0                  0
  5                   0                  0
# è§‚æµ‹æ‰€æœ‰ osd çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯  
```

### Lab: å®šä½å¯¹è±¡ä¸ OSDã€PG çš„æ˜ å°„å…³ç³»

```bash
$ rados -p <pool_name> ls
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­çš„å¯¹è±¡
$ rados -p testpool ls  #ç¤ºä¾‹
test-data
testobject
$ ceph osd map <pool_name> <object_name>
# ğŸ©º æ•…éšœæ’é™¤ï¼š
#   1. æ ¹æ®æŒ‡å®šçš„å¯¹è±¡åç§°æŸ¥æ‰¾å…¶ä¸ pg åŠ osd çš„æ˜ å°„å…³ç³»
#   2. æ­¤å‘½ä»¤ç›¸è¾ƒäº ceph pg map <pg.id> æ›´è¿›ä¸€æ­¥ï¼Œç›´æ¥æŸ¥æ‰¾å¯¹è±¡çš„æ˜ å°„å…³ç³»ã€‚
$ ceph osd map testpool test-data  #ç¤ºä¾‹
osdmap e664 pool 'testpool' (9) object 'test-data' -> pg 9.4a628b60 (9.0) -> up ([0,2,5], p0) acting ([0,2,5], p0)
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­å¯¹è±¡ã€pg ä¸ osd çš„æ˜ å°„å…³ç³»
```

### Lab: æ ‡è®° OSD åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€

OSD åœ¨é›†ç¾¤ä¸­çš„æ ‡è®°ï¼ˆflagï¼‰ï¼š`noup`, `nodown`, `noin`, `noout`, `norecover`, `nobackfill`, `norebalance`, `noscrub`, `nodeep-scrub`

```bash
$ ceph osd out <osd.id>
# å°†æŒ‡å®šçš„ osd æ ‡è®°ä¸º out çŠ¶æ€
$ ceph osd in <osd.id>
# å°†æŒ‡å®šçš„ osd æ ‡è®°ä¸º in çŠ¶æ€
# æ³¨æ„ï¼šå¯å°†è¿è¡Œä¸­çš„ osd æ ‡è®°ä¸º in æˆ– out çš„çŠ¶æ€ï¼Œå› ä¸º in æˆ– out çŠ¶æ€ä¸ä¸è¿è¡ŒçŠ¶æ€å…³è”ã€‚

$ ceph osd [set|unset] noscrub
# (ä¸)è®¾ç½® osd ä¸æ‰§è¡Œæ¸…ç†
$ ceph osd [set|unset] nodeep-scrub
# (ä¸)è®¾ç½® osd ä¸æ‰§è¡Œæ·±åº¦æ¸…ç†
```

### Lab: ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„ OSD è®¾å¤‡ï¼ˆscale upï¼‰

```bash
$ ceph orch device zap --force <hostname> /path/to/device
# å¼ºåˆ¶åˆ é™¤ä¹‹å‰ osd åˆ›å»ºçš„æ‰€æœ‰åˆ†åŒºå¹¶æ¸…é™¤å…¶ä¸­çš„æ‰€æœ‰æ•°æ®ä¸ºéƒ¨ç½² osd åšå‡†å¤‡
$ ceph orch daemon add osd <hostname>:/path/to/device
$ ceph orch daemon add osd serverd.lab.example.com:/dev/vde  #ç¤ºä¾‹
# æ·»åŠ æŒ‡å®šä¸»æœºä¸Šçš„ osd è®¾å¤‡
```
  
![ceph-orch-daemon-add-osd-demo](images/ceph-orch-daemon-add-osd-demo.png)

### Lab: ä½¿ç”¨ osd specification file éƒ¨ç½² OSD è®¾å¤‡

å…³äº `--all-available-devices` é€‰é¡¹çš„è¯´æ˜ï¼š

ä½¿ç”¨ `ceph orch apply osd --all-available-devices` å‘½ä»¤æ·»åŠ  `ceph orch device ls` å‘½ä»¤è¿”å›ä¸º Available çŠ¶æ€çš„ osdï¼Œå¦‚æœå°† osd ä»é›†ç¾¤ä¸­åˆ é™¤å¹¶ zap åï¼Œè¿™äº› osd å°†é‡æ–°è‡ªåŠ¨åŠ å…¥åˆ°é›†ç¾¤ä¸­ã€‚æœ‰æ—¶å¯¹äºæ­¤ç§è¡Œä¸ºæ˜¯ä¸éœ€è¦çš„ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹æ³•å»é™¤è‡ªåŠ¨æ·»åŠ åˆ°é›†ç¾¤ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
$ ceph orch ls --service-type osd --format yaml > ./osd-spec.yaml
$ vim ./osd-spec.yaml
service_type: osd
service_id: default_drive_group
service_name: osd.default_drive_group
placement:
  hosts:
  - serverc.lab.example.com
  - serverd.lab.example.com
  - servere.lab.example.com
spec:
  data_devices:
    paths:
    - /dev/vdc
    - /dev/vdd
  filter_logic: AND
  objectstore: bluestore
unmanaged: true    #æ­¤é€‰é¡¹å°†ä¸å†ä½¿ osd è¢«é‡æ–°è‡ªåŠ¨åŠ å…¥é›†ç¾¤
status:
  created: '2025-04-13T06:49:36.576870Z'
  running: 0
  size: 3

$ ceph orch apply -i ./osd-spec.yaml
# é‡æ–°è®¾ç½® osd é…ç½®ä½¿å…¶ç”Ÿæ•ˆ
```
  
### Lab: åˆ é™¤ OSD è®¾å¤‡
  
```bash
### ç¡®è®¤éœ€è¦æ›´æ¢ OSD è®¾å¤‡ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ ###
$ ceph osd set noscrub; ceph osd set nodeep-scrub
# æš‚æ—¶è®¾ç½®é›†ç¾¤ç¦ç”¨æ¸…ç†ä¸æ·±åº¦æ¸…ç†

$ ceph osd out $id
# å°†æŒ‡å®š osd è¸¢å‡ºé›†ç¾¤
# æ­¤ osd ä¸Š pg ä¸­çš„å¯¹è±¡å‰¯æœ¬å°†è¿ç§»é‡å¹³è¡¡è‡³å…¶ä»– osd çš„ pg å‰¯æœ¬ä¸­ï¼Œä½¿ç”¨ ceph -w è§‚å¯Ÿ pg çŠ¶æ€ï¼Œ
# ç­‰å¾… pg çŠ¶æ€æ¢å¤ active+clean åç»§ç»­æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚

$ while ! ceph osd safe-to-destroy osd.$id ; do sleep 10s ; done
# æ£€æµ‹æŒ‡å®šçš„ osd æ˜¯å¦å¯è¢«å®‰å…¨åœ°ç ´åã€‚
# è‹¥å‘½ä»¤è¿”å› 0ï¼Œè¡¨æ˜ osd å¯å®‰å…¨åœ°ç ´åï¼Œå–åæ“ä½œå¯è®© while åˆ¤æ–­é€šè¿‡ï¼Œæ‰§è¡Œæ¥ä¸‹æ¥çš„è¯­å¥ã€‚

$ ceph device ls | awk /<fqdn>/
# æŸ¥çœ‹æŒ‡å®šèŠ‚ç‚¹ä¸Šç£ç›˜è®¾å¤‡ä¸ osd çš„å¯¹åº”å…³ç³»

### å¯é€‰æ“ä½œï¼š$ ceph osd reweight $id <weight>
# è°ƒæ•´æŒ‡å®š osd çš„ reweight æƒé‡å€¼
### å¯é€‰æ“ä½œï¼š$ ceph osd reweight $id 0.0
# è°ƒæ•´æŒ‡å®š osd çš„ reweight æƒé‡å€¼ä¸º 0.0ï¼Œä½¿å¾—è¯¥ osd ä¸Š pg ä¸­çš„å¯¹è±¡å‰¯æœ¬è¿ç§»è‡³å…¶ä»– osd ä¸Šã€‚
# æ³¨æ„ï¼šè°ƒæ•´å®Œ reweight åï¼Œé›†ç¾¤çŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œç­‰å¾…é›†ç¾¤ HEALTH_OK åå†åˆ é™¤ osd è®¾å¤‡ã€‚
# $ ceph osd reweight 7 0.0  #ç¤ºä¾‹
# $ ceph -s
#  cluster:
#    id:     0b870ad2-8968-11ed-868b-52540000fa0c
#    health: HEALTH_WARN
#            Degraded data redundancy: 11/885 objects degraded (1.243%), 8 pgs degraded
#
#  services:
#    mon: 4 daemons, quorum serverc.lab.example.com,clienta,serverd,servere (age 4h)
#    mgr: serverc.lab.example.com.pqcbzl(active, since 4h), standbys: serverd.xlpfgs, clienta.jfnnze, servere.heelmi
#    osd: 9 osds: 9 up (since 4h), 8 in (since 8s); 3 remapped pgs
#    rgw: 2 daemons active (2 hosts, 1 zones)
#
#  data:
#    pools:   11 pools, 297 pgs
#    objects: 295 objects, 6.2 MiB
#    usage:   309 MiB used, 80 GiB / 80 GiB avail
#    pgs:     11/885 objects degraded (1.243%)
#             64/885 objects misplaced (7.232%)
#             278 active+clean
#             9   active+remapped+backfill_wait
#             8   active+recovery_wait+degraded
#             1   active+recovering
#             1   active+remapped+backfilling
#
#  io:
#    client:   1.3 KiB/s rd, 1 op/s rd, 0 op/s wr
#    recovery: 197 B/s, 3 keys/s, 2 objects/s
#
#  progress:
#    Global Recovery Event (0s)
#      [............................]

$ ceph orch daemon stop osd.$id
# åœæ­¢æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹
# æ³¨æ„ï¼š
#   1. è‹¥ç™»å½•è‡³ osd æ‰€åœ¨çš„èŠ‚ç‚¹ä½¿ç”¨ systemctl å‘½ä»¤æŸ¥çœ‹ï¼Œå¯å‘ç°æ­¤æœåŠ¡å•å…ƒå·²åœæ­¢ã€‚
#   2. å¯åœ¨æ­¤èŠ‚ç‚¹ä¸Šä½¿ç”¨ systemctl å‘½ä»¤å¯åŠ¨æ­¤ osd å®ˆæŠ¤è¿›ç¨‹
$ ceph orch daemon rm osd.$id --force
# å°†æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹å¼ºåˆ¶ç§»é™¤
$ ceph osd rm $id
# å°† osd ä»é›†ç¾¤ osdmapï¼ˆosd æ˜ å°„ï¼‰ä¸­åˆ é™¤
# æ³¨æ„ï¼š
#   1. æ­¤æ—¶ï¼Œceph -s é›†ç¾¤çŠ¶æ€å°†æ˜¾ç¤º crushmap ä¸­ä¾ç„¶ä¿ç•™ç€ osdï¼Œè€Œ osdmap ä¸­å·²ç§»é™¤ osdï¼Œä¸¤è€…çš„çŠ¶æ€ä¸åŒ¹é…ã€‚
#   2. ceph osd tree æ˜¾ç¤ºçš„ osd çŠ¶æ€ä¸º DNEï¼Œå½“ä» crushmap ä¸­ç§»é™¤åå°†æ¸…é™¤ã€‚
$ ceph orch osd rm status
# æŸ¥çœ‹ osd åˆ é™¤çš„çŠ¶æ€

$ ceph osd unset noscrub; ceph osd unset nodeep-scrub
# å–æ¶ˆé›†ç¾¤ç¦ç”¨æ¸…ç†ä¸æ·±åº¦æ¸…ç†

### å¯é€‰æ­¥éª¤ï¼š$ ceph osd crush rm $id
# å°† osd ä»é›†ç¾¤ crushmapï¼ˆCRUSH æ˜ å°„ï¼‰ä¸­åˆ é™¤
# æ³¨æ„ï¼šè‹¥åç»­è¿˜éœ€è¦å°† osd æ·»åŠ åˆ°é›†ç¾¤ä¸­çš„è¯ï¼Œé‚£ä¹ˆåœ¨ crushmap ä¸­å¯ç»§ç»­ä¿ç•™ï¼
```
  
![ceph-status-with-noosd-in-osdmap](images/ceph-status-with-noosd-in-osdmap.png)

### Lab: é…ç½® ceph è½¯ä»¶æº

RHCS5 (Ceph Pacific 16.x) ä¸­å…¨é¢ä½¿ç”¨ `cephadm shell` å¯ç”¨å®¹å™¨åŒ–å¯¹ Ceph é›†ç¾¤çš„ç®¡ç†ï¼Œåœ¨å®¹å™¨ä¸­åŒ…å«æœ‰ `ceph-volume` å‘½ä»¤è¡Œå·¥å…·ï¼Œè€Œåœ¨å®¿ä¸»æœºç³»ç»Ÿä¸­é»˜è®¤ä¸å®‰è£…æ­¤å·¥å…·ï¼Œéœ€è¦é…ç½® ceph è½¯ä»¶æºè¿›è¡Œå®‰è£…ã€‚

```bash
$ sudo cat /etc/yum.repos.d/ceph.repo <<EOF
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://mirrors.163.com/ceph/rpm-pacific/el8/x86_64/
enabled=1
gpgcheck=1
priority=1
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
EOF

$ sudo dnf provides ceph-volume  #ceph-volume å·¥å…·çš„è½¯ä»¶åŒ…
Last metadata expiration check: -836 days, 2:08:01 ago on Wed 16 Apr 2025 02:13:18 AM EDT.
ceph-osd-2:16.2.15-0.el8.x86_64 : Ceph Object Storage Daemon
Repo        : @System
Matched from:
Filename    : /usr/sbin/ceph-volume

ceph-osd-2:16.2.15-0.el8.x86_64 : Ceph Object Storage Daemon
Repo        : ceph-noarch
Matched from:
Filename    : /usr/sbin/ceph-volume

$ sudo dnf install -y ceph-osd
```

ğŸ”¥ å…³äº `ceph-volume` çš„è¯´æ˜ï¼š

ä» RHCS4 å¼€å§‹å¼•å…¥ `ceph-volume` å‘½ä»¤ç”¨äºåˆ›å»ºåŸºäº `BlueStore` å­˜å‚¨å¼•æ“çš„ OSDï¼Œåˆ†åˆ«ä½¿ç”¨ `ceph-volume lvm prepare` ä¸ `ceph-volume lvm activate` å­å‘½ä»¤åˆ›å»ºä¸æ¿€æ´» OSD è®¾å¤‡ï¼Œè¿‡ç¨‹å¦‚ä¸‹ç¤ºæ„ï¼š

![ceph-volume-add-bluestore-osd-backend](images/ceph-volume-add-bluestore-osd-backend.jpg)

![ceph-volume-add-bluestore-osd-backend-status](images/ceph-volume-add-bluestore-osd-backend-status.jpg)
  
![ceph-volume-active-bluestore-osd](images/ceph-volume-active-bluestore-osd.jpg)

### Lab: RHCS5 ä¸­ä½¿ç”¨ ceph-volume æ‰‹åŠ¨éƒ¨ç½² OSD

è™½ç„¶ä½¿ç”¨ `osd specification file` å®šä¹‰éœ€éƒ¨ç½²çš„ osd ç£ç›˜çµæ´»ä¸”ä¾¿æ·ï¼Œä½†æœ‰æ—¶å¯èƒ½éœ€è¦å°†å¤šä¸ª osd çš„ `wal` ä¸ `rocksdb` éƒ¨ç½²åœ¨åŒä¸€ç£ç›˜ä¸Šï¼ˆé«˜æ€§èƒ½çš„ SSD æˆ– NVMeï¼‰åŠ é€Ÿ osd çš„è¯»å†™æ€§èƒ½ï¼Œå¯é€šè¿‡æ‰‹åŠ¨æ·»åŠ çš„æ–¹å¼å®Œæˆï¼ˆä¹ŸåŒæ ·å¯ä»¥é€šè¿‡ osd å®šä¹‰æ–‡ä»¶çš„æ–¹å¼å®ç°ï¼‰ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
[root@servere ~]# lsblk -p
...
vde               252:64   0   10G  0 disk
â”œâ”€vde1            252:65   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdb osd çš„ wal
â”œâ”€vde2            252:66   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdb osd çš„ rocksdb
â”œâ”€vde3            252:67   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdc osd çš„ wal
â”œâ”€vde4            252:68   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdc osd çš„ rocksdb
â”œâ”€vde5            252:69   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdd osd çš„ wal
â””â”€vde6            252:70   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdd osd çš„ rocksdb
...
# æ³¨æ„ï¼š
#   1. æ­¤å¤„å…ˆä¸è€ƒè™‘å•ä¸ª osd çš„æ•°æ®ã€wal ä¸ rocksdb ä¹‹é—´çš„å®¹é‡åˆ†é…æ¯”ï¼Œæ¼”ç¤ºå¦‚ä½•å°† /dev/vde åˆ†é…ç»™ 3 ä¸ª osd çš„ wal ä¸ rocksdbã€‚
#   2. ä»¥ä¸Šåˆ†åŒºéœ€é¢„å…ˆæ‰‹åŠ¨å®Œæˆåˆ†åŒºï¼ˆfdisk/gdisk/parted å‡å¯ï¼‰

[root@servere ~]# ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring
# å¯¼å‡º bootstrap-osd çš„ keyring æ–‡ä»¶ç”¨äºé›†ç¾¤ä¸­æ–°å¢ osd çš„è®¤è¯å‡­æ®
# æ³¨æ„ï¼š
#   è‹¥ä¸ç”Ÿæˆæ­¤ ceph.keyring æ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨ä½¿ç”¨ ceph-volume lvm create å‘½ä»¤æ·»åŠ  osd æ—¶æŠ¥é”™å¦‚ä¸‹ï¼š
#   stderr: 2023-01-01T01:29:07.360+0000 7febf169a700 -1 auth: unable to find a keyring on /var/lib/ceph/bootstrap-osd/ceph.keyring: (2) No such file or directory
#   stderr: 2023-01-01T01:29:07.360+0000 7febf169a700 -1 AuthRegistry(0x7febec05b3f8) no keyring found at /var/lib/ceph/bootstrap-osd/ceph.keyring, disabling cephx
#   ä»¥ä¸Šæç¤º ceph.keyring æ–‡ä»¶ä¸å­˜åœ¨å¯¼è‡´è®¤è¯å¤±è´¥ã€‚

[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdb --block.wal /dev/vde1 --block.db /dev/vde2
Running command: /usr/bin/ceph-authtool --gen-print-key
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 03e01019-a426-4190-b711-b9e3ad21dd5a
Running command: vgcreate --force --yes ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a /dev/vdb
 stdout: Physical volume "/dev/vdb" successfully created.
 stdout: Volume group "ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a" successfully created
Running command: lvcreate --yes -l 2559 -n osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a
 stdout: Logical volume "osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a" created.
Running command: /usr/bin/ceph-authtool --gen-print-key
Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-6
Running command: /usr/sbin/restorecon /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/chown -h ceph:ceph /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/ln -s /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-6/activate.monmap
 stderr: got monmap epoch 4
--> Creating keyring file for osd.6  #åˆ›å»º osd.6 çš„ keyring æ–‡ä»¶
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6/keyring
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6/
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 6 --monmap /var/lib/ceph/osd/ceph-6/activate.monmap --keyfile - --bluestore-block-wal-path /dev/vde1 --bluestore-block-db-path /dev/vde2 --osd-data /var/lib/ceph/osd/ceph-6/ --osd-uuid 03e01019-a426-4190-b711-b9e3ad21dd5a --setuser ceph --setgroup ceph
 stderr: 2022-12-31T21:25:21.005-0500 7f85c0737380 -1 bluestore(/var/lib/ceph/osd/ceph-6/) _read_fsid unparsable uuid
--> ceph-volume lvm prepare successful for: /dev/vdb  #æ·»åŠ  osd å‡†å¤‡é˜¶æ®µå®Œæˆ
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a --path /var/lib/ceph/osd/ceph-6 --no-mon-config
Running command: /usr/bin/ln -snf /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/ln -snf /dev/vde2 /var/lib/ceph/osd/ceph-6/block.db
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block.db
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/ln -snf /dev/vde1 /var/lib/ceph/osd/ceph-6/block.wal
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block.wal
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/systemctl enable ceph-volume@lvm-6-03e01019-a426-4190-b711-b9e3ad21dd5a
 stderr: Created symlink /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-6-03e01019-a426-4190-b711-b9e3ad21dd5a.service â†’ /usr/lib/systemd/system/ceph-volume@.service.
Running command: /usr/bin/systemctl enable --runtime ceph-osd@6
 stderr: Created symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@6.service â†’ /usr/lib/systemd/system/ceph-osd@.service.
Running command: /usr/bin/systemctl start ceph-osd@6
--> ceph-volume lvm activate successful for osd ID: 6
--> ceph-volume lvm create successful for: /dev/vdb
# osd åˆ›å»ºæ·»åŠ æ—¥å¿—ï¼š/dev/vdb ä½œä¸º osd çš„æ•°æ®ç›˜ï¼Œ/dev/vde1 ä½œä¸º osd çš„ walï¼Œ/dev/vde2 ä½œä¸º osd çš„ rocksdbã€‚
# æ­¤ osd ä½¿ç”¨ BlueStore å­˜å‚¨é©±åŠ¨ï¼Œ/dev/vdb åœ¨åç»­è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ›å»ºä¸ºé€»è¾‘å·ã€‚
[root@servere ~]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
...
vdb                                                                                                   252:16   0   10G  0 disk
â””â”€ceph--eb4fe887--63fa--4bdf--8bd4--1f0f9a4fbb2a-osd--block--03e01019--a426--4190--b711--b9e3ad21dd5a 253:0    0   10G  0 lvm
...
# osd.6 å¯¹åº”çš„é€»è¾‘å·å·²åˆ›å»º
[root@servere ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.07047  root default
-3         0.02939      host serverc
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
-5         0.02939      host serverd
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
-9         0.01169      host servere
 6    hdd  0.01169          osd.6         up   1.00000  1.00000  #osd.6 å·²åŠ å…¥é›†ç¾¤

[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdc --block.wal /dev/vde3 --block.db /dev/vde4
[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdd --block.wal /dev/vde5 --block.db /dev/vde6
[root@servere ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.09384  root default
-3         0.02939      host serverc
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
-5         0.02939      host serverd
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
-9         0.03506      host servere
 6    hdd  0.01169          osd.6         up   1.00000  1.00000
 7    hdd  0.01169          osd.7         up   1.00000  1.00000
 8    hdd  0.01169          osd.8         up   1.00000  1.00000
# 3 ä¸ª osd å·²å…¨éƒ¨åŠ å…¥é›†ç¾¤ä¸­

[root@servere ~]# ceph-volume lvm list
# æŸ¥çœ‹ osd ä¸ lv çš„å¯¹åº”å…³ç³»ï¼ˆå¿…é¡»åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šè¿è¡Œï¼‰

[root@servere ~]# ceph orch ps --daemon_type=osd
NAME   HOST                     STATUS        REFRESHED  AGE  PORTS  VERSION           IMAGE ID      CONTAINER ID
osd.0  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  214f946e935e
osd.1  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  169b37f63a95
osd.2  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  605c9c7817a7
osd.3  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  32495d4269b9
osd.4  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  31990acee22c
osd.5  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  0a0963f26a56
# ä½†è¯·æ³¨æ„ï¼Œç”±äºæ˜¯é€šè¿‡æ‰‹åŠ¨æ·»åŠ  osdï¼Œè¿™äº›æ·»åŠ çš„ osd ä¸è¢« ceph ç¼–æ’å™¨ç®¡ç†ï¼Œå› æ­¤ä¸åœ¨ä»¥ä¸Šè¿”å›çš„ osd åˆ—è¡¨ä¸­ï¼

[root@servere ~]# ls -lh /var/lib/ceph/osd/ceph-6/
total 28K
lrwxrwxrwx. 1 ceph ceph 93 Apr 18 11:22 block -> /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a  #æ•°æ®åˆ†åŒº
lrwxrwxrwx. 1 ceph ceph  9 Apr 18 11:22 block.db -> /dev/vde2   #rocksdb åˆ†åŒº
lrwxrwxrwx. 1 ceph ceph  9 Apr 18 11:22 block.wal -> /dev/vde1  #wal åˆ†åŒº
-rw-------. 1 ceph ceph 37 Apr 18 11:22 ceph_fsid
-rw-------. 1 ceph ceph 37 Apr 18 11:22 fsid
-rw-------. 1 ceph ceph 55 Apr 18 11:22 keyring
-rw-------. 1 ceph ceph  6 Apr 18 11:22 ready
-rw-------. 1 ceph ceph  3 Apr 18 11:22 require_osd_release
-rw-------. 1 ceph ceph 10 Apr 18 11:22 type
-rw-------. 1 ceph ceph  2 Apr 18 11:22 whoami
# æ³¨æ„ï¼šé€šè¿‡ ceph-volume å•ç‹¬æ·»åŠ çš„ osdï¼Œå…¶ç›®å½•ç»“æ„å­˜åœ¨å·®å¼‚ã€‚
[root@servere ~]# df -Th /var/lib/ceph/osd/*
Filesystem     Type   Size  Used Avail Use% Mounted on
tmpfs          tmpfs  2.9G   28K  2.9G   1% /var/lib/ceph/osd/ceph-6
tmpfs          tmpfs  2.9G   28K  2.9G   1% /var/lib/ceph/osd/ceph-7
tmpfs          tmpfs  2.9G   28K  2.9G   1% /var/lib/ceph/osd/ceph-8
# osd çš„é…ç½®ç›®å½•å‡è¢«æŒ‚è½½
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œç›´æ¥åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šä½¿ç”¨ ceph-volume å‘½ä»¤å¯æˆåŠŸæ·»åŠ  osdï¼Œä½†åœ¨ cephadm è¿è¡Œçš„å®¹å™¨ä¸­ä½¿ç”¨åŒæ ·çš„æ–¹æ³•æ— æ³•å®Œæˆ osd çš„æ·»åŠ ã€‚å› ä¸ºåœ¨æ·»åŠ  osd çš„è¿‡ç¨‹ä¸­ï¼Œå°†ä¼šä½¿ç”¨ `systemctl` ç®¡ç† osd æœåŠ¡çš„å®ˆæŠ¤è¿›ç¨‹ï¼Œéœ€è¿æ¥èŠ‚ç‚¹çš„ bus æ€»çº¿ï¼Œè€Œåœ¨å®¹å™¨ä¸­æ— æ³•å®Œæˆè€ŒæŠ¥é”™ã€‚è¿”å›å¦‚ä¸‹ï¼š

```bash
[ceph: root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdb --block.wal /dev/vde1 --block.db /dev/vde2
...
 stderr: Failed to connect to bus: No such file or directory
--> Was unable to complete a new OSD, will rollback changes
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd purge-new osd.6 --yes-i-really-mean-it
 stderr: purged osd.6
-->  RuntimeError: command returned non-zero exit status: 1
```

è‹¥ ceph-volume lvm create æ·»åŠ  osd å¤±è´¥ï¼Œä¼šä¿ç•™åˆ›å»ºçš„å·ç»„ä¸é€»è¾‘å·ï¼Œä»¥åŠ `/var/lib/ceph/osd/*` ä¸­çš„å¯¹åº”ç›®å½•ã€‚å› æ­¤ï¼Œå›é€€é‡æ–°éƒ¨ç½²çš„è¯ï¼Œéœ€å°†å…¶å…¨éƒ¨åˆ é™¤ï¼Œå¯å‚è€ƒå¦‚ä¸‹ï¼š

```bash
[root@servere ~]# vgs  #æŸ¥è¯¢åˆ›å»ºçš„å·ç»„
[root@servere ~]# vgremove -f <vg_name>  #åˆ é™¤å·²åˆ›å»ºçš„å·ç»„
[root@servere ~]# rm -rf /var/lib/ceph/osd/*  #åˆ é™¤å·²åˆ›å»ºçš„ osd ç›®å½•
# é‡æ–°æ‰§è¡Œåˆ›å»º osd çš„å‘½ä»¤å®Œæˆåˆ›å»º
```

â“ è¿›ä¸€æ­¥æ€è€ƒçš„é—®é¢˜ï¼š

- å¦‚ä½•åˆ†é… osdã€wal ä¸ rocksdb ä¹‹é—´çš„æ¯”ä¾‹åˆ†é…å…³ç³»ï¼Ÿ
  - osd æ•°æ®ç›˜
  - wal+rocksdb ç¼“å­˜ç›˜
- RocksDB æ•°æ®åº“çš„ç»“æ„ï¼Ÿå¦‚ä½•å¯¹å®ƒè¿›è¡Œæ·±åº¦è°ƒä¼˜ï¼Ÿ
- å¦‚ä½•ä½¿ç”¨ radosbench è¿›è¡Œ osd çš„ IO æµ‹è¯•ï¼Ÿ

## RADOS å¯¹è±¡æ“ä½œ

```bash
$ rados -p <pool_name> put <object_name> /path/to/file
# æŒ‡å®šæ–‡ä»¶å°†å…¶ä¸Šä¼ è‡³é›†ç¾¤ä¸­ä»¥æŒ‡å®šçš„åç§°å‘½å
$ rados -p <pool_name> ls
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­çš„å¯¹è±¡
```

## Ceph Pool å­˜å‚¨æ± 

### Ceph å­˜å‚¨æ± çŠ¶æ€

```bash
$ ceph osd lspools
$ ceph osd ls pool detail
# ä»¥ä¸Šä¸¤ä¸ªå‘½ä»¤ç”¨äºæŸ¥çœ‹é›†ç¾¤ä¸­çš„å­˜å‚¨æ± 

$ rados df
# RADOS å±‚é¢æŸ¥è¯¢é›†ç¾¤çŠ¶æ€
$ ceph df
# Ceph é›†ç¾¤ä¸å­˜å‚¨æ± çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
[root@serverc ~]# ceph df  #ç¤ºä¾‹
--- RAW STORAGE ---
CLASS    SIZE   AVAIL     USED  RAW USED  %RAW USED
hdd    90 GiB  90 GiB  463 MiB   463 MiB       0.50
TOTAL  90 GiB  90 GiB  463 MiB   463 MiB       0.50

--- POOLS ---
POOL                   ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
device_health_metrics   1    1      0 B        0      0 B      0     28 GiB
.rgw.root               2   32  1.3 KiB        4   48 KiB      0     28 GiB
default.rgw.log         3   32  3.6 KiB      209  408 KiB      0     28 GiB
default.rgw.control     4   32      0 B        8      0 B      0     28 GiB
default.rgw.meta        5    8      0 B        0      0 B      0     28 GiB
iscsigw-pool            6   32  2.3 MiB       11  7.0 MiB      0     28 GiB
mycephfs0_data          7   32      0 B        0      0 B      0     28 GiB
mycephfs0_metadata      8   32      0 B        0      0 B      0     28 GiB
mycephfs1_metadata      9   32      0 B        0      0 B      0     28 GiB
mycephfs1_data         10   32      0 B        0      0 B      0     28 GiB  #90/3 = 30GiB å·¦å³å®¹é‡
sizetest               11   32      0 B        0      0 B      0     42 GiB  #90/2 = 45GiB å·¦å³å®¹é‡
# æ³¨æ„ï¼šå­˜å‚¨æ± çš„å®¹é‡ä¸é›†ç¾¤çš„æ€»å®¹é‡åŠå­˜å‚¨æ± çš„å‰¯æœ¬æ•°æœ‰å…³ã€‚å¦‚ï¼Œé›†ç¾¤å®¹é‡ä¸º 90GiBï¼Œåˆ›å»ºçš„å­˜å‚¨æ± å‰¯æœ¬æ•°ä¸º 3ï¼Œ
# é‚£ä¹ˆï¼Œè¯¥å­˜å‚¨æ± çš„å®¹é‡åœ¨ 30GiB å·¦å³ï¼Œä¸å…¶ä»–å­˜å‚¨æ± çš„å®¹é‡æ— å…³ã€‚

$ ceph osd pool stats
# ç¡®è®¤æ‰€æœ‰å­˜å‚¨æ± çš„è¯»å†™äº‹ä»¶
[root@serverc ~]# ceph osd pool stats  #ç¤ºä¾‹
pool device_health_metrics id 1
  nothing is going on

pool .rgw.root id 2
  nothing is going on

pool default.rgw.log id 3
  nothing is going on

pool default.rgw.control id 4
  nothing is going on

pool default.rgw.meta id 5
  nothing is going on

pool iscsigw-pool id 6
  client io 2.7 KiB/s rd, 2 op/s rd, 0 op/s wr
...
```

### Ceph å¤åˆ¶æ± å‘½ä»¤

```bash
$ ceph osd pool create <pool_name> <pg_num> <pgp_num> [replicated] <crush_rule_set>
# åˆ›å»ºå¤åˆ¶æ± 
# æ³¨æ„ï¼š
#   1. é»˜è®¤æƒ…å†µä¸‹å¯åªæŒ‡å®šå­˜å‚¨æ± åç§°ä¸ PG æ•°é‡ï¼Œä¹Ÿå¯è‡ªå®šä¹‰ CRUSH æ”¾ç½®è§„åˆ™åœ¨åˆ›å»ºå­˜å‚¨æ± æ—¶æŒ‡å®šã€‚
#   2. è‹¥è®¾ç½®äº†ç›¸åŒçš„ pg_num ä¸ pgp_numï¼Œä¹‹åå†è°ƒæ•´ pg_num å°†è‡ªåŠ¨è°ƒæ•´ pgp_numã€‚

$ ceph osd pool application enable <pool_name> [rbd|rgw|cephfs]
# è®¾ç½®æŒ‡å®šçš„å­˜å‚¨æ± ä¸º rbdã€rgw æˆ– cephfs åº”ç”¨ç±»å‹

$ ceph osd pool get <pool_name> all
# æŸ¥çœ‹å­˜å‚¨æ± çš„æ‰€æœ‰å‚æ•°å€¼
$ ceph osd pool get <pool_name> [size|nodelete|min_size]
# æŸ¥çœ‹å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼
$ ceph osd pool get <pool_name> pg_autoscale_mode
# æŸ¥çœ‹å­˜å‚¨æ± çš„ PG è‡ªåŠ¨æ‰©å±•æ¨¡å¼
$ ceph osd pool get mon osd_pool_default_size
# æŸ¥çœ‹é›†ä¸­é…ç½®æ•°æ®åº“ä¸­å®šä¹‰çš„é»˜è®¤å¯¹è±¡å‰¯æœ¬æ•°é‡

$ ceph osd pool set <pool_name> [size|nodelete|min_size] <value>
# è®¾ç½®å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼
$ ceph osd pool set <pool_name> pg_autoscale_mode on
# å¯ç”¨ mgr çš„ pg_autoscaler æ¨¡å—å¹¶å¯ç”¨æŒ‡å®šå­˜å‚¨æ± çš„è‡ªåŠ¨æ‰©å±• pg åŠŸèƒ½

$ ceph osd pool rename <old-pool_name> <new-pool_name>
# é‡å‘½åå­˜å‚¨æ± åç§°ï¼ˆä¸å½±å“æ± ä¸­å­˜å‚¨çš„æ•°æ®ï¼Œä½†éœ€è¦æ³¨æ„ç”¨æˆ·å¯¹æ± çš„æƒé™ï¼‰
```

### Ceph çº åˆ ä»£ç æ± å‘½ä»¤

```bash
$ ceph osd erasure-code-profile ls
# æŸ¥çœ‹çº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile get <profile_name>
# æŸ¥çœ‹æŒ‡å®šçº åˆ ä»£ç æ± çš„ profile å…·ä½“ä¿¡æ¯
$ ceph osd erasure-code-profile set <profile_name> k=<num> m=<num> crush-failure-domain=[osd|host|rack]
# åˆ›å»ºçº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile set myerasure k=3 m=2 crush-failure-domain=rack  #ç¤ºä¾‹
# rack ç±»å‹çš„æ•…éšœæ¢å¤åŸŸå¿…é¡»æå‰åœ¨ CRUSH map ä¸­æ„å»º
$ ceph osd erasure-code-profile rm <profile_name>
# åˆ é™¤æŒ‡å®šçš„çº åˆ ä»£ç æ± çš„ profile
$ ceph osd pool create <pool_name> <pg_num> <pgp_num> erasure <profile_name> <crush_rule_set>
# åˆ›å»ºçº åˆ ä»£ç æ± 
```

å…³äºçº åˆ ä»£ç  profile çš„è®¾ç½® RedHat ç»™å‡ºä»¥ä¸‹æ¨èæ–¹å¼ï¼š

<center><img src="images/redhat-reasure-code-profile-recommanded.jpg" style="width:80%"></center>

## CephX è®¤è¯ä¸ç”¨æˆ·

### CephX è®¤è¯æœºåˆ¶

<center><img src="images/cephx-request-1.jpg""></center>

Ceph å®¢æˆ·ç«¯ä½¿ç”¨ CephX åè®®å‘ monitor å‘é€ç”¨æˆ·åˆ›å»ºè¯·æ±‚ï¼Œå½“ monitor åˆ›å»ºç”¨æˆ·åå°†å­˜å‚¨ç”¨æˆ·åã€keyring ä¸ capability ç­‰ä¿¡æ¯ï¼Œå¹¶å°†ç›¸åŒçš„ keyring æ–‡ä»¶è¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå…¶ä¸­ keyring æ–‡ä»¶ä¸­çš„ key ä¸º `secret key`ï¼Œç”¨äºåŠ å¯†ä¸è§£å¯† monitor ç”Ÿæˆçš„ä¼šè¯å¯†é’¥ï¼ˆ`session key`ï¼‰ã€‚

<center><img src="images/cephx-request-2.jpg"></center>

<center><img src="images/cephx-request-4.png"></center>

æ¯ä¸ª monitor éƒ½å¯ä»¥å¯¹å®¢æˆ·ç«¯è¿›è¡Œèº«ä»½éªŒè¯å¹¶åˆ†å‘å¯†é’¥ï¼Œè¿™æ„å‘³ç€è®¤è¯ä¾é  monitor èŠ‚ç‚¹å®Œæˆï¼Œä¸ä¼šå­˜åœ¨å•ç‚¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚monitor ä¼šè¿”å›ç”¨äºèº«ä»½éªŒè¯çš„æ•°æ®ç»“æ„ï¼Œå…¶ä¸­åŒ…å«è·å– Ceph æœåŠ¡æ—¶ç”¨åˆ°çš„ session keyã€‚æ‰€è°“ session key å°±æ˜¯å®¢æˆ·ç«¯ç”¨æ¥å‘ monitor è¯·æ±‚æ‰€éœ€æœåŠ¡çš„å‡­è¯ï¼Œ`session key` æ˜¯é€šè¿‡å®¢æˆ·ç«¯çš„ `secret key` è¿›è¡ŒåŠ å¯†ä¼ è¾“ã€‚

ğŸš€ å½“ monitor æ”¶åˆ°å®¢æˆ·ç«¯è®¤è¯è¯·æ±‚åï¼Œé¦–å…ˆç”Ÿæˆ session keyï¼Œç„¶åç”¨å®¢æˆ·ç«¯çš„ secret key åŠ å¯†session keyï¼Œå†å‘é€ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯ç”¨è‡ªèº«çš„ secret key å°†å…¶è§£å¯†ï¼Œæ‹¿åˆ° session keyã€‚å®¢æˆ·ç«¯è·å– session key ä¹‹åï¼Œå®ƒå°±å¯ä»¥ç”¨æ­¤ session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œmonitor æ”¶åˆ°å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼ˆæºå¸¦ session keyï¼‰ï¼Œæ­¤æ—¶ monitor ä¼šå‘å®¢æˆ·ç«¯æä¾›ä¸€ä¸ª `ticket`ï¼ˆç¥¨æ®ï¼‰ï¼Œç„¶åä½¿ç”¨ session key åŠ å¯†åå‘é€ç»™å®¢æˆ·ç«¯ã€‚éšåå®¢æˆ·ç«¯ä½¿ç”¨ session key è§£å¯†æ­¤ç¥¨æ®ï¼Œä½¿ç”¨æ­¤ç¥¨æ®åˆ°å¯¹åº” OSD å®Œæˆè®¤è¯ã€‚

ä»¥ä¸Šè¿‡ç¨‹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¦–å…ˆï¼Œå®¢æˆ·ç«¯çš„ secret key æ˜¯é€šè¿‡ monitor èŠ‚ç‚¹åœ¨åˆ›å»ºç”¨æˆ·å¸å·æ—¶å°±ç”Ÿæˆï¼Œæ‰€ä»¥ monitor èŠ‚ç‚¹æœ‰å¯¹åº”å®¢æˆ·ç«¯çš„ secret keyï¼Œé€šè¿‡å®¢æˆ·ç«¯çš„ secret key åŠ å¯†ï¼Œå®¢æˆ·ç«¯å¯ä»¥ç”¨è‡ªèº«çš„ secret key è§£å¯†ã€‚å…¶æ¬¡ï¼Œmonitor èŠ‚ç‚¹ç”Ÿæˆçš„ session key æ˜¯æœ‰è®°å½•çš„ï¼Œæ‰€ä»¥å¯¹äºä¸åŒå®¢æˆ·ç«¯æ¥è¯´ï¼Œéƒ½æœ‰ä¸åŒçš„è®°å½•ï¼Œå¹¶ä¸”æ­¤ session key æ˜¯æœ‰æ—¶é—´é™åˆ¶çš„ï¼Œè¿‡æœŸå³ä¾¿æ˜¯å¯¹åº”å®¢æˆ·ç«¯ï¼Œä¹Ÿæ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚æ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨å¯¹åº” session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œå¯¹åº” monitor éƒ½æ˜¯è®¤å¯çš„ï¼Œmonitor ä¼šå‘å…¶å‘æ”¾ ticketã€‚æœ€åï¼Œmonitor å’Œ OSD éƒ½å…±äº«å®¢æˆ·ç«¯çš„ secret key å’Œ session keyï¼Œä»¥åŠ monitor å‘æ”¾çš„ ticketï¼Œæ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨ monitor å‘æ”¾çš„ ticketï¼Œå¯¹åº” OSD æ˜¯è®¤å¯çš„ã€‚è¿™ä¹Ÿæ„å‘³ç€ä¸ç®¡æ˜¯å“ªä¸ª monitor èŠ‚ç‚¹å‘æ”¾çš„ ticketï¼Œå¯¹åº”æ‰€æœ‰ monitor èŠ‚ç‚¹å’Œ OSD éƒ½å¯è®¤è¯ã€‚

<center><img src="images/cephx-request-3.png"></center>

ğŸ· å…³äº CephX è®¤è¯æœºåˆ¶æ›´å¤šå¯å‚è€ƒ [HIGH AVAILABILITY AUTHENTICATION](https://docs.ceph.com/en/latest/architecture/#high-availabilityauthentication) ä¸­çš„è¯´æ˜ã€‚

### CephX ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤

```bash
$ ceph auth get-or-create [--id <name>|--name client.<name>] client.<name> \
  mon 'allow r' osd 'allow rw' \
  -o /path/to/ceph.client.<name>.keyring
# ä½¿ç”¨æŒ‡å®šçš„ Ceph ç”¨æˆ·åˆ›å»ºæ–°ç”¨æˆ·å¹¶å°†å…¶ secret key æ³¨å…¥ keyring æ–‡ä»¶
# ä¹Ÿå¯ä½¿ç”¨ --keyring é€‰é¡¹æŒ‡å®š keyring æ–‡ä»¶ç”¨äºåˆ›å»ºæ–°ç”¨æˆ·

### é™åˆ¶ç”¨æˆ·çš„è®¿é—® ###
$ ceph auth get-or-create client.<name> \
  mon 'profile rbd' osd 'profile rbd pool=<pool_name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå­˜å‚¨æ± ä¸­çš„ RBD é•œåƒ
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw object_prefix <prefix-name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå‰ç¼€çš„å¯¹è±¡
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw namespace=<namespace>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®š namespace ä¸­çš„å¯¹è±¡
$ ceph fs authorize cephfs client.<name> /path/to/cephfs rw
# é™åˆ¶ç”¨æˆ·åªèƒ½è¯»å†™ CephFS ä¸­çš„æŒ‡å®šè·¯å¾„
$ ceph auth get-or-create client.<name> \
  mon 'allow r, allow command "auth get-or-create", allow command "auth list"'
# é€šè¿‡ monitor å‘½ä»¤é™åˆ¶ç”¨æˆ·åªèƒ½è¿è¡ŒæŒ‡å®šçš„ ceph å‘½ä»¤

$ ceph auth list
# åˆ—å‡ºæ‰€æœ‰çš„ Ceph ç”¨æˆ·
$ ceph auth get client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
$ ceph auth print-key client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„ secret key
$ ceph auth export client.<name> > /path/to/keyring
# å¯¼å‡ºæŒ‡å®šç”¨æˆ·çš„ keyring è‡³æ–‡ä»¶
$ ceph auth import -i /path/to/keyring
# æŒ‡å®šç”¨æˆ·çš„ keyring å¯¼å…¥é›†ç¾¤

$ ceph auth caps client.<name> mon 'allow r' osd 'allow rw'
# è¦†ç›–æ›´æ–°å½“å‰æŒ‡å®šç”¨æˆ·çš„ç°æœ‰åŠŸèƒ½
$ ceph auth caps client.<name> osd ''
# ä½¿ç”¨ç©ºå­—ç¬¦åˆ é™¤æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰åŠŸèƒ½

$ ceph auth del client.<name>
# åˆ é™¤é›†ä¸­é…ç½®æ•°æ®åº“ä¸­çš„æŒ‡å®šç”¨æˆ·
$ rm -f /path/to/keyring
# åˆ é™¤æŒ‡å®šç”¨æˆ·çš„ keyring æ–‡ä»¶ï¼ˆè‹¥å­˜åœ¨çš„è¯ï¼‰
```

## Ceph RBD é•œåƒ

### RBD é•œåƒçš„ç‰¹æ€§ï¼ˆfeatureï¼‰

- `layering`ï¼šæ˜¯å¦æ”¯æŒé•œåƒåˆ†å±‚ã€å…‹éš†ï¼ˆBIT ç ä¸º 1ï¼‰
- `exclusive-lock`ï¼šæ˜¯å¦æ”¯æŒåˆ†å¸ƒå¼ç‹¬å é”æœºåˆ¶ä»¥é™åˆ¶åŒæ—¶ä»…èƒ½ä¸€ä¸ªå®¢æˆ·ç«¯è®¿é—®å½“å‰é•œåƒï¼ˆBIT ç ä¸º 4ï¼‰
- `object-map`ï¼šæ˜¯å¦æ”¯æŒå¯¹è±¡æ˜ å°„ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿå¯¼å…¥ã€å¯¼å‡ºåŠå·²ç”¨å®¹é‡ç»Ÿè®¡ç­‰æ“ä½œï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 8ï¼‰ã€‚
- `fast-diff`ï¼šæ˜¯å¦æ”¯æŒå¿«ç…§é—´çš„å¿«é€Ÿæ¯”è¾ƒæ“ä½œï¼Œä¾èµ–äº object-map ç‰¹æ€§ï¼ˆBIT ç ä¸º 16ï¼‰ã€‚
- `deep-flatten`ï¼šæ˜¯å¦æ”¯æŒå…‹éš†åˆ†ç¦»æ—¶è§£é™¤åœ¨å…‹éš†é•œåƒæ—¶åˆ›å»ºçš„å¿«ç…§ä¸å…¶çˆ¶é•œåƒä¹‹é—´çš„å…³è”ï¼Œå³å¿«ç…§æ‰å¹³åŒ–æ“ä½œï¼ˆBIT ç ä¸º 32ï¼‰ã€‚
- `journaling`ï¼šæ˜¯å¦æ”¯æŒæ—¥å¿— I/O æ“ä½œï¼Œå³æ˜¯å¦æ”¯æŒè®°å½•é•œåƒçš„ä¿®æ”¹æ“ä½œè‡³æ—¥å¿—å¯¹è±¡ï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 64ï¼‰ã€‚
- `data-pool`ï¼šæ˜¯å¦æ”¯æŒå°†é•œåƒçš„æ•°æ®å¯¹è±¡å­˜å‚¨äºçº åˆ ç å­˜å‚¨æ± ï¼Œä¸»è¦ç”¨äºå°†é•œåƒçš„å…ƒæ•°æ®ä¸æ•°æ®æ”¾ç½®äºä¸åŒçš„å­˜å‚¨æ± ã€‚
- `striping`: æ˜¯å¦æ”¯æŒæ•°æ®å¯¹è±¡é—´çš„æ•°æ®æ¡å¸¦åŒ–ï¼ˆBIT ç ä¸º 2ï¼‰

### ç®¡ç† RBD é•œåƒï¼šåˆ›å»ºã€åˆ é™¤ã€æŸ¥è¯¢ã€æ˜ å°„ã€å¿«ç…§ã€å…‹éš†ã€æ‰å¹³åŒ–
  
```bash
$ rbd pool init <pool_name>
# åˆå§‹åŒ–æŒ‡å®šå­˜å‚¨æ± å­˜å‚¨ RBD é•œåƒ
$ rbd create [--id <name>] \
  --size <num>[M|G] [--object-size <num>[M|G]] \
  --image-feature=<feature1>,<feature2>,... \
  #--image-feature=exclusive-lock,journaling \  #ç¤ºä¾‹
  <pool_name>/<rbd_image_name>
# æŒ‡å®š RBD é•œåƒå¤§å°ï¼ˆé»˜è®¤å•ä½ MiBï¼‰ã€chunk çš„å¯¹è±¡å¤§å°ä»¥åŠæ”¯æŒçš„é•œåƒç‰¹æ€§ä»¥åˆ›å»º RBD é•œåƒ
# æ³¨æ„ï¼š
#   1. å…¶ä¸­ layering, exclusive-lock, object-map, fast-diff, deep-flatten ä¸º
#      é»˜è®¤æ”¯æŒçš„é•œåƒç‰¹æ€§
#   2. è‹¥åªéœ€å¯ç”¨ç‰¹å®šçš„é•œåƒç‰¹æ€§å¯ä½¿ç”¨ --image-feature é€‰é¡¹æŒ‡å®šå³å¯
#   3. ä»¥ä¸Šç¤ºä¾‹ä¸­çš„ exclusive-lock ä¸ journaling ç‰¹æ€§åœ¨ RBD Mirror ä¸­å¯ç”¨
  
$ rbd rm [--id <name>] <pool_name>/<rbd_image_name>
$ rbd ls [--id <name>] --pool <pool_name>
$ rbd info [--id <name>] <pool_name>/<rbd_image_name>
$ rbd resize --size <number>[M|G] <pool_name>/<rbd_image_name>
  
$ rbd map [--id <name>] <pool_name>/<rbd_image_name>
$ rbd unmap /dev/rbdX
$ rbd showmapped
# æ³¨æ„ï¼š
#   1. RBD é•œåƒçš„æ˜ å°„äºå®¢æˆ·ç«¯ä¸Šæ‰§è¡Œ
#   2. å®¢æˆ·ç«¯éœ€å®‰è£… ceph-common è½¯ä»¶åŒ…å¹¶åŠ è½½ rbd å†…æ ¸æ¨¡å—
  
$ fsfreeze --freeze /path/to/<mount_point>
$ fsfreeze --unfreeze /path/to/<mount_point>
$ rbd snap create [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd snap remove [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd snap list [--id <name>] <pool_name>/<rbd_image_name>
$ rbd snap protect [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd clone [--id <name>] <pool_name>/<rbd_image_name>@<snap-name> <pool_name>/<clone_name>
# åˆ›å»ºåŸºäº RBD é•œåƒå¿«ç…§çš„å…‹éš†
$ rbd flatten <rbd-image-clone_name>
# åˆ›å»ºæ‰å¹³åŒ–å…‹éš†
  
$ blockdev --getro /path/to/device
```

### Ceph RBD Mirror é›†ç¾¤æ¨¡å¼
  
RBD Mirror çš„ä¸¤ç§æ¨¡å¼ï¼ŒåŒ…æ‹¬ `RBD one-way mirroring` æ¨¡å¼ï¼ˆ`active-backup mode`ï¼‰ã€`RBD two-way mirroring` æ¨¡å¼ï¼ˆ`active-active mode`ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
<center><img src="images/rbd-one-way-mirroring.jpg" style="width:60%"></center>
  
<center><img src="images/rbd-two-way-mirroring.jpg" style="width:60%"></center>
  
### Lab: å®ç° RBD one-way mirroring
  
```bash
### RBD Mirror Primary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰ ###
$ rbd mirror pool enable <pool_name> <mirror-mode>
# æŒ‡å®šå­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„æŒ‡å®šæ¨¡å¼ï¼ˆpool æ¨¡å¼ã€image æ¨¡å¼ï¼‰
$ rbd mirror pool enable rbd pool  #ç¤ºä¾‹
# å¯ç”¨ rbd å­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„ pool æ¨¡å¼ï¼ˆæ± æ¨¡å¼ï¼‰
$ rbd info rbd/image1  #ç¤ºä¾‹
  rbd image 'image1':
        ...
        features: exclusive-lock, journaling
        ...
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: cccdeb7c-1ce5-4ca4-9498-1400f228bf72
        mirroring primary: true
# æ­¤ç¤ºä¾‹æ˜¾ç¤º rbd å­˜å‚¨æ± å·²å¯ç”¨ pool æ¨¡å¼ï¼Œå¹¶ä¸”å…¶ä¸­çš„ image1 é•œåƒå¯è¿›è¡ŒåŒæ­¥ã€‚
  
$ rbd mirror pool peer bootstrap create \
  --site-name <site-name> <pool_name> > /path/to/mirror-bootstrap-token
# åˆ›å»ºå¼•å¯¼å¯¹ç­‰å­˜å‚¨é›†ç¾¤çš„ bootstrap tokenï¼ˆactive-passive é›†ç¾¤æ¨¡å¼ï¼‰
  
### RBD Mirror Secondary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰ ###
$ ceph orch apply rbd-mirror --placement=<fqdn>
# æŒ‡å®šèŠ‚ç‚¹å¯ç”¨ rbd-mirror å®ˆæŠ¤è¿›ç¨‹
$ ceph orch apply rbd-mirror --placement=serverf.lab.example.com  #ç¤ºä¾‹
$ ceph orch ls  #ç¤ºä¾‹
  ...
  rbd-mirror                   1/1  8m ago     2h   serverf.lab.example.com
  ...
$ rbd mirror pool peer bootstrap import \
  --site-name <site-name> \
  --direction rx-only <pool_name> \
  /path/to/mirror-bootstrap-token
# å¯¼å…¥ primary é›†ç¾¤æä¾›çš„å¯¹ç­‰é›†ç¾¤å¼•å¯¼ tokenï¼Œå°†è®¾ç½®ä¸º secondary é›†ç¾¤ã€‚
# æ­¤é›†ç¾¤åªèƒ½ä»¥æ¥æ”¶æ–¹å¼ï¼ˆrx-onlyï¼‰å¤‡ä»½åŒæ­¥æŒ‡å®šå­˜å‚¨æ± ä¸­çš„é•œåƒ
  
$ rbd mirror pool info <pool_name>
# æŸ¥çœ‹ RBD Mirror æŒ‡å®šå­˜å‚¨æ± çš„å¯¹ç­‰ä¿¡æ¯
$ rbd mirror pool info rbd  #ç¤ºä¾‹
  Mode: pool
  Site Name: bup
  
  peer Sites:
  
  UUID: 763de4dc-ba66-4672-aaec-582b68cf9cf1
  Name: prod
  Direction: rx-only
  Client: client.rbd-mirror-peer
$ rbd mirror pool status
  health: OK
  daemon health: OK
  image health: OK
  images: 1 total
      1 replaying
# æŸ¥çœ‹ RBD Mirror çš„å¯¹ç­‰çŠ¶æ€
# æ³¨æ„ï¼šprimary é›†ç¾¤ä¸­æ— æ³•æŸ¥è¯¢ RBD é•œåƒåŒæ­¥çŠ¶æ€
  
### æŠ¥é”™ç¤ºä¾‹ ###
# RBD Mirror Secondary é›†ç¾¤
$ rbd rm rbd/image1
  2023-10-07T05:14:13.113+0000 7f6c226f3700 -1 librbd::image::PreRemoveRequest: 0x564199439410 handle_exclusive_lock:
  cannot obtain exclusive lock - not removing
  Removing image: 0% complete...failed.
  rbd: error: image still has watchers
  This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting
  30s for the crashed client to timeout.
# ç”±äº primary é›†ç¾¤å­˜å‚¨æ± ä¸­é•œåƒè®¾ç½®äº†åˆ†å¸ƒå¼é”ï¼ˆexclusive-lockï¼‰ç‰¹æ€§ï¼Œ
# å› æ­¤ï¼Œsecondary é›†ç¾¤ä¸­æ— æ³•åˆ é™¤é•œåƒï¼Œåªèƒ½ä» primary é›†ç¾¤ä¸­åˆ é™¤ã€‚
```
  
### Lab: rbd-nbd æ˜ å°„ä½¿ç”¨å·² mirroring çš„ RBD é•œåƒ
  
ç¬”è€…ç¯å¢ƒä¸­å·²éƒ¨ç½² RBD Mirror Primary ä¸ Secondary é›†ç¾¤ï¼Œä¸¤å¥—é›†ç¾¤ä¸­å‡å·²åˆ›å»º rbd å­˜å‚¨æ± å¹¶å¯ç”¨ pool æ¨¡å¼çš„ RBD Mirrorã€‚primary é›†ç¾¤çš„ rbd å­˜å‚¨æ± ä¸­å·²åˆ›å»º image1 é•œåƒï¼Œæ­¤é•œåƒå·²å¯ç”¨ `exclusive-lock` ä¸ `journaling` ç‰¹æ€§ã€‚ç°å°†æ­¤é•œåƒæ˜ å°„ç»™å®¢æˆ·ç«¯è™šæ‹Ÿæœºä½œè™šæ‹Ÿç£ç›˜ä½¿ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
```bash
### å®¢æˆ·ç«¯è™šæ‹Ÿæœº ###
$ sudo dnf install -y ceph-common
# å®‰è£… rbd å‘½ä»¤
$ sudo rbd map rbd/image1
  rbd: sysfs write failed
  RBD image feature set mismatch. You can disable features unsupported by
  the kernel with "rbd feature disable image1 journaling"
  In some cases useful info is found in syslog - try "dmesg | tail".
  rbd: map failed: (6) No such device or address
# rbd æ˜ å°„ rbd/image1 é•œåƒå†…æ ¸æ¨¡å—æŠ¥é”™
# æ³¨æ„ï¼š
#   1. ä½¿ç”¨æ­¤å‘½ä»¤å‰éœ€åŒæ­¥ primary é›†ç¾¤çš„ /etc/ceph/{ceph.conf,
#      ceph.client.admin.keyring} æ–‡ä»¶è‡³æœ¬åœ° /etc/ceph/ ç›®å½•ä¸­
#   2. rbd å†…æ ¸æ¨¡å—æŠ¥é”™æ— æ³•æ˜ å°„æ˜¯ç”±äºå½“å‰å†…æ ¸ä¸æ”¯æŒ journaling ç‰¹æ€§ã€‚
#      ä½†æ˜¯ï¼Œmirroring å¿…é¡»ä¾èµ–æ­¤ç‰¹æ€§ï¼Œå› æ­¤ä½¿ç”¨ rbd æ˜ å°„ä¸ journaling ä¹‹é—´å­˜åœ¨çŸ›ç›¾ã€‚
#      ç›®å‰ç°æœ‰çš„å†…æ ¸ç‰ˆæœ¬å‡ä¸æ”¯æŒæ­¤ç‰¹æ€§ï¼
#   3. å¯ä½¿ç”¨ rbd-nbd æ›¿ä»£ rbd ä»¥å®Œæˆé•œåƒçš„æ˜ å°„ã€‚å¯å‚çœ‹æ–‡æœ«å‚è€ƒé“¾æ¥ã€‚
$ sudo uname -r
  4.18.0-305.el8.x86_64
  
$ sudo dnf install -y rbd-nbd
# å®‰è£… rbd-nbd è½¯ä»¶åŒ…
$ sudo dnf info rbd-nbd
  ...
  Name         : rbd-nbd
  Epoch        : 2
  Version      : 16.2.0
  Release      : 117.el8cp
  Architecture : x86_64
  Size         : 511 k
  Source       : ceph-16.2.0-117.el8cp.src.rpm
  Repository   : @System
  From repo    : rhceph-5-tools-for-rhel-8-x86_64-rpms
  Summary      : Ceph RBD client base on NBD
  URL          : http://ceph.com/
  License      : LGPL-2.1 and LGPL-3.0 and CC-BY-SA-3.0 and GPL-2.0 and
                 BSL-1.0 and BSD-3-Clause and MIT
  Description  : NBD based client to map Ceph rbd images to local device
$ sudo lsmod | egrep 'rbd|nbd'
  nbd                    49152  2
  rbd                   110592  0
  libceph               454656  1 rbd
# ç¡®è®¤ rbd ä¸ nbd å†…æ ¸æ¨¡å—æ˜¯å¦åŠ è½½
$ sudo rbd-nbd map rbd/image1
  /dev/nbd1
# æ˜ å°„ rbd/image1 é•œåƒä¸º /dev/nbd1
$ sudo mkfs -t ext4 /dev/nbd1
$ sudo mkdir /mnt/rbd
$ sudo mount -t ext4 /dev/nbd1 /mnt/rbd
$ sudo dd if=/dev/zero of=/mnt/rbd/test-data1 oflag=direct bs=4M count=10
  10+0 records in
  10+0 records out
  41943040 bytes (42 MB, 40 MiB) copied, 0.747552 s, 56.1 MB/s
$ sudo ls -lh /mnt/rbd/test-data1
  -rw-r--r--. 1 root root 40M Oct  7 23:05 /mnt/rbd/test-data1
# åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
  
### RBD Mirror Primary/Secondary é›†ç¾¤ ###
$ rbd info rbd/image1
  rbd image 'image1':
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 8569a03bc09a
        block_name_prefix: rbd_data.8569a03bc09a
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sat Oct  7 17:41:54 2023
        access_timestamp: Sun Oct  8 03:05:02 2023
        modify_timestamp: Sun Oct  8 03:05:02 2023  # é•œåƒä¸­å·²å†™å…¥æ•°æ®
        journal: 8569a03bc09a
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: c28aca80-f176-49d0-9a7c-0f34f2380f51
        mirroring primary: true
# ä»ä»¥ä¸Š primary ä¸ secondary é›†ç¾¤çš„é•œåƒä¿¡æ¯å¯è§æ•°æ®å·²å†™å…¥å¹¶åŒæ­¥
```
  
### å…¶ä»– RBD Mirror ç›¸å…³å‘½ä»¤
  
```bash
$ rbd mirror pool disable <pool_name>
# æŒ‡å®šå­˜å‚¨æ± ç¦ç”¨ pool æ¨¡å¼çš„ RBD Mirror
  
$ rbd feature enable <pool_name>/<rbd_image_name> <feature-name>
# å¯ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
$ rbd feature disable <pool_name>/<rbd_image_name> <feature-name>
# ç¦ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
```
  
![rbd-mirror-other-cmds](images/rbd-mirror-other-cmds.png)
  
### RBD Mirror çš„æ•…éšœè½¬ç§»
  
å½“ primary é›†ç¾¤ä¸­çš„ RBD é•œåƒå˜ä¸ºä¸å¯ç”¨æ—¶ï¼ˆéé›†ç¾¤ä¸å¯ç”¨ï¼‰ï¼Œå¯å¯¹ primary é›†ç¾¤æ‰§è¡Œé•œåƒçš„é™çº§ï¼ˆdemoteï¼‰æ“ä½œï¼Œè€Œå¯¹ secondary é›†ç¾¤æ‰§è¡Œé•œåƒçš„å‡çº§ï¼ˆpromoteï¼‰æ“ä½œã€‚
  
```bash
$ rbd mirror image demote <pool_name>/<rbd_image_name>
  Image demoted to non-primary
# primary é›†ç¾¤èŠ‚ç‚¹ï¼šé™çº§æ‰§è¡ŒæŒ‡å®šçš„ RBD é•œåƒ
$ rbd mirror image promote <pool_name>/<rbd_image_name>
  Image promoted to primary
# secondary é›†ç¾¤èŠ‚ç‚¹ï¼šå‡çº§æŒ‡å®šçš„ RBD é•œåƒ
```

### Lab: éƒ¨ç½² Ceph iSCSI Gateway ä¸åˆ›å»º target

ä½¿ç”¨ [ceph-navigator](https://github.com/Alberthua-Perl/sc-col/blob/master/redhat-ceph-conf/v5.0/ceph-navigator) è„šæœ¬å¹¶è¾“å…¥ `6`ï¼Œå³å¯è‡ªåŠ¨å®‰è£…éƒ¨ç½² iscsi-gatewayï¼Œæ­¤æœåŠ¡å¯åŠ¨éœ€è¦ä¸€å®šæ—¶é—´ï¼Œå¯å‚è€ƒä»¥ä¸‹å‘½ä»¤æŸ¥è¯¢æœåŠ¡çš„çŠ¶æ€ï¼š

```bash
[root@serverc ~]# ceph orch ls | grep iscsi
iscsi.my_iscsi_driver        2/2  89s ago    22m  serverc.lab.example.com;servere.lab.example.com
[root@serverc ~]# ceph orch ps | grep iscsi
iscsi.my_iscsi_driver.serverc.musewn  serverc.lab.example.com  running (22m)  97s ago    22m  -              3.5               2142b60d7974  0871e5889d26  
iscsi.my_iscsi_driver.servere.gsrlcj  servere.lab.example.com  running (22m)  98s ago    22m  -              3.5               2142b60d7974  349595a1a137
# iscsi-gateway æœåŠ¡åœ¨ serverc ä¸ servere ä¸Šå·²éƒ¨ç½²æˆåŠŸ
[root@serverc ~]# rbd create --size 1024M iscsigw-pool/rbd-share0
# åˆ›å»º iSCSI å…±äº«çš„ RBD é•œåƒ
[root@serverc ~]# rbd info iscsigw-pool/rbd-share0
```

ç™»å½• Ceph Dashboard æŸ¥çœ‹å·²éƒ¨ç½²çš„ iscsi-gateway çš„çŠ¶æ€ï¼Œå¹¶åˆ›å»º targetã€‚

![ceph-iscsi-gateway-1](images/ceph-iscsi-gateway-1.png)

![ceph-iscsi-gateway-2](images/ceph-iscsi-gateway-2.png)

![ceph-iscsi-gateway-3](images/ceph-iscsi-gateway-3.png)

![ceph-iscsi-gateway-4](images/ceph-iscsi-gateway-4.png)

![ceph-iscsi-gateway-5](images/ceph-iscsi-gateway-5.png)

```bash
[root@serverc ~]# ceph -s
  cluster:
    id:     0b870ad2-8968-11ed-868b-52540000fa0c
    health: HEALTH_WARN
            2 stray daemon(s) not managed by cephadm

  services:
    mon:         4 daemons, quorum serverc.lab.example.com,clienta,serverd,servere (age 16h)
    mgr:         serverc.lab.example.com.pqcbzl(active, since 16h), standbys: serverd.xlpfgs, servere.heelmi, clienta.jfnnze
    osd:         9 osds: 9 up (since 16h), 9 in (since 2y)
    rgw:         2 daemons active (2 hosts, 1 zones)
    tcmu-runner: 2 portals active (2 hosts)  #target ä¸­çš„ portal å·²å¯ç”¨

  data:
    pools:   6 pools, 137 pgs
    objects: 232 objects, 6.2 MiB
    usage:   385 MiB used, 90 GiB / 90 GiB avail
    pgs:     137 active+clean

  io:
    client:   1.7 KiB/s rd, 1 op/s rd, 0 op/s wr
# Ceph Dashboard ä¸­åˆ›å»ºå®Œæˆ target çš„ portal åï¼Œå†æ¬¡ä½¿ç”¨ ceph -s æŸ¥çœ‹é›†ç¾¤çŠ¶æ€å°†è¿”å› tcmu-runner çš„çŠ¶æ€ä¿¡æ¯ã€‚
# è‹¥å°† portal åˆ é™¤ï¼Œåˆ™ tcmu-runner ä¸å†æ˜¾ç¤ºã€‚
```

### Lab: iscsi-initiator å®¢æˆ·ç«¯ç™»å½•è®¤è¯ iscsi-gateway å®ç°å¤šè·¯å¾„è®¿é—®

ä»¥ä¸Š Lab ä¸­æˆåŠŸåˆ›å»º target åï¼Œå¯åœ¨é›†ç¾¤å¤–å®¢æˆ·ç«¯éƒ¨ç½² iscsi-initiatorï¼Œåˆ©ç”¨ iscsi-gateway å®Œæˆ RBD é•œåƒè‡³æœ¬åœ°çš„å¤šè·¯å¾„è®¿é—®ã€‚

```bash
### ç¬”è€…ç¯å¢ƒä¸­å®¢æˆ·ç«¯èŠ‚ç‚¹ä¸º workstation
[root@workstation ~]# dnf install -y iscsi-initiator-utils
[root@workstation ~]# vim /etc/iscsi/initiatorname.iscsi  #ä¿®æ”¹ä¸º target ä¸­å¯¹åº”çš„ IQN ç¼–å·
InitiatorName=iqn.2025-04.com.example.lab.iscsi-gateway:01-02
[root@workstation ~]# systemctl enable --now iscsid.service
Created symlink /etc/systemd/system/multi-user.target.wants/iscsid.service â†’ /usr/lib/systemd/system/iscsid.service.
[root@workstation ~]# systemctl status iscsid.service
â— iscsid.service - Open-iSCSI
   Loaded: loaded (/usr/lib/systemd/system/iscsid.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2025-04-21 10:33:07 EDT; 7s ago
     Docs: man:iscsid(8)
           man:iscsiuio(8)
           man:iscsiadm(8)
 Main PID: 13775 (iscsid)
   Status: "Ready to process requests"
    Tasks: 1 (limit: 36110)
   Memory: 4.7M
   CGroup: /system.slice/iscsid.service
           â””â”€13775 /usr/sbin/iscsid -f

Apr 21 10:33:07 workstation.lab.example.com systemd[1]: Starting Open-iSCSI...
Apr 21 10:33:07 workstation.lab.example.com systemd[1]: Started Open-iSCSI.

[root@workstation ~]# iscsiadm --mode discoverydb --type sendtargets --portal serverc.lab.example.com --discover
172.25.250.12:3260,1 iqn.2025-04.com.example.lab.iscsi-gateway:01-02
172.25.250.14:3260,2 iqn.2025-04.com.example.lab.iscsi-gateway:01-02
[root@workstation ~]# iscsiadm --mode discoverydb --type sendtargets --portal servere.lab.example.com --discover
172.25.250.12:3260,1 iqn.2025-04.com.example.lab.iscsi-gateway:01-02
172.25.250.14:3260,2 iqn.2025-04.com.example.lab.iscsi-gateway:01-02
# åˆ†åˆ«å‘ç° serverc ä¸ servere èŠ‚ç‚¹ä¸Šçš„ IQN

[root@workstation ~]# iscsiadm --mode node --targetname iqn.2025-04.com.example.lab.iscsi-gateway:01-02 --portal serverc.lab.example.com --login
Logging in to [iface: default, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.12,3260]
Login to [iface: default, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.12,3260] successful.
[root@workstation ~]# iscsiadm --mode node --targetname iqn.2025-04.com.example.lab.iscsi-gateway:01-02 --portal servere.lab.example.com --login
Logging in to [iface: default, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.14,3260]
Login to [iface: default, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.14,3260] successful.
# åˆ†åˆ«ç™»å½• serverc ä¸ servere èŠ‚ç‚¹ä¸Šçš„ IQN
# å› æ­¤ï¼Œåœ¨æœ¬åœ°è™½ç„¶å¯è§ä¸¤å—ä¸åŒç£ç›˜ç¼–å·çš„ç£ç›˜ï¼Œä½†æ˜¯å®ƒä»¬æ¥è‡ªäºåŒä¸€ä¸ª RBD é•œåƒã€‚

[root@workstation ~]# lsblk -fp  #/dev/sd{a,b} å‡æ¥è‡ªäº RBD é•œåƒ
NAME        FSTYPE LABEL UUID                                 MOUNTPOINT
/dev/sda
/dev/sdb
/dev/vda
â”œâ”€/dev/vda1
â”œâ”€/dev/vda2 vfat         7B77-95E7                            /boot/efi
â””â”€/dev/vda3 xfs    root  d47ead13-ec24-428e-9175-46aefa764b26 /

[root@workstation ~]# dnf install -y device-mapper-multipath
[root@workstation ~]# mpathconf --enable  #ç”Ÿæˆå¤šè·¯å¾„çš„é»˜è®¤é…ç½®æ–‡ä»¶
[root@workstation ~]# udevadm info /dev/sda | grep ID_SERIAL=
E: ID_SERIAL=36001405b45aa463152441ea9e187cccb
[root@workstation ~]# udevadm info /dev/sdb | grep SERIAL=
E: ID_SERIAL=36001405b45aa463152441ea9e187cccb
# æŸ¥è¯¢ /dev/sd{a,b} çš„ ID_SERIALï¼Œç”¨äºå¤šè·¯å¾„é…ç½®çš„ wwidã€‚

[root@workstation ~]# vim /etc/multipath.conf
...
### customized mpath configure
devices {
  device {
    vendor                "LIO-ORG"
    hardware_handle       "1 alua"
    path_grouping_policy  "failover"
    path_selector         "queue-length 0"
    failback              60
    path_checker          tur
    prio                  alua
    prio_args             exclusive_pref_bit
    fast_io_fail_tmo      25
    no_path_retry         queue
  }
}

multipaths {
  multipath {
    wwid   36001405b45aa463152441ea9e187cccb
    alias  mpatha
  }
}

[root@workstation ~]# systemctl enable --now multipathd.service
[root@workstation ~]# systemctl status multipathd.service

[root@workstation ~]# multipath -ll
Apr 21 10:43:17 | /etc/multipath.conf line 28, invalid keyword: hardware_handle
Apr 21 10:43:17 | device config in /etc/multipath.conf missing vendor or product parameter
mpatha (36001405b45aa463152441ea9e187cccb) dm-0 LIO-ORG,TCMU device
size=1.0G features='0' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| `- 6:0:0:0 sda 8:0  active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  `- 7:0:0:0 sdb 8:16 active ready running
[root@workstation ~]# lsblk -fp
NAME                 FSTYPE       LABEL UUID                                 MOUNTPOINT
/dev/sda             mpath_member
â””â”€/dev/mapper/mpatha
/dev/sdb             mpath_member
â””â”€/dev/mapper/mpatha
/dev/vda
â”œâ”€/dev/vda1
â”œâ”€/dev/vda2          vfat               7B77-95E7                            /boot/efi
â””â”€/dev/vda3          xfs          root  d47ead13-ec24-428e-9175-46aefa764b26 /
# å¤šè·¯å¾„é…ç½®å®Œæˆ

[root@workstation ~]# mkfs.xfs /dev/mapper/mpatha
meta-data=/dev/mapper/mpatha     isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
[root@workstation ~]# mkdir /mnt/iscsi
[root@workstation ~]# mount /dev/mapper/mpatha /mnt/iscsi
[root@workstation ~]# echo "Test iscsi-gateway" > /mnt/iscsi/testfile
[root@workstation ~]# cat /mnt/iscsi/testfile
Test iscsi-gateway
# æµ‹è¯•å¤šè·¯å¾„ç£ç›˜æ˜¯å¦å¯ç”¨
```

è‹¥ä¸å†ä½¿ç”¨ iscsi-gatewayï¼Œéœ€ç™»å‡ºæ­¤æœåŠ¡ï¼Œå¯å‚è€ƒå¦‚ä¸‹ï¼š

```bash
[root@workstation ~]# iscsiadm --mode node --targetname iqn.2025-04.com.example.lab.iscsi-gateway:01-02 --portal serverc.lab.example.com --logout
Logging out of session [sid: 1, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.12,3260]
Logout of [sid: 1, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.12,3260] successful.
[root@workstation ~]# iscsiadm --mode node --targetname iqn.2025-04.com.example.lab.iscsi-gateway:01-02 --portal servere.lab.example.com --logout
Logging out of session [sid: 2, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.14,3260]
Logout of [sid: 2, target: iqn.2025-04.com.example.lab.iscsi-gateway:01-02, portal: 172.25.250.14,3260] successful.
```

## CephFS æ–‡ä»¶ç³»ç»Ÿ

```bash
$ ceph fs status
# æŸ¥çœ‹å„ä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€
[root@serverc ~]# ceph fs status  #ç¤ºä¾‹
mycephfs0 - 0 clients
=========
RANK  STATE             MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  mycephfs0.serverc.xqikij  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL  
mycephfs0_metadata  metadata   111k  28.1G  
  mycephfs0_data      data       0   28.1G  
mycephfs1 - 0 clients
=========
RANK  STATE             MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  mycephfs1.serverc.gjllbn  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL  
mycephfs1_metadata  metadata   134k  28.1G  
  mycephfs1_data      data       0   28.1G  
      STANDBY MDS
mycephfs1.servere.javarf  
mycephfs1.serverd.xsqsch  
mycephfs0.servere.apdcgl  
mycephfs0.serverd.hkfahf  
MDS version: ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)

$ ceph mds stat
# æŸ¥çœ‹å„ä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿ active çš„ mds æœåŠ¡å™¨
[root@serverc ~]# ceph mds stat  #ç¤ºä¾‹
mycephfs0:1 mycephfs1:1 {mycephfs0:0=mycephfs0.serverc.xqikij=up:active,mycephfs1:0=mycephfs1.serverc.gjllbn=up:active} 4 up:standby
```

### Lab: éƒ¨ç½²å¤šä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿä¸ standby åˆ‡æ¢éªŒè¯

```bash
[root@serverc ~]# ceph osd pool create mycephfs0_metadata 32 32
[root@serverc ~]# ceph osd pool create mycephfs0_data 32 32
[root@serverc ~]# ceph fs new mycephfs1 mycephfs0_metadata mycephfs0_data
# åˆ›å»º mycephfs0 cephfs

[root@serverc ~]# ceph osd pool create mycephfs1_metadata 32 32
[root@serverc ~]# ceph osd pool create mycephfs1_data 32 32
[root@serverc ~]# ceph fs new mycephfs1 mycephfs1_metadata mycephfs1_data
# åˆ›å»º mycephfs1 cephfs

[root@serverc ~]# ceph orch apply mds mycephfs0 --placement="3 serverc.lab.example.com serverd.lab.example.com servere.lab.example.com"
Scheduled mds.mycephfs0 update...
[root@serverc ~]# ceph orch apply mds mycephfs1 --placement="3 serverc.lab.example.com serverd.lab.example.com servere.lab.example.com"
Scheduled mds.mycephfs1 update...
# åœ¨3ä¸ªèŠ‚ç‚¹ä¸Šåˆ†åˆ«éƒ¨ç½² mds æœåŠ¡

[root@serverc ~]# ceph fs status
mycephfs0 - 0 clients
=========
RANK  STATE             MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  mycephfs0.serverc.xqikij  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL  
mycephfs0_metadata  metadata   110k  28.2G  
  mycephfs0_data      data       0   28.2G  
mycephfs1 - 0 clients
=========
RANK  STATE             MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  mycephfs1.serverd.xsqsch  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL  
mycephfs1_metadata  metadata   133k  28.2G  
  mycephfs1_data      data       0   28.2G  
      STANDBY MDS
mycephfs1.serverc.gjllbn  
mycephfs0.serverd.hkfahf  
mycephfs1.servere.javarf  
mycephfs0.servere.apdcgl  
MDS version: ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)
# æŸ¥çœ‹å„ä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€
[root@serverc ~]# ceph mds stat
mycephfs0:1 mycephfs1:1 {mycephfs0:0=mycephfs0.serverc.xqikij=up:active,mycephfs1:0=mycephfs1.serverd.xsqsch=up:active} 4 up:standby
# æŸ¥çœ‹å„ä¸ª cephfs æ–‡ä»¶ç³»ç»Ÿ active çš„ mds æœåŠ¡å™¨

# æ³¨æ„ï¼šæ­¤æ—¶å…³é—­ç³»ç»Ÿ serverdï¼Œè§‚å¯Ÿ mycephfs1 æ–‡ä»¶ç³»ç»Ÿçš„ mds æœåŠ¡æ˜¯å¦å¯å®Œæˆåˆ‡æ¢ã€‚
[root@serverc ~]# ceph -w
...
2025-04-22T12:02:22.650782-0400 mon.serverc.lab.example.com [WRN] Replacing daemon mds.mycephfs1.serverd.xsqsch as rank 0 with standby daemon mds.mycephfs1.serverc.gjllbn
2025-04-22T12:02:22.650878-0400 mon.serverc.lab.example.com [INF] MDS daemon mds.mycephfs1.serverd.xsqsch is removed because it is dead or otherwise unavailable.
2025-04-22T12:02:22.650893-0400 mon.serverc.lab.example.com [INF] MDS daemon mds.mycephfs0.serverd.hkfahf is removed because it is dead or otherwise unavailable.
2025-04-22T12:02:22.653507-0400 mon.serverc.lab.example.com [WRN] Health check failed: 1 filesystem is degraded (FS_DEGRADED)
2025-04-22T12:02:24.722995-0400 mon.serverc.lab.example.com [INF] daemon mds.mycephfs1.serverc.gjllbn is now active in filesystem mycephfs1 as rank 0
...
# ä»¥ä¸Šæ—¥å¿—ååº” mycephfs1 æ–‡ä»¶ç³»ç»Ÿçš„ mds æœåŠ¡å·²åˆ‡æ¢è‡³ serverc
```

### Lab: åˆ é™¤ CephFS

```bash
[root@serverc ~]# ceph fs set mycephfs0 down true
mycephfs0 marked down.
[root@serverc ~]# ceph fs set mycephfs1 down true
mycephfs1 marked down.
# å°†æ´»åŠ¨çš„ cephfs è®¾ç½®ä¸º down çŠ¶æ€
# æ³¨æ„ï¼šè¯·åŠ¡å¿…å¸è½½æ‰€æœ‰çš„å®¢æˆ·ç«¯æŒ‚è½½ï¼
[root@serverc ~]# ceph fs ls
name: mycephfs0, metadata pool: mycephfs0_metadata, data pools: [mycephfs0_data ]
name: mycephfs1, metadata pool: mycephfs1_metadata, data pools: [mycephfs1_data ]
[root@serverc ~]# ceph mds stat
mycephfs0:1 mycephfs1:1 {mycephfs0:0=mycephfs0.serverc.xqikij=up:stopping,mycephfs1:0=mycephfs1.serverc.gjllbn=up:stopping} 4 up:standby
# cephfs å…¨éƒ¨å¤„äº stopping çŠ¶æ€

[root@serverc ~]# ceph fs rm mycephfs0 --yes-i-really-mean-it
[root@serverc ~]# ceph fs rm mycephfs1 --yes-i-really-mean-it
# åˆ é™¤æŒ‡å®šçš„ cephfs

[root@serverc ~]# ceph fs ls
No filesystems enabled
# æŸ¥çœ‹ cephfs æ˜¯å¦å·²ç»åˆ é™¤æˆåŠŸ
```

### Lab: è®¾ç½® CephFS å…·å¤‡å¤šä¸ªæ´»è·ƒçš„ MDS æœåŠ¡å™¨

```bash
$ ceph fs get <fs_name>
# æŸ¥çœ‹æŒ‡å®š cephfs æ–‡ä»¶ç³»ç»Ÿçš„é…ç½®å‚æ•°
$ ceph fs get mycephfs0 | grep max_mds

[root@serverc ~]# ceph fs get mycephfs0 | grep max_mds
max_mds 1
# æŸ¥çœ‹æŒ‡å®š cephfs æ–‡ä»¶ç³»ç»Ÿçš„æœ€å¤§æ´»è·ƒ mds æœåŠ¡å™¨
[root@serverc ~]# ceph fs set mycephfs0 max_mds 2
[root@serverc ~]# ceph mds stat
mycephfs0:2 mycephfs1:1 {mycephfs0:0=mycephfs0.serverc.xqikij=up:active,mycephfs0:1=mycephfs0.servere.apdcgl=up:active,mycephfs1:0=mycephfs1.serverc.gjllbn=up:active} 3 up:standby
# è®¾ç½®å¤šä¸ªæ´»è·ƒçš„ mds æœåŠ¡å™¨çš„ä¼˜ç‚¹ï¼š
#   1. é«˜å¯ç”¨æ€§ï¼šä¸ºäº†ç¡®ä¿é«˜å¯ç”¨æ€§ï¼Œå»ºè®® max_mds çš„å€¼è‡³å°‘æ¯”å®é™…è¿è¡Œçš„ MDS å®ˆæŠ¤è¿›ç¨‹æ•°é‡å°‘ 1ï¼Œä»¥ä¾¿æœ‰è¶³å¤Ÿçš„å¤‡ç”¨å®ˆæŠ¤è¿›ç¨‹æ¥ç®¡æ•…éšœã€‚
#   2. æ€§èƒ½ä¼˜åŒ–ï¼šå¢åŠ  max_mds çš„å€¼å¯ä»¥æé«˜å…ƒæ•°æ®æ€§èƒ½ï¼Œä½†éœ€è¦æ ¹æ®å®é™…å·¥ä½œè´Ÿè½½è¿›è¡Œè°ƒæ•´ã€‚
#      å¯¹äºå¤§è§„æ¨¡ç³»ç»Ÿï¼Œå¤šä¸ªæ´»è·ƒçš„ MDS å®ˆæŠ¤è¿›ç¨‹å¯ä»¥åˆ†æ‹…å…ƒæ•°æ®å·¥ä½œè´Ÿè½½ã€‚
```

`max_mds` å‚æ•°çš„è¯´æ˜ï¼š

- æ¯ä¸€ Ceph æ–‡ä»¶ç³»ç»Ÿ (CephFS) å…·æœ‰å¤šä¸ªç­‰çº§ï¼Œé»˜è®¤ä¸ºä¸€ï¼Œä»é›¶å¼€å§‹ã€‚
- ç­‰çº§å®šä¹‰åœ¨å¤šä¸ªå…ƒæ•°æ®æœåŠ¡å™¨ (MDS) å®ˆæŠ¤è¿›ç¨‹ä¹‹é—´å…±äº«å…ƒæ•°æ®å·¥ä½œè´Ÿè½½çš„æ–¹å¼ã€‚ç­‰çº§æ•°æ˜¯ä¸€æ¬¡å¯ä»¥å¤„äºæ´»è·ƒçŠ¶æ€çš„æœ€å¤§ MDS å®ˆæŠ¤è¿›ç¨‹æ•°ã€‚æ¯ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹å¤„ç†åˆ†é…ç»™è¯¥ç­‰çº§çš„ CephFS å…ƒæ•°æ®å­é›†ã€‚
- æ¯ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹æœ€åˆå¯åŠ¨ä¸”æ²¡æœ‰ç­‰çº§ã€‚Ceph ç›‘æ§å™¨å°†æ’ååˆ†é…åˆ° å®ˆæŠ¤è¿›ç¨‹ã€‚MDS å®ˆæŠ¤è¿›ç¨‹ä¸€æ¬¡åªèƒ½æœ‰ä¸€ä¸ªæ’åã€‚åå°ç¨‹åºä»…åœ¨åœæ­¢æ—¶ä¸¢å¤±ç­‰çº§ã€‚
- max_mds è®¾ç½®æ§åˆ¶å°†åˆ›å»ºç­‰çº§æ•°ã€‚
- åªæœ‰å¤‡ç”¨å®ˆæŠ¤è¿›ç¨‹å¯ç”¨äºæ¥å—æ–°ç­‰çº§æ—¶ï¼ŒCephFS ä¸­å®é™…çš„æ’åæ•°é‡æ‰ä¼šå¢åŠ ã€‚

## å‚è€ƒé“¾æ¥

- [ä½¿ç”¨ cephadm æ­å»º ceph (octopus) è¿‡ç¨‹](https://juejin.cn/post/7160585472538837006)
- [Product Documentation for Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5)
- â¤ [Placement Group States | Ceph Docs](https://docs.ceph.com/en/pacific/rados/operations/pg-states/)
- â¤ [Chapter 17. PG Command Line Reference | RedHat Docs](https://docs.redhat.com/en/documentation/red_hat_ceph_storage/1.2.3/html/storage_strategies/pg-command-line-reference#pg-command-line-reference)
- â¤ [Detailed explanation of PG state of distributed storage Ceph](https://programmer.group/detailed-explanation-of-pg-state-of-distributed-storage-ceph.html)
- [ChapterÂ 2.Â Management of services using the Ceph Orchestrator Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/operations_guide/management-of-services-using-the-ceph-orchestrator#deploying-the-ceph-daemons-using-the-service-specification_ops)
- â¤ [5.4.Â ä½¿ç”¨ Ceph Manager è´Ÿè½½å‡è¡¡å™¨æ¨¡å— Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5/html/operations_guide/using-the-ceph-manager-balancer-module_ops)
- [SERVICE MANAGEMENT | Ceph Docs](https://docs.ceph.com/en/latest/cephadm/services/?highlight=service_id#)
- [ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢æ›´æ”¹ Ceph ä»ªè¡¨æ¿å¯†ç ](https://www.ibm.com/docs/zh/storage-ceph/6?topic=ia-changing-ceph-dashboard-password-using-command-line-interface)
- [Stray daemon tcmu-runner is reported not managed by cephadm | Red Hat Customer Portal](https://access.redhat.com/solutions/6472281)
- [Ceph - mapping rbd image is failing with RBD image feature set mismatch or image uses unsupported features | Red Hat Customer Portal](https://access.redhat.com/solutions/4270092)
- [maillist - rbd map image with journaling](https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/377S7XFN74MUYKVSXXRAN534FNZTDICK/)
- â¤ [æ–‡ä»¶ç³»ç»ŸæŒ‡å— | RedHat Docs](https://docs.redhat.com/zh-cn/documentation/red_hat_ceph_storage/5/html/file_system_guide/index)
- [2.3. å…ƒæ•°æ®æœåŠ¡å™¨æ’å | RedHat Docs](https://docs.redhat.com/zh-cn/documentation/red_hat_ceph_storage/5/html/file_system_guide/metadata-server-ranks_fs#metadata-server-ranks_fs)


[def]: #å‚è€ƒé“¾æ¥