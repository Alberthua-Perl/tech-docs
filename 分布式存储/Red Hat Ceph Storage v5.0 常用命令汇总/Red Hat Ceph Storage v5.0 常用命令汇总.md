# ğŸ™ Red Hat Ceph Storage v5.0 å¸¸ç”¨å‘½ä»¤æ±‡æ€»

## æ–‡æ¡£è¯´æ˜

- æ­¤æ–‡æ¡£æ¶‰åŠçš„å‘½ä»¤å·²åœ¨ `Red Hat Ceph Storage v5.0` é›†ç¾¤ä¸­è¿›è¡ŒéªŒè¯
- Red Hat Ceph Storage v5.0 å¯¹åº” `Ceph 16.2.0 pacific (stable)` å¼€æºç‰ˆæœ¬
- æ­¤æ–‡æ¡£å°†æŒç»­æ›´æ–°
- ğŸ§ª æ–‡æ¡£ä¸­å¸¦æœ‰ `Lab` æ ‡è®°çš„å†…å®¹å¯è¿›è¡Œå®éªŒéªŒè¯

## æ–‡æ¡£ç›®å½•

- [Ceph é›†ç¾¤çŠ¶æ€](#ceph-é›†ç¾¤çŠ¶æ€)
  - [cephadm å‘½ä»¤](#cephadm-å‘½ä»¤)
  - [Ceph Orchestrator ç¼–æ’å™¨ä½¿ç”¨](#ceph-orchestrator-ç¼–æ’å™¨ä½¿ç”¨)
  - [Ceph é›†ç¾¤é…ç½®](#ceph-é›†ç¾¤é…ç½®)
- [Ceph é›†ç¾¤ç½‘ç»œ](#ceph-é›†ç¾¤ç½‘ç»œ)
  - [é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜](#é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜)
  - [Lab: é…ç½® Ceph é›†ç¾¤ç½‘ç»œ](#lab-é…ç½®-ceph-é›†ç¾¤ç½‘ç»œ)
- [Ceph Monitor ç›‘æ§å™¨](#ceph-monitor-ç›‘æ§å™¨)
- [Ceph Manager ç®¡ç†å™¨](#ceph-manager-ç®¡ç†å™¨)
  - [Ceph Mgr å‘½ä»¤](#ceph-mgr-å‘½ä»¤)
  - [Lab: é…ç½®ç®¡ç† Ceph Dashboard](#lab-é…ç½®ç®¡ç†-ceph-dashboard)
  - [Lab: ä¿®æ”¹ Dashboard ä»ªè¡¨æ¿å¯†ç ](#lab-ä¿®æ”¹-dashboard-ä»ªè¡¨æ¿å¯†ç )
  - [Ceph balancer å‡è¡¡å™¨](#ceph-balancer-å‡è¡¡å™¨)
- [Ceph Cluster Map é›†ç¾¤æ˜ å°„](#ceph-cluster-map-é›†ç¾¤æ˜ å°„)
- [Ceph CRUSH Map æ˜ å°„](#ceph-crush-map-æ˜ å°„)
- [Ceph PG æ”¾ç½®ç»„](#ceph-pg-æ”¾ç½®ç»„)
  - [Ceph PG å¸¸ç”¨ç®¡ç†å‘½ä»¤](#ceph-pg-å¸¸ç”¨ç®¡ç†å‘½ä»¤)
- [Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡](#ceph-osd-å¯¹è±¡å­˜å‚¨è®¾å¤‡)
  - [Lab: å®šä½å¯¹è±¡ä¸ OSDã€PG çš„æ˜ å°„å…³ç³»](#lab-å®šä½å¯¹è±¡ä¸-osdpg-çš„æ˜ å°„å…³ç³»)
  - [Lab: æ ‡è®° OSD åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€](#lab-æ ‡è®°-osd-åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€)
  - [Lab: ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„ OSD è®¾å¤‡ï¼ˆscale upï¼‰](#lab-ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„-osd-è®¾å¤‡scale-up)
  - [Lab: ä½¿ç”¨ osd specification file éƒ¨ç½² OSD è®¾å¤‡](#lab-ä½¿ç”¨-osd-specification-file-éƒ¨ç½²-osd-è®¾å¤‡)
  - [Lab: åˆ é™¤ OSD è®¾å¤‡](#lab-åˆ é™¤-osd-è®¾å¤‡)
  - [Lab: é…ç½® ceph è½¯ä»¶æº](#lab-é…ç½®-ceph-è½¯ä»¶æº)
  - [Lab: RHCS5 ä¸­ä½¿ç”¨ ceph-volume æ‰‹åŠ¨éƒ¨ç½² OSD](#lab-rhcs5-ä¸­ä½¿ç”¨-ceph-volume-æ‰‹åŠ¨éƒ¨ç½²-osd)
- [RADOS å¯¹è±¡æ“ä½œ](#rados-å¯¹è±¡æ“ä½œ)
- [Ceph Pool å­˜å‚¨æ± ](#ceph-pool-å­˜å‚¨æ± )
  - [Ceph å­˜å‚¨æ± çŠ¶æ€](#ceph-å­˜å‚¨æ± çŠ¶æ€)
  - [Ceph å¤åˆ¶æ± å‘½ä»¤](#ceph-å¤åˆ¶æ± å‘½ä»¤)
  - [Ceph çº åˆ ä»£ç æ± å‘½ä»¤](#ceph-çº åˆ ä»£ç æ± å‘½ä»¤)
- [CephX è®¤è¯ä¸ç”¨æˆ·](#cephx-è®¤è¯ä¸ç”¨æˆ·)
  - [CephX è®¤è¯æœºåˆ¶](#cephx-è®¤è¯æœºåˆ¶)
  - [CephX ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤](#cephx-ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤)
- [Ceph RBD é•œåƒ](#ceph-rbd-é•œåƒ)
  - [RBD é•œåƒçš„ç‰¹æ€§ï¼ˆfeatureï¼‰](#rbd-é•œåƒçš„ç‰¹æ€§feature)
  - [RBD é•œåƒå¸¸ç”¨å‘½ä»¤](#rbd-é•œåƒå¸¸ç”¨å‘½ä»¤)
  - [Ceph RBD Mirror é›†ç¾¤æ¨¡å¼](#ceph-rbd-mirror-é›†ç¾¤æ¨¡å¼)
  - [Lab: å®ç° RBD one-way mirroring](#lab-å®ç°-rbd-one-way-mirroring)
  - [Lab: `rbd-nbd` æ˜ å°„ä½¿ç”¨å·² mirroring çš„ RBD é•œåƒ](#lab-rbd-nbd-æ˜ å°„ä½¿ç”¨å·²-mirroring-çš„-rbd-é•œåƒ)
  - [å…¶ä»– RBD Mirror ç›¸å…³å‘½ä»¤](#å…¶ä»–-rbd-mirror-ç›¸å…³å‘½ä»¤)
  - [RBD Mirror çš„æ•…éšœè½¬ç§»](#rbd-mirror-çš„æ•…éšœè½¬ç§»)
- [CephFS æ–‡ä»¶ç³»ç»Ÿ](#cephfs-æ–‡ä»¶ç³»ç»Ÿ)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## Ceph é›†ç¾¤çŠ¶æ€

Red Hat Ceph Storage v5.0 ä¸­å·²ä¸å†ä½¿ç”¨æ—§å¼çš„ systemd ç®¡ç†å®ˆæŠ¤è¿›ç¨‹çš„æ–¹å¼ç®¡ç† Ceph è¿›ç¨‹ï¼Œè€Œä½¿ç”¨ `Podman rootfull` å®¹å™¨çš„æ–¹å¼å°è£…è¿è¡Œ Ceph è¿›ç¨‹ï¼Œå› æ­¤ï¼Œå¯ä½¿ç”¨ `cephadm` å‘½ä»¤è¡Œå·¥å…·å–ä»£ä»¥å¾€çš„ `ceph-ansible` æ¥å®ç°é›†ç¾¤çš„éƒ¨ç½²ä¸ç®¡ç†ã€‚

ç®¡ç† Ceph é›†ç¾¤çš„ä¸¤ç§æ–¹å¼ï¼š

- 1ï¸âƒ£ åœ¨ç®¡ç†èŠ‚ç‚¹ä¸Šç›´æ¥ä½¿ç”¨ ceph å‘½ä»¤ã€‚
- 2ï¸âƒ£ è¿›å…¥ç®¡ç†èŠ‚ç‚¹çš„ cephadm shell ä¸´æ—¶å®¹å™¨ä¸­ï¼Œä½¿ç”¨äº¤äº’å¼å‘½ä»¤ç®¡ç†ã€‚

### cephadm å‘½ä»¤

cephadm å·¥å…·ä¸€èˆ¬å®‰è£…äºéƒ¨ç½²èŠ‚ç‚¹æˆ–ç®¡ç†èŠ‚ç‚¹ä¸­ï¼Œå› æ­¤è¿è¡Œä»¥ä¸‹å‘½ä»¤éœ€åœ¨å¯¹åº”èŠ‚ç‚¹ä¸­è¿è¡Œã€‚
  
```bash
$ cephadm --help
$ cephadm version
$ cephadm inspect-image
# æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ Ceph å®¹å™¨é•œåƒä»“åº“ä¸ç‰ˆæœ¬
$ cephadm list-networks
# æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ç½‘æ®µä¿¡æ¯
  
$ cephadm shell -- <command>
$ cephadm shell -- ceph status  #ç¤ºä¾‹
# åœ¨é›†ç¾¤ bootstrap èŠ‚ç‚¹ä½¿ç”¨ cephadm shell ä»¥ç¡®è®¤é›†ç¾¤å¥åº·çŠ¶æ€
# æ­¤å‘½ä»¤è¿è¡Œè¿‡ç¨‹ä¸­å°†å¯åŠ¨ä¸€ä¸ªä¸´æ—¶å®¹å™¨ä»¥è¿è¡ŒæŒ‡å®šçš„å‘½ä»¤
$ cephadm shell --mount /root/<mount-point>
# å¯åŠ¨ä¸´æ—¶å®¹å™¨å¹¶å°†æŒ‡å®šç›®å½•æ˜ å°„è‡³å®¹å™¨ /mnt ç›®å½•ä¸Š
  
$ cephadm bootstrap --config /etc/ceph/ceph.conf
# é›†ç¾¤å¼•å¯¼è¿‡ç¨‹ä¸­æ›´æ”¹é›†ç¾¤é…ç½®æ–‡ä»¶ä»¥ä¼ é€’ä¿®æ”¹çš„é…ç½®ä½¿å…¶ç”Ÿæ•ˆ
  
$ ceph versions
# æŸ¥çœ‹é›†ç¾¤æ‰€æœ‰æœåŠ¡ç»„ä»¶çš„ç‰ˆæœ¬
$ ceph tell osd.* version
# æŸ¥çœ‹æ‰€æœ‰ osd çš„ç‰ˆæœ¬
$ ceph tell mon.* version
# æŸ¥çœ‹æ‰€æœ‰ mon çš„ç‰ˆæœ¬
```
  
![cephadm-demo](images/cephadm-demo.png)
  
![cephadm-shell-demo](images/cephadm-shell-demo.png)
  
âœ¨ æ³¨æ„ï¼šä½¿ç”¨ ceph orch host add æ·»åŠ é¢å¤–çš„èŠ‚ç‚¹æ—¶ï¼Œéœ€å…ˆå°†é›†ç¾¤å…¬é’¥å¯¼å‡ºå¹¶åŒæ­¥è‡³èŠ‚ç‚¹ã€‚
  
```bash
$ ceph cephadm get-pub-key > /path/to/ceph.pub
# ç”Ÿæˆé›†ç¾¤å…¬é’¥
$ ssh-copy-id -f -i /path/to/ceph.pub root@<hostname>
# å°†é›†ç¾¤å…¬é’¥åŒæ­¥è‡³èŠ‚ç‚¹ï¼Œå³å¯æ·»åŠ æ­¤èŠ‚ç‚¹ã€‚
$ ceph orch host add <node>
# æ·»åŠ èŠ‚ç‚¹ä¸ºé›†ç¾¤èŠ‚ç‚¹
```

### Ceph Orchestrator ç¼–æ’å™¨ä½¿ç”¨
  
```bash
$ ceph orch status
  Backend: cephadm
  Available: Yes
  Paused: No
# æŸ¥çœ‹é›†ç¾¤æ•´ä½“çŠ¶æ€
  
$ ceph orch ls [--service_type=<name>]
  NAME                     RUNNING  REFRESHED  AGE  PLACEMENT                                                                
  alertmanager                 1/1  9m ago     2y   count:1                                                                  
  crash                        4/4  9m ago     2y   *                                                                        
  grafana                      1/1  9m ago     2y   count:1                                                                  
  mgr                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
  mon                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
  node-exporter                4/4  9m ago     2y   *                                                                        
  osd.default_drive_group     9/12  9m ago     2y   server*                                                                  
  prometheus                   1/1  9m ago     2y   count:1                                                                  
  rgw.realm.zone               2/2  9m ago     23M  serverc.lab.example.com;serverd.lab.example.com
# æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡ï¼ˆserviceï¼‰çŠ¶æ€
# ä½¿ç”¨ --service_type é€‰é¡¹æŒ‡å®š Ceph æœåŠ¡ç±»å‹
$ ceph orch ls --service_type=mon
  NAME  RUNNING  REFRESHED  AGE  PLACEMENT                                                                                        
  mon       4/4  3m ago     2y   clienta.lab.example.com;serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com
# æŸ¥çœ‹ mon æœåŠ¡çš„çŠ¶æ€
  
$ ceph orch ps [--daemon_type=<name>]
  NAME                         HOST                     STATUS         REFRESHED  AGE  PORTS  VERSION           IMAGE ID      CONTAINER ID  
  mon.clienta                  clienta.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  6c85145c2b22  
  mon.serverc.lab.example.com  serverc.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  f93b366a9f74  
  mon.serverd                  serverd.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  d35695b5833e  
  mon.servere                  servere.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  97319d05259c
# æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡å®ä¾‹çŠ¶æ€
# ä½¿ç”¨ --daemon_type é€‰é¡¹å¯æŒ‡å®šæœåŠ¡å®ä¾‹çš„åç§°è¿›è¡Œè¿‡æ»¤
  
$ ceph orch host ls
  HOST                     ADDR           LABELS  STATUS  
  clienta.lab.example.com  172.25.250.10  _admin          
  serverc.lab.example.com  172.25.250.12  _admin          
  serverd.lab.example.com  172.25.250.13                  
  servere.lab.example.com  172.25.250.14
# ä½¿ç”¨ç¼–æ’å™¨æŸ¥çœ‹å„èŠ‚ç‚¹æ¦‚è¦ä¸ label æ ‡ç­¾
# æ³¨æ„ï¼šlabel æ ‡ç­¾çš„ä½œç”¨åœ¨äºåŒºåˆ†å„ä¸ªèŠ‚ç‚¹ï¼Œå¯¹èŠ‚ç‚¹è¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œå¹¶ä¸”ä¸€ä¸ªèŠ‚ç‚¹å¯å…·æœ‰å¤šä¸ªæ ‡ç­¾ã€‚
$ ceph orch host label add <node> _admin
# ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  admin ç®¡ç†å‘˜ label æ ‡ç­¾
$ ceph orch host label add <node> prometheus
# ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  prometheus æ ‡ç­¾
$ ceph orch apply prometheus --placement="label:prometheus"
# æ ¹æ®é›†ç¾¤èŠ‚ç‚¹æ ‡ç­¾éƒ¨ç½² prometheus
  
$ ceph orch device ls [--wide]
# æŸ¥çœ‹é›†ç¾¤å„èŠ‚ç‚¹å¯ç”¨ä¸ä¸å¯ç”¨çš„è®¾å¤‡ä¿¡æ¯ï¼Œ--wide é€‰é¡¹å°†æŒ‡å®šå…¨éƒ¨çš„è¯¦ç»†è®¾å¤‡ä¿¡æ¯ã€‚
  
$ sudo systemctl list-units --all "ceph*"
# ç™»å½• Ceph é›†ç¾¤èŠ‚ç‚¹æŸ¥çœ‹æ‰€æœ‰çš„ ceph æœåŠ¡å•å…ƒ
  
$ ceph orch host maintenance enter <node>
# å°†æŒ‡å®šèŠ‚ç‚¹è®¾ç½®ä¸ºç»´æŠ¤æ¨¡å¼
$ ceph orch host maintenance exit <node>
# å°†æŒ‡å®šèŠ‚ç‚¹é€€å‡ºç»´æŠ¤æ¨¡å¼
```

### Ceph é›†ç¾¤é…ç½®
  
æ¯ä¸ª `ceph monitor` èŠ‚ç‚¹ç®¡ç†ä¸€ä¸ªé›†ä¸­å¼é…ç½®æ•°æ®åº“ï¼Œä½äº `/var/lib/ceph/$fsid/mon.$host/store.db/` ä¸­ã€‚åœ¨é›†ç¾¤å¯åŠ¨æ—¶ï¼ŒCeph å®ˆæŠ¤è¿›ç¨‹è§£æç”±å‘½ä»¤è¡Œé€‰é¡¹ã€ç¯å¢ƒå˜é‡ä¸æœ¬åœ°é›†ç¾¤é…ç½®çš„é…ç½®é€‰é¡¹ã€‚Ceph å®ˆæŠ¤è¿›ç¨‹è¿æ¥åˆ°é›†ç¾¤ä»¥è·å–å­˜å‚¨åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®è®¾å®šã€‚ä» RHCS 4 å¼€å§‹å¼ƒç”¨ `/etc/ceph/ceph.conf` é›†ç¾¤é…ç½®æ–‡ä»¶ï¼Œè€Œå°†é›†ä¸­å¼é…ç½®æ•°æ®åº“ä½œä¸ºé…ç½®å­˜å‚¨çš„é¦–é€‰æ–¹å¼ã€‚`ceph config set` å‘½ä»¤å¯ç”¨äºæ›´æ”¹é›†ç¾¤å„ç±»é…ç½®ã€‚
  
```bash
$ ceph -s
$ ceph status
$ ceph -w
# å®æ—¶åˆ·æ–°æ—¥å¿—
```
  
```bash
$ ceph config ls
# åˆ—å‡ºé›†ç¾¤æ‰€æœ‰çš„å¯é…ç½®å‚æ•°ï¼ˆRHCS5 åŒ…å« 2026 ä¸ªé…ç½®å‚æ•°ï¼‰
$ ceph config help <config_setting>
# æŸ¥çœ‹æŒ‡å®šé…ç½®å‚æ•°çš„å…·ä½“ä¿¡æ¯
$ ceph config dump
# æŸ¥çœ‹é›†ä¸­å¼é…ç½®æ•°æ®åº“çš„é…ç½®ï¼ˆåŒºåˆ«äºé›†ç¾¤é…ç½®æ–‡ä»¶ï¼‰
$ ceph config show $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é…ç½®
$ ceph config show-with-defaults $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é»˜è®¤é…ç½®
  
$ ceph config get $type.$id
# æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
$ ceph config set $type.$id <config_setting>
# è®¾ç½®ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
$ ceph config set mon.serverd mon_max_pool_pg_num 65536  #ç¤ºä¾‹
# è®¾ç½® mon.serverd çš„ mon_max_pool_pg_num ä¸º 65536
  
$ ceph config assimilate-conf -i /path/to/config_file
# å°†æŒ‡å®šé…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°èå…¥å½“å‰çš„é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­
  
$ ceph config get mon mon_allow_pool_delete
# æŸ¥çœ‹é›†ç¾¤æ˜¯å¦å…è®¸åˆ é™¤å­˜å‚¨æ± 
$ ceph config get mon mon_compact_on_start
  false
# æŸ¥çœ‹ mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­æ˜¯å¦å¯æ”¶ç¼©æ•°æ®åº“ï¼Œä»¥æé«˜æ•°æ®åº“æ€§èƒ½ã€‚
$ ceph config set mon mon_compact_on_start true
# è®¾ç½® mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®åº“å‹ç¼©
  
$ ceph config get mon auth_service_required
$ ceph config get mon auth_cluster_required
$ ceph config get mon auth_client_required
# æŸ¥çœ‹é›†ç¾¤çš„å„è®¤è¯æ–¹å¼
  
### é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®è®¾ç½® ###
$ ceph tell $type.$id config get <config_setting>
$ ceph tell mon.serverc config get mon_max_pool_pg_num  #ç¤ºä¾‹
# æŸ¥çœ‹ serverc æ§åˆ¶èŠ‚ç‚¹ä¸Šæ”¯æŒçš„æ¯ä¸ªå­˜å‚¨æ± å¯åŒ…å«çš„æœ€å¤§ pg æ•°
# æ³¨æ„ï¼š
#   1. ä»¥ä¸Šå‘½ä»¤å¯åœ¨é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®ï¼Œæ­¤æ–¹å¼ä½œä¸ºä¸´æ—¶é…ç½®ï¼Œå½“å®ˆæŠ¤è¿›ç¨‹é‡å¯åå°†å¤±æ•ˆã€‚
#   2. ceph tell å­å‘½ä»¤ä¾ç„¶ä½¿ç”¨ monã€‚
```

## Ceph é›†ç¾¤ç½‘ç»œ

### é›†ç¾¤ç½‘ç»œç±»å‹è¯´æ˜

- `public_network`ï¼šå…¬å…±ç½‘ç»œï¼Œä¸»è¦ç”¨äºå®¢æˆ·ç«¯ä¸ Monã€Mgrã€Mdsã€osd ä»¥åŠå…¶ä»–ç›¸å…³ç»„ä»¶ä¹‹é—´çš„æ•°æ®è¯·æ±‚ä¸äº¤æ¢ã€‚
- `cluster_network`ï¼šé›†ç¾¤ç½‘ç»œï¼Œä¸»è¦ç”¨äº osd ä¹‹é—´çš„æ•°æ®åŒæ­¥ã€æ•°æ®é‡å¹³è¡¡ã€æ•°æ®æ¢å¤ã€æ•°æ®å›å¡«ã€å¿ƒè·³æ£€æµ‹ç­‰ã€‚
  
<center><img src="images/ceph-arch-network.png" style="width:60%"></center>

<center><img src="images/osd-network-community.jpg" style="width:60%"></center>
  
<center>Ceph OSD èŠ‚ç‚¹çš„ç½‘ç»œè¿æ¥ç¤ºæ„</center>

- æ¯ä¸ª OSD ä½¿ç”¨ä¸€ä¸ªç«¯å£é€šè¿‡ public ç½‘ç»œä¸å®¢æˆ·ç«¯åŠ Mon é€šä¿¡
- ä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œä¸å…¶ä»– OSD é—´ä¼ è¾“æ•°æ®
- ä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œäº¤æ¢ heatbeat å¿ƒè·³åŒ…ï¼‰

<center><img src="images/default-ports-rhcs5.jpg" style="width:60%"></center>

<center>RHCS5 ä¸­çš„é»˜è®¤ç«¯å£èŒƒå›´</center>

### Lab: é…ç½® Ceph é›†ç¾¤ç½‘ç»œ

Ceph é›†ç¾¤ç½‘ç»œé…ç½®çš„ä¸¤ç§æ–¹å¼ï¼š

```bash
### æ–¹å¼1ï¼šé…ç½®æ–‡ä»¶æ–¹å¼
$ vim ./cluster-public-network.conf
[osd]  #é…ç½® osd ä½¿ç”¨çš„ä¸¤å¤§ç±»ç½‘ç»œï¼Œä¹Ÿå¯å®šä¹‰ mon ä½¿ç”¨çš„ç½‘ç»œã€‚
  public network = 172.25.250.0/24
  cluster network = 172.25.249.0/24

$ ceph config assimilate-conf -i ./cluster-public-network.conf
$ ceph config get osd public_network
$ ceph config get osd cluster_network
$ ceph orch restart osd  #é‡å¯æ‰€æœ‰ osd æœåŠ¡ç¡®ä¿é…ç½®ç”Ÿæ•ˆ

### æ–¹å¼2ï¼šå‘½ä»¤è¡Œé…ç½®é›†ä¸­å¼æ•°æ®åº“
$ ceph config set mon cluster_network 172.25.249.0/24
$ ceph config get mon cluster_network
$ ceph orch restart mon  #é‡å¯æ‰€æœ‰ mon æœåŠ¡ç¡®ä¿é…ç½®ç”Ÿæ•ˆ
```

## Ceph Monitor ç›‘æ§å™¨

```bash
$ ceph mon stat
  e5: 3 mons at {serverc.lab.example.com=[v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0],
  serverd=[v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0],servere=[v2:172.25.250.14:3300/0,
  v1:172.25.250.14:6789/0]}, election epoch 42, leader 0 serverc.lab.example.com, 
  quorum 0,1,2 serverc.lab.example.com,serverd,servere
# æŸ¥çœ‹ mon çŠ¶æ€ï¼ŒåŒ…æ‹¬ mon çš„ IPv4 åœ°å€ä¸ leader monã€‚

$ ceph quorum_status -f json-pretty
{
  "election_epoch": 42,
  "quorum": [
    0,
    1,
    2
  ],
  "quorum_names": [
    "serverc.lab.example.com",
    "serverd",
    "servere"
  ],
  "quorum_leader_name": "serverc.lab.example.com",
  ...
# æŸ¥çœ‹ mon çš„é€‰ä¸¾çŠ¶æ€
```

## Ceph Manager ç®¡ç†å™¨

### Ceph Mgr å‘½ä»¤

```bash
$ ceph mgr stat
{
    "epoch": 19,
    "available": true,
    "active_name": "serverc.lab.example.com.weqbtr",
    "num_standby": 2
}
# ç¡®è®¤ mgr çš„æ•´ä½“çŠ¶æ€

$ ceph mgr module enable pg_autoscaler
$ ceph mgr module ls
{
    "always_on_modules": [
        "balancer",
        "crash",
        "devicehealth",
        "orchestrator",
        "pg_autoscaler",
        "progress",
        "rbd_support",
        "status",
        "telemetry",
        "volumes"
    ],
    ...
}
```

### Lab: é…ç½®ç®¡ç† Ceph Dashboard

```bash
$ ceph mgr module enable dashboard --force
# å¯ç”¨ dashboard æ¨¡å—
$ ceph mgr module ls | \
  perl -MJSON -MYAML -0777 -wnl -e 'print YAML::Dump(decode_json($_))' -
# æŸ¥çœ‹ mgr ç®¡ç†çš„æ¨¡å—å¹¶ä½¿ç”¨ perl å°†è¾“å‡ºçš„ JSON æ ¼å¼è½¬æ¢ä¸º YAML æ ¼å¼
# æ³¨æ„ï¼šä½¿ç”¨ perl æ—¶éœ€æå‰å®‰è£… perl-JSON ä¸ perl-YAML è½¯ä»¶åŒ…ä»¥æä¾›å¯¹åº”æ¨¡å—
$ ceph dashboard create-self-signed-cert
# é…ç½® dashboard ç”Ÿæˆè¯ä¹¦
$ ceph config set mgr mgr/dashboard/server_addr <dashboard_server_address>
# é…ç½® dashboard çš„ä¸»æœº IP åœ°å€
$ ceph config set mgr mgr/dashboard/server_port 8080
# é…ç½® dashboard ç›‘å¬çš„é SSL ç«¯å£
$ ceph config set mgr mgr/dashboard/ssl_server_port 8443
# é…ç½® dashboard ç›‘å¬çš„ SSL ç«¯å£
$ ceph mgr services
{
    "dashboard": "https://172.25.250.12:8443/",
    "prometheus": "http://172.25.250.12:9283/"
}
# è·å– mgr ç®¡ç†çš„ dashboard URL
$ ceph dashboard ac-user-create \
<dashboard_user> <dashboard_password> administrator
# åˆ›å»ºç™»å½• dashboard çš„ç”¨æˆ·åä¸å¯†ç ï¼Œä»¥ç®¡ç†å‘˜ç”¨æˆ·èº«ä»½ç™»å½•ã€‚
```

### Lab: ä¿®æ”¹ Dashboard ä»ªè¡¨æ¿å¯†ç 

```bash
$ echo "<password>" > /path/to/dashboard_password.yml
# æ³¨å…¥ä¿®æ”¹çš„æ–°å¯†ç 
$ ceph dashboard ac-user-set-password <dashboard_user> -i /path/to/dashboard_password.yml 
# è®¾ç½®æŒ‡å®šç”¨æˆ·ä¸å¯†ç 
```

### Ceph balancer å‡è¡¡å™¨

```bash
$ ceph balancer status
{
    "active": true,
    "last_optimize_duration": "0:00:00.000462",
    "last_optimize_started": "Thu Jan 11 05:51:41 2024",
    "mode": "upmap",
    "optimize_result": "Unable to find further optimization, or pool(s) pg_num is decreasing, or distribution is already perfect",
    "plans": []
} 
# æŸ¥çœ‹ balancer å‡è¡¡å™¨çŠ¶æ€
```

## Ceph Cluster Map é›†ç¾¤æ˜ å°„

Ceph Cluster Map åŒ…å« `MON Map`ã€`OSD Map`ã€`PG Map`ã€`MDS Map` å’Œ `CRUSH Map`ã€‚

```bash
$ ceph mon dump
  epoch 55
  fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
  last_changed 2023-09-17T23:46:59.748723+0000
  created 2021-10-01T09:30:30.146231+0000
  min_mon_release 16 (pacific)
  election_strategy: 1
  0: [v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0] mon.serverc.lab.example.com
  1: [v2:172.25.250.14:3300/0,v1:172.25.250.14:6789/0] mon.servere
  2: [v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0] mon.serverd
  dumped monmap epoch 55
# æŸ¥çœ‹é›†ç¾¤ monmap
$ ceph osd dump
# æŸ¥çœ‹é›†ç¾¤ osdmap
$ ceph pg dump
# æŸ¥çœ‹é›†ç¾¤ pgmap
$ ceph pg dump pgs_brief
# æŸ¥çœ‹é›†ç¾¤ç®€è¦ pgmap
$ ceph osd crush dump
# æŸ¥çœ‹é›†ç¾¤ crush mapï¼ˆå¯æŸ¥çœ‹é›†ç¾¤æ•´ä½“çš„ CRUSH å±‚æ¬¡ç»“æ„ä¸æ•…éšœæ¢å¤åŸŸï¼‰
$ ceph fs dump
# æŸ¥çœ‹é›†ç¾¤ cephfs map
```

## Ceph CRUSH Map æ˜ å°„

ğŸš€ å…³äº `CRUSH map` çš„è¯´æ˜å¯å‚è€ƒ [Ceph CRUSH map æ¦‚è¿°ä¸å®ç°](https://github.com/Alberthua-Perl/tech-docs/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph%20CRUSH%20map%20%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0.md#-ceph-crush-map-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0)

## Ceph PG æ”¾ç½®ç»„

### Ceph PG å¸¸ç”¨ç®¡ç†å‘½ä»¤

Ceph PGã€CRUSH æ”¾ç½®è§„åˆ™ä¸ OSD ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<center><img src="images/ceph-pg-crush-osd-mapping.png" style="width:80%"></center>


```bash
$ ceph osd pool set <pool_name> pg_autoscale_mode on
# å¯ç”¨ mgr çš„ pg_autoscaler æ¨¡å—å¹¶å¯ç”¨æŒ‡å®šå­˜å‚¨æ± çš„è‡ªåŠ¨æ‰©å±• pg åŠŸèƒ½

$ ceph pg stat
  105 pgs: 105 active+clean; 4.9 KiB data, 181 MiB used, 90 GiB / 90 GiB avail
# æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰ pg çš„çŠ¶æ€
$ ceph pg map <pg.id>
# é‡è¦ï¼šæ ¹æ® pd id æŸ¥æ‰¾æŒ‡å®š pg ä¸ osd çš„å¯¹åº”å…³ç³»
$ ceph pg <pg.id> query
# é‡è¦ï¼šæ ¹æ® pg id æŸ¥çœ‹å…¶è¯¦ç»†çŠ¶æ€ä¿¡æ¯
```

## Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡

```bash
$ ceph osd df
# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
$ ceph osd df tree
# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µå¹¶æ˜¾ç¤º osd åœ¨ CRUSH map ä¸­çš„ä½ç½®

$ ceph osd tree
# osd ç£ç›˜åœ¨é›†ç¾¤ä¸­çš„åˆ†å¸ƒæƒ…å†µ
$ ceph osd stat
  9 osds: 9 up (since 13h), 9 in (since 2y); epoch: e199
# æŸ¥çœ‹æ‰€æœ‰ osd çš„çŠ¶æ€

$ ceph osd find <osd.id>
# æ ¹æ® osd id æŸ¥æ‰¾ osd çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹
$ ceph osd find 2  #ç¤ºä¾‹
{
    "osd": 2,
    "addrs": {
        "addrvec": [
            {
                "type": "v2",
                "addr": "172.25.250.12:6816",
                "nonce": 3631084615
            },
            {
                "type": "v1",
                "addr": "172.25.250.12:6817",
                "nonce": 3631084615
            }
        ]
    },
    "osd_fsid": "a28ad912-a54a-423a-a2d1-4889b0788a47",
    "host": "serverc.lab.example.com",
    "crush_location": {
        "host": "serverc",
        "root": "default"
    }
}
# æŸ¥æ‰¾ osd.2 çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹

$ ceph osd metadata <osd.id>
# é‡è¦ï¼šæ ¹æ® osd id æŸ¥çœ‹ osd çš„å…ƒæ•°æ®ä¿¡æ¯
```

### Lab: å®šä½å¯¹è±¡ä¸ OSDã€PG çš„æ˜ å°„å…³ç³»

```bash
$ rados -p <pool_name> ls
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­çš„å¯¹è±¡
$ rados -p testpool ls  #ç¤ºä¾‹
test-data
testobject
$ ceph osd map <pool_name> <object_name>
# é‡è¦ï¼š
#   1. æ ¹æ®æŒ‡å®šçš„å¯¹è±¡åç§°æŸ¥æ‰¾å…¶ä¸ pg åŠ osd çš„æ˜ å°„å…³ç³»
#   2. æ­¤å‘½ä»¤ç›¸è¾ƒäº ceph pg map <pg.id> æ›´è¿›ä¸€æ­¥ï¼Œç›´æ¥æŸ¥æ‰¾å¯¹è±¡çš„æ˜ å°„å…³ç³»ã€‚
$ ceph osd map testpool test-data  #ç¤ºä¾‹
osdmap e664 pool 'testpool' (9) object 'test-data' -> pg 9.4a628b60 (9.0) -> up ([0,2,5], p0) acting ([0,2,5], p0)
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­å¯¹è±¡ã€pg ä¸ osd çš„æ˜ å°„å…³ç³»
```

### Lab: æ ‡è®° OSD åœ¨é›†ç¾¤ä¸­çš„çŠ¶æ€

```bash
$ ceph osd out <osd.id>
# å°†æŒ‡å®šçš„ osd æ ‡è®°ä¸º out çŠ¶æ€
$ ceph osd in <osd.id>
# å°†æŒ‡å®šçš„ osd æ ‡è®°ä¸º in çŠ¶æ€
# æ³¨æ„ï¼šå¯å°†è¿è¡Œä¸­çš„ osd æ ‡è®°ä¸º in æˆ– out çš„çŠ¶æ€ï¼Œå› ä¸º in æˆ– out çŠ¶æ€ä¸ä¸è¿è¡ŒçŠ¶æ€å…³è”ã€‚

$ ceph osd [set|unset] noscrub
# (ä¸)è®¾ç½® osd ä¸æ‰§è¡Œæ¸…ç†
$ ceph osd [set|unset] nodeep-scrub
# (ä¸)è®¾ç½® osd ä¸æ‰§è¡Œæ·±åº¦æ¸…ç†
```

### Lab: ä½¿ç”¨å‘½ä»¤è¡Œéƒ¨ç½²æ–°çš„ OSD è®¾å¤‡ï¼ˆscale upï¼‰

```bash
$ ceph orch device zap --force <hostname> /path/to/device
# å¼ºåˆ¶åˆ é™¤ä¹‹å‰ osd åˆ›å»ºçš„æ‰€æœ‰åˆ†åŒºå¹¶æ¸…é™¤å…¶ä¸­çš„æ‰€æœ‰æ•°æ®ä¸ºéƒ¨ç½² osd åšå‡†å¤‡
$ ceph orch daemon add osd <hostname>:/path/to/device
$ ceph orch daemon add osd serverd.lab.example.com:/dev/vde  #ç¤ºä¾‹
# æ·»åŠ æŒ‡å®šä¸»æœºä¸Šçš„ osd è®¾å¤‡
```
  
![ceph-orch-daemon-add-osd-demo](images/ceph-orch-daemon-add-osd-demo.png)

### Lab: ä½¿ç”¨ osd specification file éƒ¨ç½² OSD è®¾å¤‡

å…³äº `--all-available-devices` é€‰é¡¹çš„è¯´æ˜ï¼š

ä½¿ç”¨ `ceph orch apply osd --all-available-devices` å‘½ä»¤æ·»åŠ  `ceph orch device ls` å‘½ä»¤è¿”å›ä¸º Available çŠ¶æ€çš„ osdï¼Œå¦‚æœå°† osd ä»é›†ç¾¤ä¸­åˆ é™¤å¹¶ zap åï¼Œè¿™äº› osd å°†é‡æ–°è‡ªåŠ¨åŠ å…¥åˆ°é›†ç¾¤ä¸­ã€‚æœ‰æ—¶å¯¹äºæ­¤ç§è¡Œä¸ºæ˜¯ä¸éœ€è¦çš„ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹æ³•å»é™¤è‡ªåŠ¨æ·»åŠ åˆ°é›†ç¾¤ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
$ ceph orch ls --service-type osd --format yaml > ./osd-spec.yaml
$ vim ./osd-spec.yaml
service_type: osd
service_id: default_drive_group
service_name: osd.default_drive_group
placement:
  hosts:
  - serverc.lab.example.com
  - serverd.lab.example.com
  - servere.lab.example.com
spec:
  data_devices:
    paths:
    - /dev/vdc
    - /dev/vdd
  filter_logic: AND
  objectstore: bluestore
unmanaged: true    #æ­¤é€‰é¡¹å°†ä¸å†ä½¿ osd è¢«é‡æ–°è‡ªåŠ¨åŠ å…¥é›†ç¾¤
status:
  created: '2025-04-13T06:49:36.576870Z'
  running: 0
  size: 3

$ ceph orch apply -i ./osd-spec.yaml
# é‡æ–°è®¾ç½® osd é…ç½®ä½¿å…¶ç”Ÿæ•ˆ
```
  
### Lab: åˆ é™¤ OSD è®¾å¤‡
  
```bash
$ ceph device ls | awk /<hostname>/
# æŸ¥çœ‹æŒ‡å®šèŠ‚ç‚¹ä¸Šç£ç›˜è®¾å¤‡ä¸ osd çš„å¯¹åº”å…³ç³»
$ ceph orch daemon stop osd.$id
# åœæ­¢æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹
# æ³¨æ„ï¼š
#   1. è‹¥ç™»å½•è‡³ osd æ‰€åœ¨çš„èŠ‚ç‚¹ä½¿ç”¨ systemctl å‘½ä»¤æŸ¥çœ‹ï¼Œå¯å‘ç°æ­¤æœåŠ¡å•å…ƒå·²åœæ­¢ã€‚
#   2. å¯åœ¨æ­¤èŠ‚ç‚¹ä¸Šä½¿ç”¨ systemctl å‘½ä»¤å¯åŠ¨æ­¤ osd å®ˆæŠ¤è¿›ç¨‹
$ ceph orch daemon rm osd.$id --force
# å°†æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹å¼ºåˆ¶ç§»é™¤
$ ceph osd rm $id
# å°† osd ä»é›†ç¾¤ osdmapï¼ˆosd æ˜ å°„ï¼‰ä¸­åˆ é™¤
# æ³¨æ„ï¼š
#   1. æ­¤æ—¶ï¼Œceph -s é›†ç¾¤çŠ¶æ€å°†æ˜¾ç¤º crushmap ä¸­ä¾ç„¶ä¿ç•™ç€ osdï¼Œè€Œ osdmap ä¸­å·²ç§»é™¤ osdï¼Œä¸¤è€…çš„çŠ¶æ€ä¸åŒ¹é…ã€‚
#   2. ceph osd tree æ˜¾ç¤ºçš„ osd çŠ¶æ€ä¸º DNEï¼Œå½“ä» crushmap ä¸­ç§»é™¤åå°†æ¸…é™¤ã€‚
$ ceph orch osd rm status
# æŸ¥çœ‹ osd åˆ é™¤çš„çŠ¶æ€

$ ceph osd crush rm $id
# å°† osd ä»é›†ç¾¤ crushmapï¼ˆCRUSH æ˜ å°„ï¼‰ä¸­åˆ é™¤
# æ³¨æ„ï¼šè‹¥åç»­è¿˜éœ€è¦å°† osd æ·»åŠ åˆ°é›†ç¾¤ä¸­çš„è¯ï¼Œé‚£ä¹ˆåœ¨ crushmap ä¸­å¯ç»§ç»­ä¿ç•™ï¼
```
  
![ceph-status-with-noosd-in-osdmap](images/ceph-status-with-noosd-in-osdmap.png)

### Lab: é…ç½® ceph è½¯ä»¶æº

RHCS5 (Ceph Pacific 16.x) ä¸­å…¨é¢ä½¿ç”¨ `cephadm shell` å¯ç”¨å®¹å™¨åŒ–å¯¹ Ceph é›†ç¾¤çš„ç®¡ç†ï¼Œåœ¨å®¹å™¨ä¸­åŒ…å«æœ‰ `ceph-volume` å‘½ä»¤è¡Œå·¥å…·ï¼Œè€Œåœ¨å®¿ä¸»æœºç³»ç»Ÿä¸­é»˜è®¤ä¸å®‰è£…æ­¤å·¥å…·ï¼Œéœ€è¦é…ç½® ceph è½¯ä»¶æºè¿›è¡Œå®‰è£…ã€‚

```bash
$ sudo cat /etc/yum.repos.d/ceph.repo <<EOF
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://mirrors.163.com/ceph/rpm-pacific/el8/x86_64/
enabled=1
gpgcheck=1
priority=1
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
EOF

$ sudo dnf provides ceph-volume  #ceph-volume å·¥å…·çš„è½¯ä»¶åŒ…
Last metadata expiration check: -836 days, 2:08:01 ago on Wed 16 Apr 2025 02:13:18 AM EDT.
ceph-osd-2:16.2.15-0.el8.x86_64 : Ceph Object Storage Daemon
Repo        : @System
Matched from:
Filename    : /usr/sbin/ceph-volume

ceph-osd-2:16.2.15-0.el8.x86_64 : Ceph Object Storage Daemon
Repo        : ceph-noarch
Matched from:
Filename    : /usr/sbin/ceph-volume

$ sudo dnf install -y ceph-osd
```

ğŸ”¥ å…³äº `ceph-volume` çš„è¯´æ˜ï¼š

ä» RHCS4 å¼€å§‹å¼•å…¥ `ceph-volume` å‘½ä»¤ç”¨äºåˆ›å»ºåŸºäº `BlueStore` å­˜å‚¨å¼•æ“çš„ OSDï¼Œåˆ†åˆ«ä½¿ç”¨ `ceph-volume lvm prepare` ä¸ `ceph-volume lvm activate` å­å‘½ä»¤åˆ›å»ºä¸æ¿€æ´» OSD è®¾å¤‡ï¼Œè¿‡ç¨‹å¦‚ä¸‹ç¤ºæ„ï¼š

![ceph-volume-add-bluestore-osd-backend](images/ceph-volume-add-bluestore-osd-backend.jpg)

![ceph-volume-add-bluestore-osd-backend-status](images/ceph-volume-add-bluestore-osd-backend-status.jpg)
  
![ceph-volume-active-bluestore-osd](images/ceph-volume-active-bluestore-osd.jpg)

### Lab: RHCS5 ä¸­ä½¿ç”¨ ceph-volume æ‰‹åŠ¨éƒ¨ç½² OSD

è™½ç„¶ä½¿ç”¨ `osd specification file` å®šä¹‰éœ€éƒ¨ç½²çš„ osd ç£ç›˜çµæ´»ä¸”ä¾¿æ·ï¼Œä½†æœ‰æ—¶å¯èƒ½éœ€è¦å°†å¤šä¸ª osd çš„ `wal` ä¸ `rocksdb` éƒ¨ç½²åœ¨åŒä¸€ç£ç›˜ä¸Šï¼ˆé«˜æ€§èƒ½çš„ SSD æˆ– NVMeï¼‰åŠ é€Ÿ osd çš„è¯»å†™æ€§èƒ½ï¼Œå¯é€šè¿‡æ‰‹åŠ¨æ·»åŠ çš„æ–¹å¼å®Œæˆï¼ˆä¹ŸåŒæ ·å¯ä»¥é€šè¿‡ osd å®šä¹‰æ–‡ä»¶çš„æ–¹å¼å®ç°ï¼‰ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
[root@servere ~]# lsblk -p
...
vde               252:64   0   10G  0 disk
â”œâ”€vde1            252:65   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdb osd çš„ wal
â”œâ”€vde2            252:66   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdb osd çš„ rocksdb
â”œâ”€vde3            252:67   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdc osd çš„ wal
â”œâ”€vde4            252:68   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdc osd çš„ rocksdb
â”œâ”€vde5            252:69   0    1G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdd osd çš„ wal
â””â”€vde6            252:70   0    2G  0 part  #æœ¬èŠ‚ç‚¹ /dev/vdd osd çš„ rocksdb
...
# æ³¨æ„ï¼š
#   1. æ­¤å¤„å…ˆä¸è€ƒè™‘å•ä¸ª osd çš„æ•°æ®ã€wal ä¸ rocksdb ä¹‹é—´çš„å®¹é‡åˆ†é…æ¯”ï¼Œæ¼”ç¤ºå¦‚ä½•å°† /dev/vde åˆ†é…ç»™ 3 ä¸ª osd çš„ wal ä¸ rocksdbã€‚
#   2. ä»¥ä¸Šåˆ†åŒºéœ€é¢„å…ˆæ‰‹åŠ¨å®Œæˆåˆ†åŒºï¼ˆfdisk/gdisk/parted å‡å¯ï¼‰

[root@servere ~]# ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring
# å¯¼å‡º bootstrap-osd çš„ keyring æ–‡ä»¶ç”¨äºé›†ç¾¤ä¸­æ–°å¢ osd çš„è®¤è¯å‡­æ®
# æ³¨æ„ï¼š
#   è‹¥ä¸ç”Ÿæˆæ­¤ ceph.keyring æ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨ä½¿ç”¨ ceph-volume lvm create å‘½ä»¤æ·»åŠ  osd æ—¶æŠ¥é”™å¦‚ä¸‹ï¼š
#   stderr: 2023-01-01T01:29:07.360+0000 7febf169a700 -1 auth: unable to find a keyring on /var/lib/ceph/bootstrap-osd/ceph.keyring: (2) No such file or directory
#   stderr: 2023-01-01T01:29:07.360+0000 7febf169a700 -1 AuthRegistry(0x7febec05b3f8) no keyring found at /var/lib/ceph/bootstrap-osd/ceph.keyring, disabling cephx
#   ä»¥ä¸Šæç¤º ceph.keyring æ–‡ä»¶ä¸å­˜åœ¨å¯¼è‡´è®¤è¯å¤±è´¥ã€‚

[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdb --block.wal /dev/vde1 --block.db /dev/vde2
Running command: /usr/bin/ceph-authtool --gen-print-key
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 03e01019-a426-4190-b711-b9e3ad21dd5a
Running command: vgcreate --force --yes ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a /dev/vdb
 stdout: Physical volume "/dev/vdb" successfully created.
 stdout: Volume group "ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a" successfully created
Running command: lvcreate --yes -l 2559 -n osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a
 stdout: Logical volume "osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a" created.
Running command: /usr/bin/ceph-authtool --gen-print-key
Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-6
Running command: /usr/sbin/restorecon /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/chown -h ceph:ceph /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/ln -s /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-6/activate.monmap
 stderr: got monmap epoch 4
--> Creating keyring file for osd.6  #åˆ›å»º osd.6 çš„ keyring æ–‡ä»¶
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6/keyring
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6/
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 6 --monmap /var/lib/ceph/osd/ceph-6/activate.monmap --keyfile - --bluestore-block-wal-path /dev/vde1 --bluestore-block-db-path /dev/vde2 --osd-data /var/lib/ceph/osd/ceph-6/ --osd-uuid 03e01019-a426-4190-b711-b9e3ad21dd5a --setuser ceph --setgroup ceph
 stderr: 2022-12-31T21:25:21.005-0500 7f85c0737380 -1 bluestore(/var/lib/ceph/osd/ceph-6/) _read_fsid unparsable uuid
--> ceph-volume lvm prepare successful for: /dev/vdb  #æ·»åŠ  osd å‡†å¤‡é˜¶æ®µå®Œæˆ
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a --path /var/lib/ceph/osd/ceph-6 --no-mon-config
Running command: /usr/bin/ln -snf /dev/ceph-eb4fe887-63fa-4bdf-8bd4-1f0f9a4fbb2a/osd-block-03e01019-a426-4190-b711-b9e3ad21dd5a /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-6
Running command: /usr/bin/ln -snf /dev/vde2 /var/lib/ceph/osd/ceph-6/block.db
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block.db
Running command: /usr/bin/chown -R ceph:ceph /dev/vde2
Running command: /usr/bin/ln -snf /dev/vde1 /var/lib/ceph/osd/ceph-6/block.wal
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-6/block.wal
Running command: /usr/bin/chown -R ceph:ceph /dev/vde1
Running command: /usr/bin/systemctl enable ceph-volume@lvm-6-03e01019-a426-4190-b711-b9e3ad21dd5a
 stderr: Created symlink /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-6-03e01019-a426-4190-b711-b9e3ad21dd5a.service â†’ /usr/lib/systemd/system/ceph-volume@.service.
Running command: /usr/bin/systemctl enable --runtime ceph-osd@6
 stderr: Created symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@6.service â†’ /usr/lib/systemd/system/ceph-osd@.service.
Running command: /usr/bin/systemctl start ceph-osd@6
--> ceph-volume lvm activate successful for osd ID: 6
--> ceph-volume lvm create successful for: /dev/vdb
# osd åˆ›å»ºæ·»åŠ æ—¥å¿—ï¼š/dev/vdb ä½œä¸º osd çš„æ•°æ®ç›˜ï¼Œ/dev/vde1 ä½œä¸º osd çš„ walï¼Œ/dev/vde2 ä½œä¸º osd çš„ rocksdbã€‚
# æ­¤ osd ä½¿ç”¨ BlueStore å­˜å‚¨é©±åŠ¨ï¼Œ/dev/vdb åœ¨åç»­è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ›å»ºä¸ºé€»è¾‘å·ã€‚
[root@servere ~]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
...
vdb                                                                                                   252:16   0   10G  0 disk 
â””â”€ceph--eb4fe887--63fa--4bdf--8bd4--1f0f9a4fbb2a-osd--block--03e01019--a426--4190--b711--b9e3ad21dd5a 253:0    0   10G  0 lvm
...
# osd.6 å¯¹åº”çš„é€»è¾‘å·å·²åˆ›å»º
[root@servere ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.07047  root default                               
-3         0.02939      host serverc                           
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
-5         0.02939      host serverd                           
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
-9         0.01169      host servere                           
 6    hdd  0.01169          osd.6         up   1.00000  1.00000  #osd.6 å·²åŠ å…¥é›†ç¾¤

[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdc --block.wal /dev/vde3 --block.db /dev/vde4
[root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdd --block.wal /dev/vde5 --block.db /dev/vde6
[root@servere ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.09384  root default                               
-3         0.02939      host serverc                           
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
-5         0.02939      host serverd                           
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
-9         0.03506      host servere                           
 6    hdd  0.01169          osd.6         up   1.00000  1.00000
 7    hdd  0.01169          osd.7         up   1.00000  1.00000
 8    hdd  0.01169          osd.8         up   1.00000  1.00000
# 3 ä¸ª osd å·²å…¨éƒ¨åŠ å…¥é›†ç¾¤ä¸­

[root@servere ~]# ceph-volume lvm list
# æŸ¥çœ‹ osd ä¸ lv çš„å¯¹åº”å…³ç³»ï¼ˆå¿…é¡»åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šè¿è¡Œï¼‰

[root@servere ~]# ceph orch ps --daemon_type=osd
NAME   HOST                     STATUS        REFRESHED  AGE  PORTS  VERSION           IMAGE ID      CONTAINER ID
osd.0  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  214f946e935e
osd.1  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  169b37f63a95
osd.2  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  605c9c7817a7
osd.3  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  32495d4269b9
osd.4  serverd.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  31990acee22c
osd.5  serverc.lab.example.com  running (2d)  9m ago     2d   -      16.2.0-117.el8cp  2142b60d7974  0a0963f26a56
# ä½†è¯·æ³¨æ„ï¼Œç”±äºæ˜¯é€šè¿‡æ‰‹åŠ¨æ·»åŠ  osdï¼Œè¿™äº›æ·»åŠ çš„ osd ä¸è¢« ceph ç¼–æ’å™¨ç®¡ç†ï¼Œå› æ­¤ä¸åœ¨ä»¥ä¸Šè¿”å›çš„ osd åˆ—è¡¨ä¸­ï¼
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œç›´æ¥åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šä½¿ç”¨ ceph-volume å‘½ä»¤å¯æˆåŠŸæ·»åŠ  osdï¼Œä½†åœ¨ cephadm è¿è¡Œçš„å®¹å™¨ä¸­ä½¿ç”¨åŒæ ·çš„æ–¹æ³•æ— æ³•å®Œæˆ osd çš„æ·»åŠ ã€‚å› ä¸ºåœ¨æ·»åŠ  osd çš„è¿‡ç¨‹ä¸­ï¼Œå°†ä¼šä½¿ç”¨ `systemctl` ç®¡ç† osd æœåŠ¡çš„å®ˆæŠ¤è¿›ç¨‹ï¼Œéœ€è¿æ¥èŠ‚ç‚¹çš„ bus æ€»çº¿ï¼Œè€Œåœ¨å®¹å™¨ä¸­æ— æ³•å®Œæˆè€ŒæŠ¥é”™ã€‚è¿”å›å¦‚ä¸‹ï¼š

```bash
[ceph: root@servere ~]# ceph-volume lvm create --bluestore --data /dev/vdb --block.wal /dev/vde1 --block.db /dev/vde2
...
 stderr: Failed to connect to bus: No such file or directory
--> Was unable to complete a new OSD, will rollback changes
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd purge-new osd.6 --yes-i-really-mean-it
 stderr: purged osd.6
-->  RuntimeError: command returned non-zero exit status: 1
```

è‹¥ ceph-volume lvm create æ·»åŠ  osd å¤±è´¥ï¼Œä¼šä¿ç•™åˆ›å»ºçš„å·ç»„ä¸é€»è¾‘å·ï¼Œä»¥åŠ `/var/lib/ceph/osd/*` ä¸­çš„å¯¹åº”ç›®å½•ã€‚å› æ­¤ï¼Œå›é€€é‡æ–°éƒ¨ç½²çš„è¯ï¼Œéœ€å°†å…¶å…¨éƒ¨åˆ é™¤ï¼Œå¯å‚è€ƒå¦‚ä¸‹ï¼š

```bash
[root@servere ~]# vgs  #æŸ¥è¯¢åˆ›å»ºçš„å·ç»„
[root@servere ~]# vgremove -f <vg_name>  #åˆ é™¤å·²åˆ›å»ºçš„å·ç»„
[root@servere ~]# rm -rf /var/lib/ceph/osd/*  #åˆ é™¤å·²åˆ›å»ºçš„ osd ç›®å½•
# é‡æ–°æ‰§è¡Œåˆ›å»º osd çš„å‘½ä»¤å®Œæˆåˆ›å»º
```

## RADOS å¯¹è±¡æ“ä½œ

```bash
$ rados -p <pool_name> put <object_name> /path/to/file
# æŒ‡å®šæ–‡ä»¶å°†å…¶ä¸Šä¼ è‡³é›†ç¾¤ä¸­ä»¥æŒ‡å®šçš„åç§°å‘½å
$ rados -p <pool_name> ls
# æŸ¥çœ‹æŒ‡å®šå­˜å‚¨æ± ä¸­çš„å¯¹è±¡
```

## Ceph Pool å­˜å‚¨æ± 

### Ceph å­˜å‚¨æ± çŠ¶æ€

```bash
$ ceph osd lspools
$ ceph osd ls pool detail
# ä»¥ä¸Šä¸¤ä¸ªå‘½ä»¤ç”¨äºæŸ¥çœ‹é›†ç¾¤ä¸­çš„å­˜å‚¨æ± 

$ rados df
# RADOS å±‚é¢æŸ¥è¯¢é›†ç¾¤çŠ¶æ€
$ ceph df
# Ceph é›†ç¾¤ä¸å­˜å‚¨æ± çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
$ ceph osd pool stats
# ç¡®è®¤æ‰€æœ‰å­˜å‚¨æ± çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
```

### Ceph å¤åˆ¶æ± å‘½ä»¤

```bash
$ ceph osd pool create <pool_name> \
  <pg_num> <pgp_num> [replicated] <crush_rule_set>
# åˆ›å»ºå¤åˆ¶æ± 
# æ³¨æ„ï¼š
#   1. é»˜è®¤æƒ…å†µä¸‹å¯åªæŒ‡å®šå­˜å‚¨æ± åç§°ä¸ PG æ•°é‡ï¼Œä¹Ÿå¯è‡ªå®šä¹‰ CRUSH æ”¾ç½®è§„åˆ™åœ¨åˆ›å»ºå­˜å‚¨æ± æ—¶æŒ‡å®šã€‚
#   2. è‹¥è®¾ç½®äº†ç›¸åŒçš„ pg_num ä¸ pgp_numï¼Œä¹‹åå†è°ƒæ•´ pg_num å°†è‡ªåŠ¨è°ƒæ•´ pgp_numã€‚

$ ceph osd pool application enable <pool_name> [rbd|rgw|cephfs]
# è®¾ç½®æŒ‡å®šçš„å­˜å‚¨æ± ä¸º rbdã€rgw æˆ– cephfs åº”ç”¨ç±»å‹

$ ceph osd pool get <pool_name> all
# æŸ¥çœ‹å­˜å‚¨æ± çš„æ‰€æœ‰å‚æ•°å€¼
$ ceph osd pool get <pool_name> [size|nodelete|min_size]
# æŸ¥çœ‹å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼
$ ceph osd pool get <pool_name> pg_autoscale_mode
# æŸ¥çœ‹å­˜å‚¨æ± çš„ PG è‡ªåŠ¨æ‰©å±•æ¨¡å¼
$ ceph osd pool set <pool_name> [size|nodelete|min_size] <value>
# è®¾ç½®å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼

$ ceph osd pool get mon osd_pool_default_size
# æŸ¥çœ‹é›†ä¸­é…ç½®æ•°æ®åº“ä¸­å®šä¹‰çš„é»˜è®¤å¯¹è±¡å‰¯æœ¬æ•°é‡

$ ceph osd pool rename <old-pool_name> <new-pool_name>
# é‡å‘½åå­˜å‚¨æ± åç§°ï¼ˆä¸å½±å“æ± ä¸­å­˜å‚¨çš„æ•°æ®ï¼Œä½†éœ€è¦æ³¨æ„ç”¨æˆ·å¯¹æ± çš„æƒé™ï¼‰
```

### Ceph çº åˆ ä»£ç æ± å‘½ä»¤

```bash
$ ceph osd erasure-code-profile ls
# æŸ¥çœ‹çº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile get <profile_name>
# æŸ¥çœ‹æŒ‡å®šçº åˆ ä»£ç æ± çš„ profile å…·ä½“ä¿¡æ¯
$ ceph osd erasure-code-profile set <profile_name> k=<num> m=<num> crush-failure-domain=[osd|host|rack]
# åˆ›å»ºçº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile set myerasure k=3 m=2 crush-failure-domain=rack  #ç¤ºä¾‹
# rack ç±»å‹çš„æ•…éšœæ¢å¤åŸŸå¿…é¡»æå‰åœ¨ CRUSH map ä¸­æ„å»º
$ ceph osd erasure-code-profile rm <profile_name>
# åˆ é™¤æŒ‡å®šçš„çº åˆ ä»£ç æ± çš„ profile
$ ceph osd pool create <pool_name> <pg_num> <pgp_num> erasure <profile_name> <crush_rule_set>
# åˆ›å»ºçº åˆ ä»£ç æ± 
```

å…³äºçº åˆ ä»£ç  profile çš„è®¾ç½® RedHat ç»™å‡ºä»¥ä¸‹æ¨èæ–¹å¼ï¼š

<center><img src="images/redhat-reasure-code-profile-recommanded.jpg" style="width:80%"></center>

## CephX è®¤è¯ä¸ç”¨æˆ·

### CephX è®¤è¯æœºåˆ¶

<center><img src="images/cephx-request-1.jpg""></center>

Ceph å®¢æˆ·ç«¯ä½¿ç”¨ CephX åè®®å‘ monitor å‘é€ç”¨æˆ·åˆ›å»ºè¯·æ±‚ï¼Œå½“ monitor åˆ›å»ºç”¨æˆ·åå°†å­˜å‚¨ç”¨æˆ·åã€keyring ä¸ capability ç­‰ä¿¡æ¯ï¼Œå¹¶å°†ç›¸åŒçš„ keyring æ–‡ä»¶è¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå…¶ä¸­ keyring æ–‡ä»¶ä¸­çš„ key ä¸º `secret key`ï¼Œç”¨äºåŠ å¯†ä¸è§£å¯† monitor ç”Ÿæˆçš„ä¼šè¯å¯†é’¥ï¼ˆ`session key`ï¼‰ã€‚

<center><img src="images/cephx-request-2.jpg"></center>

<center><img src="images/cephx-request-4.png"></center>

æ¯ä¸ª monitor éƒ½å¯ä»¥å¯¹å®¢æˆ·ç«¯è¿›è¡Œèº«ä»½éªŒè¯å¹¶åˆ†å‘å¯†é’¥ï¼Œè¿™æ„å‘³ç€è®¤è¯ä¾é  monitor èŠ‚ç‚¹å®Œæˆï¼Œä¸ä¼šå­˜åœ¨å•ç‚¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚monitor ä¼šè¿”å›ç”¨äºèº«ä»½éªŒè¯çš„æ•°æ®ç»“æ„ï¼Œå…¶ä¸­åŒ…å«è·å– Ceph æœåŠ¡æ—¶ç”¨åˆ°çš„ session keyã€‚æ‰€è°“ session key å°±æ˜¯å®¢æˆ·ç«¯ç”¨æ¥å‘ monitor è¯·æ±‚æ‰€éœ€æœåŠ¡çš„å‡­è¯ï¼Œ`session key` æ˜¯é€šè¿‡å®¢æˆ·ç«¯çš„ `secret key` è¿›è¡ŒåŠ å¯†ä¼ è¾“ã€‚

ğŸš€ å½“ monitor æ”¶åˆ°å®¢æˆ·ç«¯è®¤è¯è¯·æ±‚åï¼Œé¦–å…ˆç”Ÿæˆ session keyï¼Œç„¶åç”¨å®¢æˆ·ç«¯çš„ secret key åŠ å¯†session keyï¼Œå†å‘é€ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯ç”¨è‡ªèº«çš„ secret key å°†å…¶è§£å¯†ï¼Œæ‹¿åˆ° session keyã€‚å®¢æˆ·ç«¯è·å– session key ä¹‹åï¼Œå®ƒå°±å¯ä»¥ç”¨æ­¤ session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œmonitor æ”¶åˆ°å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼ˆæºå¸¦ session keyï¼‰ï¼Œæ­¤æ—¶ monitor ä¼šå‘å®¢æˆ·ç«¯æä¾›ä¸€ä¸ª `ticket`ï¼ˆç¥¨æ®ï¼‰ï¼Œç„¶åä½¿ç”¨ session key åŠ å¯†åå‘é€ç»™å®¢æˆ·ç«¯ã€‚éšåå®¢æˆ·ç«¯ä½¿ç”¨ session key è§£å¯†æ­¤ç¥¨æ®ï¼Œä½¿ç”¨æ­¤ç¥¨æ®åˆ°å¯¹åº” OSD å®Œæˆè®¤è¯ã€‚

ä»¥ä¸Šè¿‡ç¨‹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¦–å…ˆï¼Œå®¢æˆ·ç«¯çš„ secret key æ˜¯é€šè¿‡ monitor èŠ‚ç‚¹åœ¨åˆ›å»ºç”¨æˆ·å¸å·æ—¶å°±ç”Ÿæˆï¼Œæ‰€ä»¥ monitor èŠ‚ç‚¹æœ‰å¯¹åº”å®¢æˆ·ç«¯çš„ secret keyï¼Œé€šè¿‡å®¢æˆ·ç«¯çš„ secret key åŠ å¯†ï¼Œå®¢æˆ·ç«¯å¯ä»¥ç”¨è‡ªèº«çš„ secret key è§£å¯†ã€‚å…¶æ¬¡ï¼Œmonitor èŠ‚ç‚¹ç”Ÿæˆçš„ session key æ˜¯æœ‰è®°å½•çš„ï¼Œæ‰€ä»¥å¯¹äºä¸åŒå®¢æˆ·ç«¯æ¥è¯´ï¼Œéƒ½æœ‰ä¸åŒçš„è®°å½•ï¼Œå¹¶ä¸”æ­¤ session key æ˜¯æœ‰æ—¶é—´é™åˆ¶çš„ï¼Œè¿‡æœŸå³ä¾¿æ˜¯å¯¹åº”å®¢æˆ·ç«¯ï¼Œä¹Ÿæ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚æ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨å¯¹åº” session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œå¯¹åº” monitor éƒ½æ˜¯è®¤å¯çš„ï¼Œmonitor ä¼šå‘å…¶å‘æ”¾ ticketã€‚æœ€åï¼Œmonitor å’Œ OSD éƒ½å…±äº«å®¢æˆ·ç«¯çš„ secret key å’Œ session keyï¼Œä»¥åŠ monitor å‘æ”¾çš„ ticketï¼Œæ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨ monitor å‘æ”¾çš„ ticketï¼Œå¯¹åº” OSD æ˜¯è®¤å¯çš„ã€‚è¿™ä¹Ÿæ„å‘³ç€ä¸ç®¡æ˜¯å“ªä¸ª monitor èŠ‚ç‚¹å‘æ”¾çš„ ticketï¼Œå¯¹åº”æ‰€æœ‰ monitor èŠ‚ç‚¹å’Œ OSD éƒ½å¯è®¤è¯ã€‚

<center><img src="images/cephx-request-3.png"></center>

ğŸ· å…³äº CephX è®¤è¯æœºåˆ¶æ›´å¤šå¯å‚è€ƒ [HIGH AVAILABILITY AUTHENTICATION](https://docs.ceph.com/en/latest/architecture/#high-availabilityauthentication) ä¸­çš„è¯´æ˜ã€‚

### CephX ç”¨æˆ·è®¤è¯ç®¡ç†å‘½ä»¤

```bash
$ ceph auth get-or-create [--id <name>|--name client.<name>] client.<name> \
  mon 'allow r' osd 'allow rw' \
  -o /path/to/ceph.client.<name>.keyring
# ä½¿ç”¨æŒ‡å®šçš„ Ceph ç”¨æˆ·åˆ›å»ºæ–°ç”¨æˆ·å¹¶å°†å…¶ secret key æ³¨å…¥ keyring æ–‡ä»¶
# ä¹Ÿå¯ä½¿ç”¨ --keyring é€‰é¡¹æŒ‡å®š keyring æ–‡ä»¶ç”¨äºåˆ›å»ºæ–°ç”¨æˆ·

### é™åˆ¶ç”¨æˆ·çš„è®¿é—® ###
$ ceph auth get-or-create client.<name> \
  mon 'profile rbd' osd 'profile rbd pool=<pool_name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå­˜å‚¨æ± ä¸­çš„ RBD é•œåƒ
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw object_prefix <prefix-name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå‰ç¼€çš„å¯¹è±¡
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw namespace=<namespace>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®š namespace ä¸­çš„å¯¹è±¡
$ ceph fs authorize cephfs client.<name> /path/to/cephfs rw
# é™åˆ¶ç”¨æˆ·åªèƒ½è¯»å†™ CephFS ä¸­çš„æŒ‡å®šè·¯å¾„
$ ceph auth get-or-create client.<name> \
  mon 'allow r, allow command "auth get-or-create", allow command "auth list"'
# é€šè¿‡ monitor å‘½ä»¤é™åˆ¶ç”¨æˆ·åªèƒ½è¿è¡ŒæŒ‡å®šçš„ ceph å‘½ä»¤

$ ceph auth list
# åˆ—å‡ºæ‰€æœ‰çš„ Ceph ç”¨æˆ·
$ ceph auth get client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
$ ceph auth print-key client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„ secret key
$ ceph auth export client.<name> > /path/to/keyring
# å¯¼å‡ºæŒ‡å®šç”¨æˆ·çš„ keyring è‡³æ–‡ä»¶
$ ceph auth import -i /path/to/keyring
# æŒ‡å®šç”¨æˆ·çš„ keyring å¯¼å…¥é›†ç¾¤

$ ceph auth caps client.<name> mon 'allow r' osd 'allow rw'
# è¦†ç›–æ›´æ–°å½“å‰æŒ‡å®šç”¨æˆ·çš„ç°æœ‰åŠŸèƒ½
$ ceph auth caps client.<name> osd ''
# ä½¿ç”¨ç©ºå­—ç¬¦åˆ é™¤æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰åŠŸèƒ½

$ ceph auth del client.<name>
# åˆ é™¤é›†ä¸­é…ç½®æ•°æ®åº“ä¸­çš„æŒ‡å®šç”¨æˆ·
$ rm -f /path/to/keyring
# åˆ é™¤æŒ‡å®šç”¨æˆ·çš„ keyring æ–‡ä»¶ï¼ˆè‹¥å­˜åœ¨çš„è¯ï¼‰
```

## Ceph RBD é•œåƒ

### RBD é•œåƒçš„ç‰¹æ€§ï¼ˆfeatureï¼‰

- `layering`ï¼šæ˜¯å¦æ”¯æŒé•œåƒåˆ†å±‚ã€å…‹éš†ï¼ˆBIT ç ä¸º 1ï¼‰
- `exclusive-lock`ï¼šæ˜¯å¦æ”¯æŒåˆ†å¸ƒå¼ç‹¬å é”æœºåˆ¶ä»¥é™åˆ¶åŒæ—¶ä»…èƒ½ä¸€ä¸ªå®¢æˆ·ç«¯è®¿é—®å½“å‰é•œåƒï¼ˆBIT ç ä¸º 4ï¼‰
- `object-map`ï¼šæ˜¯å¦æ”¯æŒå¯¹è±¡æ˜ å°„ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿå¯¼å…¥ã€å¯¼å‡ºåŠå·²ç”¨å®¹é‡ç»Ÿè®¡ç­‰æ“ä½œï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 8ï¼‰ã€‚
- `fast-diff`ï¼šæ˜¯å¦æ”¯æŒå¿«ç…§é—´çš„å¿«é€Ÿæ¯”è¾ƒæ“ä½œï¼Œä¾èµ–äº object-map ç‰¹æ€§ï¼ˆBIT ç ä¸º 16ï¼‰ã€‚
- `deep-flatten`ï¼šæ˜¯å¦æ”¯æŒå…‹éš†åˆ†ç¦»æ—¶è§£é™¤åœ¨å…‹éš†é•œåƒæ—¶åˆ›å»ºçš„å¿«ç…§ä¸å…¶çˆ¶é•œåƒä¹‹é—´çš„å…³è”ï¼Œå³å¿«ç…§æ‰å¹³åŒ–æ“ä½œï¼ˆBIT ç ä¸º 32ï¼‰ã€‚
- `journaling`ï¼šæ˜¯å¦æ”¯æŒæ—¥å¿— I/O æ“ä½œï¼Œå³æ˜¯å¦æ”¯æŒè®°å½•é•œåƒçš„ä¿®æ”¹æ“ä½œè‡³æ—¥å¿—å¯¹è±¡ï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 64ï¼‰ã€‚
- `data-pool`ï¼šæ˜¯å¦æ”¯æŒå°†é•œåƒçš„æ•°æ®å¯¹è±¡å­˜å‚¨äºçº åˆ ç å­˜å‚¨æ± ï¼Œä¸»è¦ç”¨äºå°†é•œåƒçš„å…ƒæ•°æ®ä¸æ•°æ®æ”¾ç½®äºä¸åŒçš„å­˜å‚¨æ± ã€‚
- `striping`: æ˜¯å¦æ”¯æŒæ•°æ®å¯¹è±¡é—´çš„æ•°æ®æ¡å¸¦åŒ–ï¼ˆBIT ç ä¸º 2ï¼‰

### RBD é•œåƒå¸¸ç”¨å‘½ä»¤
  
```bash
$ rbd pool init <pool_name>
# åˆå§‹åŒ–æŒ‡å®šå­˜å‚¨æ± å­˜å‚¨ RBD é•œåƒ
$ rbd create [--id <name>] \
  --size <num>[M|G] [--object-size <num>[M|G]] \
  --image-feature=<feature1>,<feature2>,... \
  #--image-feature=exclusive-lock,journaling \  #ç¤ºä¾‹
  <pool_name>/<rbd_image_name>
# æŒ‡å®š RBD é•œåƒå¤§å°ï¼ˆé»˜è®¤å•ä½ MiBï¼‰ã€chunk çš„å¯¹è±¡å¤§å°ä»¥åŠæ”¯æŒçš„é•œåƒç‰¹æ€§ä»¥åˆ›å»º RBD é•œåƒ
# æ³¨æ„ï¼š
#   1. å…¶ä¸­ layering, exclusive-lock, object-map, fast-diff, deep-flatten ä¸º
#      é»˜è®¤æ”¯æŒçš„é•œåƒç‰¹æ€§
#   2. è‹¥åªéœ€å¯ç”¨ç‰¹å®šçš„é•œåƒç‰¹æ€§å¯ä½¿ç”¨ --image-feature é€‰é¡¹æŒ‡å®šå³å¯
#   3. ä»¥ä¸Šç¤ºä¾‹ä¸­çš„ exclusive-lock ä¸ journaling ç‰¹æ€§åœ¨ RBD Mirror ä¸­å¯ç”¨
  
$ rbd rm [--id <name>] <pool_name>/<rbd_image_name>
$ rbd ls [--id <name>] --pool <pool_name>
$ rbd info [--id <name>] <pool_name>/<rbd_image_name>
$ rbd resize --size <number>[M|G] <pool_name>/<rbd_image_name>
  
$ rbd map [--id <name>] <pool_name>/<rbd_image_name>
$ rbd unmap /dev/rbdX
$ rbd showmapped
# æ³¨æ„ï¼š
#   1. RBD é•œåƒçš„æ˜ å°„äºå®¢æˆ·ç«¯ä¸Šæ‰§è¡Œ
#   2. å®¢æˆ·ç«¯éœ€å®‰è£… ceph-common è½¯ä»¶åŒ…å¹¶åŠ è½½ rbd å†…æ ¸æ¨¡å—
  
$ fsfreeze --freeze /path/to/<mount-point>
$ fsfreeze --unfreeze /path/to/<mount-point>
$ rbd snap create [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd snap remove [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd snap list [--id <name>] <pool_name>/<rbd_image_name>
$ rbd snap protect [--id <name>] <pool_name>/<rbd_image_name>@<snap-name>
$ rbd clone [--id <name>] <pool_name>/<rbd_image_name>@<snap-name> <pool_name>/<clone_name>
# åˆ›å»ºåŸºäº RBD é•œåƒå¿«ç…§çš„å…‹éš†
$ rbd flatten <rbd-image-clone_name>
#åˆ›å»ºæ‰å¹³åŒ–å…‹éš†
  
$ blockdev --getro /path/to/device
```

### Ceph RBD Mirror é›†ç¾¤æ¨¡å¼
  
RBD Mirror çš„ä¸¤ç§æ¨¡å¼ï¼ŒåŒ…æ‹¬ `RBD one-way mirroring` æ¨¡å¼ï¼ˆ`active-backup mode`ï¼‰ã€`RBD two-way mirroring` æ¨¡å¼ï¼ˆ`active-active mode`ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
<center><img src="images/rbd-one-way-mirroring.jpg" style="width:60%"></center>
  
<center><img src="images/rbd-two-way-mirroring.jpg" style="width:60%"></center>
  
### Lab: å®ç° RBD one-way mirroring
  
```bash
### RBD Mirror Primary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰###
$ rbd mirror pool enable <pool_name> <mirror-mode>
# æŒ‡å®šå­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„æŒ‡å®šæ¨¡å¼ï¼ˆpool æ¨¡å¼ã€image æ¨¡å¼ï¼‰
$ rbd mirror pool enable rbd pool  #ç¤ºä¾‹
# å¯ç”¨ rbd å­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„ pool æ¨¡å¼ï¼ˆæ± æ¨¡å¼ï¼‰
$ rbd info rbd/image1  #ç¤ºä¾‹
  rbd image 'image1':
        ...
        features: exclusive-lock, journaling
        ...
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: cccdeb7c-1ce5-4ca4-9498-1400f228bf72
        mirroring primary: true
# æ­¤ç¤ºä¾‹æ˜¾ç¤º rbd å­˜å‚¨æ± å·²å¯ç”¨ pool æ¨¡å¼ï¼Œå¹¶ä¸”å…¶ä¸­çš„ image1 é•œåƒå¯è¿›è¡ŒåŒæ­¥ã€‚
  
$ rbd mirror pool peer bootstrap create \
  --site-name <site-name> <pool_name> > /path/to/mirror-bootstrap-token
# åˆ›å»ºå¼•å¯¼å¯¹ç­‰å­˜å‚¨é›†ç¾¤çš„ bootstrap tokenï¼ˆactive-passive é›†ç¾¤æ¨¡å¼ï¼‰
  
### RBD Mirror Secondary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰###
$ ceph orch apply rbd-mirror --placement=<fqdn>
# æŒ‡å®šèŠ‚ç‚¹å¯ç”¨ rbd-mirror å®ˆæŠ¤è¿›ç¨‹
$ ceph orch apply rbd-mirror --placement=serverf.lab.example.com  #ç¤ºä¾‹
$ ceph orch ls  #ç¤ºä¾‹
  ...
  rbd-mirror                   1/1  8m ago     2h   serverf.lab.example.com
  ...
$ rbd mirror pool peer bootstrap import \
  --site-name <site-name> \
  --direction rx-only <pool_name> \
  /path/to/mirror-bootstrap-token
# å¯¼å…¥ primary é›†ç¾¤æä¾›çš„å¯¹ç­‰é›†ç¾¤å¼•å¯¼ tokenï¼Œå°†è®¾ç½®ä¸º secondary é›†ç¾¤ã€‚
# æ­¤é›†ç¾¤åªèƒ½ä»¥æ¥æ”¶æ–¹å¼ï¼ˆrx-onlyï¼‰å¤‡ä»½åŒæ­¥æŒ‡å®šå­˜å‚¨æ± ä¸­çš„é•œåƒ
  
$ rbd mirror pool info <pool_name>
# æŸ¥çœ‹ RBD Mirror æŒ‡å®šå­˜å‚¨æ± çš„å¯¹ç­‰ä¿¡æ¯
$ rbd mirror pool info rbd  #ç¤ºä¾‹
  Mode: pool
  Site Name: bup
  
  Peer Sites: 
  
  UUID: 763de4dc-ba66-4672-aaec-582b68cf9cf1
  Name: prod
  Direction: rx-only
  Client: client.rbd-mirror-peer
$ rbd mirror pool status
  health: OK
  daemon health: OK
  image health: OK
  images: 1 total
      1 replaying
# æŸ¥çœ‹ RBD Mirror çš„å¯¹ç­‰çŠ¶æ€
# æ³¨æ„ï¼šprimary é›†ç¾¤ä¸­æ— æ³•æŸ¥è¯¢ RBD é•œåƒåŒæ­¥çŠ¶æ€
  
### æŠ¥é”™ç¤ºä¾‹ ###
# RBD Mirror Secondary é›†ç¾¤
$ rbd rm rbd/image1
  2023-10-07T05:14:13.113+0000 7f6c226f3700 -1 librbd::image::PreRemoveRequest: 0x564199439410 handle_exclusive_lock:
  cannot obtain exclusive lock - not removing
  Removing image: 0% complete...failed.
  rbd: error: image still has watchers
  This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 
  30s for the crashed client to timeout.
# ç”±äº primary é›†ç¾¤å­˜å‚¨æ± ä¸­é•œåƒè®¾ç½®äº†åˆ†å¸ƒå¼é”ï¼ˆexclusive-lockï¼‰ç‰¹æ€§ï¼Œ
# å› æ­¤ï¼Œsecondary é›†ç¾¤ä¸­æ— æ³•åˆ é™¤é•œåƒï¼Œåªèƒ½ä» primary é›†ç¾¤ä¸­åˆ é™¤ã€‚
```
  
### Lab: `rbd-nbd` æ˜ å°„ä½¿ç”¨å·² mirroring çš„ RBD é•œåƒ
  
ç¬”è€…ç¯å¢ƒä¸­å·²éƒ¨ç½² RBD Mirror Primary ä¸ Secondary é›†ç¾¤ï¼Œä¸¤å¥—é›†ç¾¤ä¸­å‡å·²åˆ›å»º rbd å­˜å‚¨æ± å¹¶å¯ç”¨ pool æ¨¡å¼çš„ RBD Mirrorã€‚primary é›†ç¾¤çš„ rbd å­˜å‚¨æ± ä¸­å·²åˆ›å»º image1 é•œåƒï¼Œæ­¤é•œåƒå·²å¯ç”¨ `exclusive-lock` ä¸ `journaling` ç‰¹æ€§ã€‚ç°å°†æ­¤é•œåƒæ˜ å°„ç»™å®¢æˆ·ç«¯è™šæ‹Ÿæœºä½œè™šæ‹Ÿç£ç›˜ä½¿ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
```bash
### å®¢æˆ·ç«¯è™šæ‹Ÿæœº ###
$ sudo dnf install -y ceph-common
# å®‰è£… rbd å‘½ä»¤
$ sudo rbd map rbd/image1
  rbd: sysfs write failed
  RBD image feature set mismatch. You can disable features unsupported by 
  the kernel with "rbd feature disable image1 journaling"
  In some cases useful info is found in syslog - try "dmesg | tail".
  rbd: map failed: (6) No such device or address
# rbd æ˜ å°„ rbd/image1 é•œåƒå†…æ ¸æ¨¡å—æŠ¥é”™
# æ³¨æ„ï¼š
#   1. ä½¿ç”¨æ­¤å‘½ä»¤å‰éœ€åŒæ­¥ primary é›†ç¾¤çš„ /etc/ceph/{ceph.conf,
#      ceph.client.admin.keyring} æ–‡ä»¶è‡³æœ¬åœ° /etc/ceph/ ç›®å½•ä¸­
#   2. rbd å†…æ ¸æ¨¡å—æŠ¥é”™æ— æ³•æ˜ å°„æ˜¯ç”±äºå½“å‰å†…æ ¸ä¸æ”¯æŒ journaling ç‰¹æ€§ã€‚
#      ä½†æ˜¯ï¼Œmirroring å¿…é¡»ä¾èµ–æ­¤ç‰¹æ€§ï¼Œå› æ­¤ä½¿ç”¨ rbd æ˜ å°„ä¸ journaling ä¹‹é—´å­˜åœ¨çŸ›ç›¾ã€‚
#      ç›®å‰ç°æœ‰çš„å†…æ ¸ç‰ˆæœ¬å‡ä¸æ”¯æŒæ­¤ç‰¹æ€§ï¼
#   3. å¯ä½¿ç”¨ rbd-nbd æ›¿ä»£ rbd ä»¥å®Œæˆé•œåƒçš„æ˜ å°„ã€‚å¯å‚çœ‹æ–‡æœ«å‚è€ƒé“¾æ¥ã€‚
$ sudo uname -r
  4.18.0-305.el8.x86_64
  
$ sudo dnf install -y rbd-nbd
# å®‰è£… rbd-nbd è½¯ä»¶åŒ…
$ sudo dnf info rbd-nbd
  ...
  Name         : rbd-nbd
  Epoch        : 2
  Version      : 16.2.0
  Release      : 117.el8cp
  Architecture : x86_64
  Size         : 511 k
  Source       : ceph-16.2.0-117.el8cp.src.rpm
  Repository   : @System
  From repo    : rhceph-5-tools-for-rhel-8-x86_64-rpms
  Summary      : Ceph RBD client base on NBD
  URL          : http://ceph.com/
  License      : LGPL-2.1 and LGPL-3.0 and CC-BY-SA-3.0 and GPL-2.0 and 
                 BSL-1.0 and BSD-3-Clause and MIT
  Description  : NBD based client to map Ceph rbd images to local device
$ sudo lsmod | egrep 'rbd|nbd'
  nbd                    49152  2
  rbd                   110592  0
  libceph               454656  1 rbd
# ç¡®è®¤ rbd ä¸ nbd å†…æ ¸æ¨¡å—æ˜¯å¦åŠ è½½
$ sudo rbd-nbd map rbd/image1
  /dev/nbd1
# æ˜ å°„ rbd/image1 é•œåƒä¸º /dev/nbd1
$ sudo mkfs -t ext4 /dev/nbd1
$ sudo mkdir /mnt/rbd
$ sudo mount -t ext4 /dev/nbd1 /mnt/rbd
$ sudo dd if=/dev/zero of=/mnt/rbd/test-data1 oflag=direct bs=4M count=10
  10+0 records in
  10+0 records out
  41943040 bytes (42 MB, 40 MiB) copied, 0.747552 s, 56.1 MB/s
$ sudo ls -lh /mnt/rbd/test-data1 
  -rw-r--r--. 1 root root 40M Oct  7 23:05 /mnt/rbd/test-data1
# åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
  
### RBD Mirror Primary/Secondary é›†ç¾¤ ###
$ rbd info rbd/image1
  rbd image 'image1':
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 8569a03bc09a
        block_name_prefix: rbd_data.8569a03bc09a
        format: 2
        features: exclusive-lock, journaling
        op_features: 
        flags: 
        create_timestamp: Sat Oct  7 17:41:54 2023
        access_timestamp: Sun Oct  8 03:05:02 2023
        modify_timestamp: Sun Oct  8 03:05:02 2023  # é•œåƒä¸­å·²å†™å…¥æ•°æ®
        journal: 8569a03bc09a
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: c28aca80-f176-49d0-9a7c-0f34f2380f51
        mirroring primary: true
# ä»ä»¥ä¸Š primary ä¸ secondary é›†ç¾¤çš„é•œåƒä¿¡æ¯å¯è§æ•°æ®å·²å†™å…¥å¹¶åŒæ­¥
```
  
### å…¶ä»– RBD Mirror ç›¸å…³å‘½ä»¤
  
```bash
$ rbd mirror pool disable <pool_name>
# æŒ‡å®šå­˜å‚¨æ± ç¦ç”¨ pool æ¨¡å¼çš„ RBD Mirror
  
$ rbd feature enable <pool_name>/<rbd_image_name> <feature-name>
# å¯ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
$ rbd feature disable <pool_name>/<rbd_image_name> <feature-name>
# ç¦ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
```
  
![rbd-mirror-other-cmds](images/rbd-mirror-other-cmds.png)
  
### RBD Mirror çš„æ•…éšœè½¬ç§»
  
å½“ primary é›†ç¾¤ä¸­çš„ RBD é•œåƒå˜ä¸ºä¸å¯ç”¨æ—¶ï¼ˆéé›†ç¾¤ä¸å¯ç”¨ï¼‰ï¼Œå¯å¯¹ primary é›†ç¾¤æ‰§è¡Œé•œåƒçš„é™çº§ï¼ˆdemoteï¼‰æ“ä½œï¼Œè€Œå¯¹ secondary é›†ç¾¤æ‰§è¡Œé•œåƒçš„å‡çº§ï¼ˆpromoteï¼‰æ“ä½œã€‚
  
```bash
$ rbd mirror image demote <pool_name>/<rbd_image_name>
  Image demoted to non-primary
# primary é›†ç¾¤èŠ‚ç‚¹ï¼šé™çº§æ‰§è¡ŒæŒ‡å®šçš„ RBD é•œåƒ
$ rbd mirror image promote <pool_name>/<rbd_image_name>
  Image promoted to primary
# secondary é›†ç¾¤èŠ‚ç‚¹ï¼šå‡çº§æŒ‡å®šçš„ RBD é•œåƒ
```

- Ceph iSCSI Gateway ç›¸å…³å‘½ä»¤ï¼š

## CephFS æ–‡ä»¶ç³»ç»Ÿ

```bash
$ ceph fs status
```

## å‚è€ƒé“¾æ¥

- [Product Documentation for Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5)
- â¤ [5.4.Â ä½¿ç”¨ Ceph Manager è´Ÿè½½å‡è¡¡å™¨æ¨¡å— Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5/html/operations_guide/using-the-ceph-manager-balancer-module_ops)
- [SERVICE MANAGEMENT | Ceph Docs](https://docs.ceph.com/en/latest/cephadm/services/?highlight=service_id#)
- [ä½¿ç”¨ cephadm æ­å»º cephï¼ˆoctopusï¼‰è¿‡ç¨‹](https://juejin.cn/post/7160585472538837006)
- [ChapterÂ 2.Â Management of services using the Ceph Orchestrator Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/operations_guide/management-of-services-using-the-ceph-orchestrator#deploying-the-ceph-daemons-using-the-service-specification_ops)
- [Stray daemon tcmu-runner is reported not managed by cephadm - Red Hat Customer Portal](https://access.redhat.com/solutions/6472281)
- [Ceph - mapping rbd image is failing with RBD image feature set mismatch or image uses unsupported features - Red Hat Customer Portal](https://access.redhat.com/solutions/4270092)
- [maillist - rbd map image with journaling](https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/377S7XFN74MUYKVSXXRAN534FNZTDICK/)
- [ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢æ›´æ”¹ Ceph ä»ªè¡¨æ¿å¯†ç ](https://www.ibm.com/docs/zh/storage-ceph/6?topic=ia-changing-ceph-dashboard-password-using-command-line-interface)
