# ğŸ™ Red Hat Ceph Storage v5.0 å¸¸ç”¨å‘½ä»¤æ±‡æ€»

## æ–‡æ¡£è¯´æ˜

- è¯¥æ–‡æ¡£æ¶‰åŠçš„å‘½ä»¤åœ¨ `Red Hat Ceph Storage v5.0` é›†ç¾¤ä¸­è¿›è¡ŒéªŒè¯ã€‚
- Red Hat Ceph Storage v5.0 å¯¹åº” `Ceph 16.2.0 pacific (stable)` å¼€æºç‰ˆæœ¬ã€‚
- è¯¥æ–‡æ¡£å°†æŒç»­æ›´æ–°ã€‚

## æ–‡æ¡£ç›®å½•

- Ceph é›†ç¾¤çŠ¶æ€
- Ceph Monitor ç›‘æ§å™¨
- Ceph Manager ç®¡ç†å™¨ä¸æ¨¡å—
- Ceph Cluster Map é›†ç¾¤æ˜ å°„
- Ceph CRUSH Map æ˜ å°„
- Ceph PG æ”¾ç½®ç»„
- Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡
- RADOS å¯¹è±¡æ“ä½œ
- Ceph Pool å­˜å‚¨æ± 
- CephX è®¤è¯ä¸ç”¨æˆ·
- Ceph RBD é•œåƒ
- CephFS æ–‡ä»¶ç³»ç»Ÿ
- å‚è€ƒé“¾æ¥

## Ceph é›†ç¾¤çŠ¶æ€

- Red Hat Ceph Storage v5.0 ä¸­å·²ä¸å†ä½¿ç”¨æ—§å¼çš„ systemd ç®¡ç†å®ˆæŠ¤è¿›ç¨‹çš„æ–¹å¼ç®¡ç† Ceph è¿›ç¨‹ï¼Œè€Œä½¿ç”¨ `Podman rootfull` å®¹å™¨çš„æ–¹å¼å°è£…è¿è¡Œ Ceph è¿›ç¨‹ï¼Œå› æ­¤ï¼Œå¯ä½¿ç”¨ `cephadm` å‘½ä»¤è¡Œå·¥å…·å–ä»£ä»¥å¾€çš„ `ceph-ansible` æ¥å®ç°é›†ç¾¤çš„éƒ¨ç½²ä¸ç®¡ç†ã€‚

- ç®¡ç† Ceph é›†ç¾¤çš„ä¸¤ç§æ–¹å¼ï¼š
  
  - 1ï¸âƒ£ åœ¨ç®¡ç†èŠ‚ç‚¹ä¸Šç›´æ¥ä½¿ç”¨ ceph å‘½ä»¤ã€‚
  
  - 2ï¸âƒ£ è¿›å…¥ç®¡ç†èŠ‚ç‚¹çš„ cephadm shell ä¸´æ—¶å®¹å™¨ä¸­ï¼Œä½¿ç”¨äº¤äº’å¼å‘½ä»¤ç®¡ç†ã€‚

- cephadm å‘½ä»¤ä½¿ç”¨ï¼š
  
  cephadm å·¥å…·ä¸€èˆ¬å®‰è£…äºéƒ¨ç½²èŠ‚ç‚¹æˆ–ç®¡ç†èŠ‚ç‚¹ä¸­ï¼Œå› æ­¤è¿è¡Œä»¥ä¸‹å‘½ä»¤éœ€åœ¨å¯¹åº”èŠ‚ç‚¹ä¸­è¿è¡Œã€‚
  
  ```bash
  $ cephadm --help
  $ cephadm version
  $ cephadm inspect-image
  # æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ Ceph å®¹å™¨é•œåƒä»“åº“ä¸ç‰ˆæœ¬
  $ cephadm list-networks
  # æŸ¥çœ‹å½“å‰é›†ç¾¤ä½¿ç”¨çš„ç½‘æ®µä¿¡æ¯
  
  $ cephadm shell -- <command>
  $ cephadm shell -- ceph status  #ç¤ºä¾‹
  # åœ¨é›†ç¾¤ bootstrap èŠ‚ç‚¹ä½¿ç”¨ cephadm shell ä»¥ç¡®è®¤é›†ç¾¤å¥åº·çŠ¶æ€
  # è¯¥å‘½ä»¤è¿è¡Œè¿‡ç¨‹ä¸­å°†å¯åŠ¨ä¸€ä¸ªä¸´æ—¶å®¹å™¨ä»¥è¿è¡ŒæŒ‡å®šçš„å‘½ä»¤
  $ cephadm shell --mount /root/<mount-point>
  # å¯åŠ¨ä¸´æ—¶å®¹å™¨å¹¶å°†æŒ‡å®šç›®å½•æ˜ å°„è‡³å®¹å™¨ /mnt ç›®å½•ä¸Š
  
  $ cephadm bootstrap --config /etc/ceph/ceph.conf
  # é›†ç¾¤å¼•å¯¼è¿‡ç¨‹ä¸­æ›´æ”¹é›†ç¾¤é…ç½®æ–‡ä»¶ä»¥ä¼ é€’ä¿®æ”¹çš„é…ç½®ä½¿å…¶ç”Ÿæ•ˆ
  
  $ ceph versions
  # æŸ¥çœ‹é›†ç¾¤æ‰€æœ‰æœåŠ¡ç»„ä»¶çš„ç‰ˆæœ¬
  $ ceph tell osd.* version
  # æŸ¥çœ‹æ‰€æœ‰ osd çš„ç‰ˆæœ¬
  ```
  
  ![](pictures/cephadm-demo.png)
  
  ![](pictures/cephadm-shell-demo.png)
  
  âœ¨ æ³¨æ„ï¼šä½¿ç”¨ ceph orch host add æ·»åŠ é¢å¤–çš„èŠ‚ç‚¹æ—¶ï¼Œéœ€å…ˆå°†é›†ç¾¤å…¬é’¥å¯¼å‡ºå¹¶åŒæ­¥è‡³èŠ‚ç‚¹ã€‚
  
  ```bash
  $ ceph cephadm get-pub-key > /path/to/ceph.pub
  # ç”Ÿæˆé›†ç¾¤å…¬é’¥
  $ ssh-copy-id -f -i /path/to/ceph.pub root@<hostname>
  # å°†é›†ç¾¤å…¬é’¥åŒæ­¥è‡³èŠ‚ç‚¹ï¼Œå³å¯æ·»åŠ è¯¥èŠ‚ç‚¹ã€‚
  $ ceph orch host add <node>
  # æ·»åŠ èŠ‚ç‚¹ä¸ºé›†ç¾¤èŠ‚ç‚¹
  ```

- Ceph Orchestrator ç¼–æ’å™¨ä½¿ç”¨ï¼š
  
  ```bash
  $ ceph orch status
    Backend: cephadm
    Available: Yes
    Paused: No
  # æŸ¥çœ‹é›†ç¾¤æ•´ä½“çŠ¶æ€
  
  $ ceph orch ls [--service_type=<name>]
    NAME                     RUNNING  REFRESHED  AGE  PLACEMENT                                                                
    alertmanager                 1/1  9m ago     2y   count:1                                                                  
    crash                        4/4  9m ago     2y   *                                                                        
    grafana                      1/1  9m ago     2y   count:1                                                                  
    mgr                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
    mon                          3/3  9m ago     5d   serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com  
    node-exporter                4/4  9m ago     2y   *                                                                        
    osd.default_drive_group     9/12  9m ago     2y   server*                                                                  
    prometheus                   1/1  9m ago     2y   count:1                                                                  
    rgw.realm.zone               2/2  9m ago     23M  serverc.lab.example.com;serverd.lab.example.com
  # æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡ï¼ˆserviceï¼‰çŠ¶æ€
  # ä½¿ç”¨ --service_type é€‰é¡¹æŒ‡å®š Ceph æœåŠ¡ç±»å‹
  $ ceph orch ls --service_type=mon
    NAME  RUNNING  REFRESHED  AGE  PLACEMENT                                                                                        
    mon       4/4  3m ago     2y   clienta.lab.example.com;serverc.lab.example.com;serverd.lab.example.com;servere.lab.example.com
  # æŸ¥çœ‹ mon æœåŠ¡çš„çŠ¶æ€
  
  $ ceph orch ps [--daemon_type=<name>]
    NAME                         HOST                     STATUS         REFRESHED  AGE  PORTS  VERSION           IMAGE ID      CONTAINER ID  
    mon.clienta                  clienta.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  6c85145c2b22  
    mon.serverc.lab.example.com  serverc.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  f93b366a9f74  
    mon.serverd                  serverd.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  d35695b5833e  
    mon.servere                  servere.lab.example.com  running (12h)  3m ago     2y   -      16.2.0-117.el8cp  2142b60d7974  97319d05259c
  # æŸ¥çœ‹ Ceph é›†ç¾¤ä¸­çš„æœåŠ¡å®ä¾‹çŠ¶æ€
  # ä½¿ç”¨ --daemon_type é€‰é¡¹å¯æŒ‡å®šæœåŠ¡å®ä¾‹çš„åç§°è¿›è¡Œè¿‡æ»¤
  
  $ ceph orch host ls
    HOST                     ADDR           LABELS  STATUS  
    clienta.lab.example.com  172.25.250.10  _admin          
    serverc.lab.example.com  172.25.250.12  _admin          
    serverd.lab.example.com  172.25.250.13                  
    servere.lab.example.com  172.25.250.14
  # ä½¿ç”¨ç¼–æ’å™¨æŸ¥çœ‹å„èŠ‚ç‚¹æ¦‚è¦ä¸ label æ ‡ç­¾
  # æ³¨æ„ï¼šlabel æ ‡ç­¾çš„ä½œç”¨åœ¨äºåŒºåˆ†å„ä¸ªèŠ‚ç‚¹ï¼Œå¯¹èŠ‚ç‚¹è¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œå¹¶ä¸”ä¸€ä¸ªèŠ‚ç‚¹å¯å…·æœ‰å¤šä¸ªæ ‡ç­¾ã€‚
  $ ceph orch host label add <node> _admin
  # ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  admin ç®¡ç†å‘˜ label æ ‡ç­¾
  $ ceph orch host label add <node> prometheus
  # ä¸ºé›†ç¾¤èŠ‚ç‚¹æ·»åŠ  prometheus æ ‡ç­¾
  $ ceph orch apply prometheus --placement="label:prometheus"
  # æ ¹æ®é›†ç¾¤èŠ‚ç‚¹æ ‡ç­¾éƒ¨ç½² prometheus
  
  $ ceph orch device ls [--wide]
  # æŸ¥çœ‹é›†ç¾¤å„èŠ‚ç‚¹å¯ç”¨ä¸ä¸å¯ç”¨çš„è®¾å¤‡ä¿¡æ¯ï¼Œ--wide é€‰é¡¹å°†æŒ‡å®šå…¨éƒ¨çš„è¯¦ç»†è®¾å¤‡ä¿¡æ¯ã€‚
  
  $ sudo systemctl list-units --all "ceph*"
  # ç™»å½• Ceph é›†ç¾¤èŠ‚ç‚¹æŸ¥çœ‹æ‰€æœ‰çš„ ceph æœåŠ¡å•å…ƒ
  
  $ ceph orch host maintenance enter <node>
  # å°†æŒ‡å®šèŠ‚ç‚¹è®¾ç½®ä¸ºç»´æŠ¤æ¨¡å¼
  $ ceph orch host maintenance exit <node>
  # å°†æŒ‡å®šèŠ‚ç‚¹é€€å‡ºç»´æŠ¤æ¨¡å¼
  ```
  
  ğŸ‘‰ éƒ¨ç½²æ–°çš„ OSD è®¾å¤‡ï¼š
  
  ```bash
  $ ceph orch apply osd --all-available-devices
  # æ·»åŠ æ‰€æœ‰ ceph orch device ls è¿”å›çš„å¯ç”¨èŠ‚ç‚¹
  
  $ ceph orch device zap --force <hostname> /path/to/device
  # å¼ºåˆ¶åˆ é™¤ä¹‹å‰ osd åˆ›å»ºçš„æ‰€æœ‰åˆ†åŒºå¹¶æ¸…é™¤å…¶ä¸­çš„æ‰€æœ‰æ•°æ®ä¸ºéƒ¨ç½² osd åšå‡†å¤‡
  $ ceph orch daemon add osd <hostname>:/path/to/device
  $ ceph orch daemon add osd serverd.lab.example.com:/dev/vde  #ç¤ºä¾‹
  # æ·»åŠ æŒ‡å®šä¸»æœºä¸Šçš„ osd è®¾å¤‡
  ```
  
  ![](pictures/ceph-orch-daemon-add-osd-demo.png)
  
  ğŸ‘‰ åˆ é™¤ OSD è®¾å¤‡ï¼š
  
  ```bash
  $ ceph device ls | awk /<hostname>/
  # æŸ¥çœ‹æŒ‡å®šèŠ‚ç‚¹ä¸Šç£ç›˜è®¾å¤‡ä¸ osd çš„å¯¹åº”å…³ç³»
  $ ceph orch daemon stop osd.$id
  # åœæ­¢æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹
  # æ³¨æ„ï¼š
  #   1. è‹¥ç™»å½•è‡³ osd æ‰€åœ¨çš„èŠ‚ç‚¹ä½¿ç”¨ systemctl å‘½ä»¤æŸ¥çœ‹ï¼Œå¯å‘ç°æ­¤æœåŠ¡å•å…ƒå·²åœæ­¢ã€‚
  #   2. å¯åœ¨æ­¤èŠ‚ç‚¹ä¸Šä½¿ç”¨ systemctl å‘½ä»¤å¯åŠ¨æ­¤ osd å®ˆæŠ¤è¿›ç¨‹
  $ ceph orch daemon rm osd.$id --force
  # å°†æŒ‡å®šçš„ osd å®ˆæŠ¤è¿›ç¨‹å¼ºåˆ¶ç§»é™¤
  $ ceph orch osd rm status
  # æŸ¥çœ‹ osd åˆ é™¤çš„çŠ¶æ€
  $ ceph osd rm $id
  # å°† osd ä»é›†ç¾¤ osdmapï¼ˆosd æ˜ å°„ï¼‰ä¸­åˆ é™¤
  # æ³¨æ„ï¼š
  #   1. æ­¤æ—¶ï¼Œceph -s é›†ç¾¤çŠ¶æ€å°†æ˜¾ç¤º crushmap ä¸­ä¾ç„¶ä¿ç•™ç€ osdï¼Œè€Œ osdmap ä¸­å·²ç§»é™¤ osdï¼Œä¸¤è€…çš„çŠ¶æ€ä¸åŒ¹é…ã€‚
  #   2. ceph osd tree æ˜¾ç¤ºçš„ osd çŠ¶æ€ä¸º DNEï¼Œå½“ä» crushmap ä¸­ç§»é™¤åå°†æ¸…é™¤ã€‚
  $ ceph osd crush rm $id
  # å°† osd ä»é›†ç¾¤ crushmapï¼ˆCRUSH æ˜ å°„ï¼‰ä¸­åˆ é™¤
  ```
  
  ![](pictures/ceph-status-with-noosd-in-osdmap.png)
  
  âœ¨ ä» RHCS4 å¼€å§‹å¼•å…¥ `ceph-volume` å‘½ä»¤ç”¨äºåˆ›å»ºåŸºäº `BlueStore` å­˜å‚¨å¼•æ“çš„ OSDï¼Œåˆ†åˆ«ä½¿ç”¨ `ceph-volume lvm prepare` ä¸ `ceph-volume lvm activate` å­å‘½ä»¤åˆ›å»ºä¸æ¿€æ´» OSD è®¾å¤‡ï¼Œè¿‡ç¨‹å¦‚ä¸‹ç¤ºæ„ï¼š
  
  ![](pictures/ceph-volume-add-bluestore-osd-backend.jpg)
  
  ![](pictures/ceph-volume-add-bluestore-osd-backend-status.jpg)
  
  ![](pictures/ceph-volume-active-bluestore-osd.jpg)
  
  ```bash
  ### RHCS5 ä¸­ä¾ç„¶å¯åœ¨
  $ ceph-volume lvm list
  # ç™»å½•å¯¹åº”èŠ‚ç‚¹æŸ¥çœ‹æœ¬èŠ‚ç‚¹ osd ä¸ lv çš„å¯¹åº”å…³ç³»
  $ ceph osd metadata <osd_id>
  # é‡è¦ï¼šæ ¹æ® osd id æŸ¥çœ‹ osd çš„å…ƒæ•°æ®ä¿¡æ¯
  ```

- Ceph é›†ç¾¤é…ç½®ï¼š
  
  æ¯ä¸ª `ceph monitor` èŠ‚ç‚¹ç®¡ç†ä¸€ä¸ªé›†ä¸­å¼é…ç½®æ•°æ®åº“ï¼Œä½äº `/var/lib/ceph/$fsid/mon.$host/store.db/` ä¸­ã€‚åœ¨é›†ç¾¤å¯åŠ¨æ—¶ï¼ŒCeph å®ˆæŠ¤è¿›ç¨‹è§£æç”±å‘½ä»¤è¡Œé€‰é¡¹ã€ç¯å¢ƒå˜é‡ä¸æœ¬åœ°é›†ç¾¤é…ç½®çš„é…ç½®é€‰é¡¹ã€‚Ceph å®ˆæŠ¤è¿›ç¨‹è¿æ¥åˆ°é›†ç¾¤ä»¥è·å–å­˜å‚¨åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®è®¾å®šã€‚ä» RHCS 4 å¼€å§‹å¼ƒç”¨ `/etc/ceph/ceph.conf` é›†ç¾¤é…ç½®æ–‡ä»¶ï¼Œè€Œå°†é›†ä¸­å¼é…ç½®æ•°æ®åº“ä½œä¸ºé…ç½®å­˜å‚¨çš„é¦–é€‰æ–¹å¼ã€‚ceph config set å‘½ä»¤å¯ç”¨äºæ›´æ”¹é›†ç¾¤å„ç±»é…ç½®ï¼ŒåŒ…æ‹¬ `public_network` ä¸ `cluster_network`ã€‚
  
  ![](pictures/ceph-arch-network.png)
  
  ![](pictures/osd-network-community.jpg)
  
  â˜ Ceph OSD èŠ‚ç‚¹çš„ç½‘ç»œè¿æ¥ç¤ºæ„ï¼ˆæ¯ä¸ª OSD ä½¿ç”¨ä¸€ä¸ªç«¯å£é€šè¿‡ public ç½‘ç»œä¸å®¢æˆ·ç«¯åŠ Mon é€šä¿¡ï¼Œä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œä¸å…¶ä»– OSD é—´ä¼ è¾“æ•°æ®ï¼Œä¸€ä¸ªç«¯å£é€šè¿‡ cluster ç½‘ç»œäº¤æ¢ heatbeat å¿ƒè·³åŒ…ï¼‰
  
  ![](pictures/default-ports-rhcs5.jpg)
  
  â˜ RHCS5 ä¸­çš„é»˜è®¤ç«¯å£èŒƒå›´
  
  ```bash
  $ ceph -s
  $ ceph status
  $ ceph -w
  # å®æ—¶åˆ·æ–°æ—¥å¿—
  ```
  
  ```bash
  $ ceph config ls
  # åˆ—å‡ºé›†ç¾¤æ‰€æœ‰çš„å¯é…ç½®å‚æ•°ï¼ˆRHCS5 åŒ…å« 2026 ä¸ªé…ç½®å‚æ•°ï¼‰
  $ ceph config help <config_setting>
  # æŸ¥çœ‹æŒ‡å®šé…ç½®å‚æ•°çš„å…·ä½“ä¿¡æ¯
  $ ceph config dump
  # æŸ¥çœ‹é›†ä¸­å¼é…ç½®æ•°æ®åº“çš„é…ç½®ï¼ˆåŒºåˆ«äºé›†ç¾¤é…ç½®æ–‡ä»¶ï¼‰
  $ ceph config show $type.$id
  # æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é…ç½®
  $ ceph config show-with-defaults $type.$id
  # æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œçš„é»˜è®¤é…ç½®
  
  $ ceph config get $type.$id
  # æŸ¥çœ‹ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
  $ ceph config set $type.$id <config_setting>
  # è®¾ç½®ç‰¹å®šå®ˆæŠ¤è¿›ç¨‹åœ¨é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­çš„é…ç½®
  $ ceph config set mon.serverd mon_max_pool_pg_num 65536  #ç¤ºä¾‹
  # è®¾ç½® mon.serverd çš„ mon_max_pool_pg_num ä¸º 65536
  
  $ ceph config assimilate-conf -i /path/to/config_file
  # å°†æŒ‡å®šé…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°èå…¥å½“å‰çš„é›†ä¸­å¼é…ç½®æ•°æ®åº“ä¸­
  
  $ ceph config get mon mon_allow_pool_delete
  # æŸ¥çœ‹é›†ç¾¤æ˜¯å¦å…è®¸åˆ é™¤å­˜å‚¨æ± 
  $ ceph config get mon mon_compact_on_start
    false
  # æŸ¥çœ‹ mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­æ˜¯å¦å¯æ”¶ç¼©æ•°æ®åº“ï¼Œä»¥æé«˜æ•°æ®åº“æ€§èƒ½ã€‚
  $ ceph config set mon mon_compact_on_start true
  # è®¾ç½® mon åœ¨é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®åº“å‹ç¼©
  
  $ ceph config get mon auth_service_required
  $ ceph config get mon auth_cluster_required
  $ ceph config get mon auth_client_required
  # æŸ¥çœ‹é›†ç¾¤çš„å„è®¤è¯æ–¹å¼
  
  ### é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®è®¾ç½® ###
  $ ceph tell $type.$id config get <config_setting>
  $ ceph tell mon.serverc config get mon_max_pool_pg_num  #ç¤ºä¾‹
  # æŸ¥çœ‹ serverc æ§åˆ¶èŠ‚ç‚¹ä¸Šæ”¯æŒçš„æ¯ä¸ªå­˜å‚¨æ± å¯åŒ…å«çš„æœ€å¤§ pg æ•°
  # æ³¨æ„ï¼š
  #   1. ä»¥ä¸Šå‘½ä»¤å¯åœ¨é›†ç¾¤è¿è¡Œæ—¶è¦†ç›–é…ç½®ï¼Œè¯¥æ–¹å¼ä½œä¸ºä¸´æ—¶é…ç½®ï¼Œå½“å®ˆæŠ¤è¿›ç¨‹é‡å¯åå°†å¤±æ•ˆã€‚
  #   2. ceph tell å­å‘½ä»¤ä¾ç„¶ä½¿ç”¨ monã€‚
  ```

## Ceph Monitor ç›‘æ§å™¨

```bash
$ ceph mon stat
  e5: 3 mons at {serverc.lab.example.com=[v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0],
  serverd=[v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0],servere=[v2:172.25.250.14:3300/0,
  v1:172.25.250.14:6789/0]}, election epoch 42, leader 0 serverc.lab.example.com, 
  quorum 0,1,2 serverc.lab.example.com,serverd,servere
# æŸ¥çœ‹ mon çŠ¶æ€ï¼ŒåŒ…æ‹¬ mon çš„ IPv4 åœ°å€ä¸ leader monã€‚

$ ceph quorum_status -f json-pretty
{
  "election_epoch": 42,
  "quorum": [
    0,
    1,
    2
  ],
  "quorum_names": [
    "serverc.lab.example.com",
    "serverd",
    "servere"
  ],
  "quorum_leader_name": "serverc.lab.example.com",
  ...
# æŸ¥çœ‹ mon çš„é€‰ä¸¾çŠ¶æ€
```

## Ceph Manager ç®¡ç†å™¨

```bash
### Ceph é…ç½® Dashboard ç®¡ç† ###
$ ceph mgr module enable dashboard --force
# å¯ç”¨ dashboard æ¨¡å—
$ ceph mgr module ls | \
  perl -MJSON -MYAML -0777 -wnl -e 'print YAML::Dump(decode_json($_))' -
# æŸ¥çœ‹ mgr ç®¡ç†çš„æ¨¡å—å¹¶ä½¿ç”¨ perl å°†è¾“å‡ºçš„ JSON æ ¼å¼è½¬æ¢ä¸º YAML æ ¼å¼
# æ³¨æ„ï¼šä½¿ç”¨ perl æ—¶éœ€æå‰å®‰è£… perl-JSON ä¸ perl-YAML è½¯ä»¶åŒ…ä»¥æä¾›å¯¹åº”æ¨¡å—
$ ceph dashboard create-self-signed-cert
# é…ç½® dashboard ç”Ÿæˆè¯ä¹¦
$ ceph config set mgr mgr/dashboard/server_addr <dashboard_server_address>
# é…ç½® dashboard çš„ä¸»æœº IP åœ°å€
$ ceph config set mgr mgr/dashboard/server_port 8080
# é…ç½® dashboard ç›‘å¬çš„é SSL ç«¯å£
$ ceph config set mgr mgr/dashboard/ssl_server_port 8443
# é…ç½® dashboard ç›‘å¬çš„ SSL ç«¯å£
$ ceph mgr services
# è·å– mgr ç®¡ç†çš„ dashboard URL
$ ceph dashboard ac-user-create \
<dashboard_user> <dashboard_password> administrator
# åˆ›å»ºç™»å½• dashboard çš„ç”¨æˆ·åä¸å¯†ç ï¼Œä»¥ç®¡ç†å‘˜ç”¨æˆ·èº«ä»½ç™»å½•ã€‚

### ä¿®æ”¹ Dashboard ä»ªè¡¨æ¿å¯†ç  ###
$ echo "<password>" > /path/to/dashboard_password.yml
# æ³¨å…¥ä¿®æ”¹çš„æ–°å¯†ç 
$ ceph dashboard ac-user-set-password <dashboard_user> -i /path/to/dashboard_password.yml 
# è®¾ç½®æŒ‡å®šç”¨æˆ·ä¸å¯†ç 
```

Ceph balancer å‡è¡¡å™¨ï¼š

```bash
$ ceph balancer status
{
    "active": true,
    "last_optimize_duration": "0:00:00.000462",
    "last_optimize_started": "Thu Jan 11 05:51:41 2024",
    "mode": "upmap",
    "optimize_result": "Unable to find further optimization, or pool(s) pg_num is decreasing, or distribution is already perfect",
    "plans": []
} 
# æŸ¥çœ‹ balancer å‡è¡¡å™¨çŠ¶æ€
```

## Ceph Cluster Map é›†ç¾¤æ˜ å°„

Ceph Cluster Map åŒ…å« MON Mapã€OSD Mapã€PG Mapã€MDS Map å’Œ CRUSH Mapã€‚

```bash
$ ceph mon dump
  epoch 55
  fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
  last_changed 2023-09-17T23:46:59.748723+0000
  created 2021-10-01T09:30:30.146231+0000
  min_mon_release 16 (pacific)
  election_strategy: 1
  0: [v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0] mon.serverc.lab.example.com
  1: [v2:172.25.250.14:3300/0,v1:172.25.250.14:6789/0] mon.servere
  2: [v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0] mon.serverd
  dumped monmap epoch 55
# æŸ¥çœ‹é›†ç¾¤ monmap
$ ceph osd dump
# æŸ¥çœ‹é›†ç¾¤ osdmap
$ ceph pg dump
# æŸ¥çœ‹é›†ç¾¤ pgmap
$ ceph osd crush dump
# æŸ¥çœ‹é›†ç¾¤ crush mapï¼ˆå¯æŸ¥çœ‹é›†ç¾¤æ•´ä½“çš„ CRUSH å±‚æ¬¡ç»“æ„ä¸æ•…éšœæ¢å¤åŸŸï¼‰
$ ceph fs dump
# æŸ¥çœ‹é›†ç¾¤ cephfs map
```

## Ceph CRUSH Map æ˜ å°„

ğŸš€ å…³äº `CRUSH map` çš„è¯´æ˜å¯å‚è€ƒ [Ceph CRUSH map æ¦‚è¿°ä¸å®ç°](https://github.com/Alberthua-Perl/tech-docs/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph%20CRUSH%20map%20%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0.md#-ceph-crush-map-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%9E%E7%8E%B0)

## Ceph PG æ”¾ç½®ç»„ä¸ OSD

Ceph PGã€CRUSH æ”¾ç½®è§„åˆ™ä¸ OSD ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](pictures/ceph-pg-crush-osd-mapping.png)

```bash
$ ceph mgr stat
{
    "epoch": 19,
    "available": true,
    "active_name": "serverc.lab.example.com.weqbtr",
    "num_standby": 2
}
# ç¡®è®¤ mgr çš„æ•´ä½“çŠ¶æ€

$ ceph mgr module enable pg_autoscaler
$ ceph mgr module ls
{
    "always_on_modules": [
        "balancer",
        "crash",
        "devicehealth",
        "orchestrator",
        "pg_autoscaler",
        "progress",
        "rbd_support",
        "status",
        "telemetry",
        "volumes"
    ],
    ...
}
$ ceph osd pool set <pool-name> pg_autoscale_mode on
# å¯ç”¨ mgr çš„ pg_autoscaler æ¨¡å—å¹¶å¯ç”¨æŒ‡å®šå­˜å‚¨æ± çš„è‡ªåŠ¨æ‰©å±• pg åŠŸèƒ½

$ ceph pg stat
  105 pgs: 105 active+clean; 4.9 KiB data, 181 MiB used, 90 GiB / 90 GiB avail
# æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰ pg çš„çŠ¶æ€
$ ceph pg map <pgid>
# é‡è¦ï¼šæ ¹æ® pd id æŸ¥æ‰¾æŒ‡å®š pg ä¸ osd çš„å¯¹åº”å…³ç³»
$ ceph pg <pgid> query
# é‡è¦ï¼šæ ¹æ® pg id æŸ¥çœ‹å…¶è¯¦ç»†çŠ¶æ€ä¿¡æ¯
```

## Ceph OSD å¯¹è±¡å­˜å‚¨è®¾å¤‡

```bash
$ ceph osd df
# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
$ ceph osd df tree
# osd çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µå¹¶æ˜¾ç¤º osd åœ¨ CRUSH map ä¸­çš„ä½ç½®
$ ceph osd tree
# osd ç£ç›˜åœ¨é›†ç¾¤ä¸­çš„åˆ†å¸ƒæƒ…å†µ

$ ceph osd stat
  9 osds: 9 up (since 13h), 9 in (since 2y); epoch: e199
# æŸ¥çœ‹æ‰€æœ‰ osd çš„çŠ¶æ€
$ ceph osd find <osd_id>
# æ ¹æ® osd id æŸ¥æ‰¾ osd çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹
$ ceph osd find 3
# æŸ¥æ‰¾ osd.3 çš„çŠ¶æ€åŠæ‰€åœ¨çš„èŠ‚ç‚¹
$ ceph osd map <pool_name> <object_name>
# é‡è¦ï¼š
#   1. æ ¹æ®æŒ‡å®šçš„å¯¹è±¡åç§°æŸ¥æ‰¾å…¶ä¸ pg åŠ osd çš„æ˜ å°„å…³ç³»
#   2. æ­¤å‘½ä»¤ç›¸è¾ƒäº ceph pg map <pgid> æ›´è¿›ä¸€æ­¥ï¼Œç›´æ¥æŸ¥æ‰¾å¯¹è±¡çš„æ˜ å°„å…³ç³»ã€‚

$ ceph osd [set|unset] noscrub
# è®¾ç½® osd æ ‡è®°ä¸ºä¸æ‰§è¡Œæ¸…ç†
$ ceph osd [set|unset] nodeep-scrub
# è®¾ç½® osd æ ‡è®°ä¸ºä¸æ‰§è¡Œæ·±åº¦æ¸…ç†
```

## RADOS å¯¹è±¡æ“ä½œ

```bash
$ rados -p <pool_name> put <object_name> /path/to/file
# æŒ‡å®šæ–‡ä»¶å°†å…¶ä¸Šä¼ è‡³é›†ç¾¤ä¸­ä»¥æŒ‡å®šçš„åç§°å‘½å
```

## Ceph Pool å­˜å‚¨æ± 

```bash
### Ceph å­˜å‚¨æ± çŠ¶æ€ ###
$ ceph osd lspools
$ ceph osd ls pool detail
# ä»¥ä¸Šä¸¤ä¸ªå‘½ä»¤ç”¨äºæŸ¥çœ‹é›†ç¾¤ä¸­çš„å­˜å‚¨æ± 

$ rados df
# RADOS å±‚é¢æŸ¥è¯¢é›†ç¾¤çŠ¶æ€
$ ceph df
# Ceph é›†ç¾¤ä¸å­˜å‚¨æ± çº§åˆ«çš„å­˜å‚¨ä½¿ç”¨æƒ…å†µ
$ ceph osd pool stats
# ç¡®è®¤æ‰€æœ‰å­˜å‚¨æ± çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯

### Ceph å¤åˆ¶æ±  ###
$ ceph osd pool create <pool-name> \
  <pg-num> <pgp-num> [replicated] <crush-rule-set>
# åˆ›å»ºå¤åˆ¶æ± 
# æ³¨æ„ï¼š
#   1. é»˜è®¤æƒ…å†µä¸‹å¯åªæŒ‡å®šå­˜å‚¨æ± åç§°ä¸ PG æ•°é‡ï¼Œä¹Ÿå¯è‡ªå®šä¹‰ CRUSH æ”¾ç½®è§„åˆ™åœ¨åˆ›å»ºå­˜å‚¨æ± æ—¶æŒ‡å®šã€‚
#   2. è‹¥è®¾ç½®äº†ç›¸åŒçš„ pg-num ä¸ pgp-numï¼Œä¹‹åå†è°ƒæ•´ pg-num å°†è‡ªåŠ¨è°ƒæ•´ pgp-numã€‚

$ ceph osd pool application enable <pool-name> [rbd|rgw|cephfs]
# è®¾ç½®æŒ‡å®šçš„å­˜å‚¨æ± ä¸º rbdã€rgw æˆ– cephfs åº”ç”¨ç±»å‹

$ ceph osd pool get <pool-name> all
# æŸ¥çœ‹å­˜å‚¨æ± çš„æ‰€æœ‰å‚æ•°å€¼
$ ceph osd pool get <pool-name> [size|nodelete|min_size]
# æŸ¥çœ‹å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼
$ ceph osd pool get <pool-name> pg_autoscale_mode
# æŸ¥çœ‹å­˜å‚¨æ± çš„ PG è‡ªåŠ¨æ‰©å±•æ¨¡å¼
$ ceph osd pool set <pool-name> [size|nodelete|min_size] <value>
# è®¾ç½®å­˜å‚¨æ± çš„æŒ‡å®šå‚æ•°å€¼

$ ceph osd pool get mon osd_pool_default_size
# æŸ¥çœ‹é›†ä¸­é…ç½®æ•°æ®åº“ä¸­å®šä¹‰çš„é»˜è®¤å¯¹è±¡å‰¯æœ¬æ•°é‡

$ ceph osd pool rename <old-pool-name> <new-pool-name>
# é‡å‘½åå­˜å‚¨æ± åç§°ï¼ˆä¸å½±å“æ± ä¸­å­˜å‚¨çš„æ•°æ®ï¼Œä½†éœ€è¦æ³¨æ„ç”¨æˆ·å¯¹æ± çš„æƒé™ï¼‰

### Ceph çº åˆ ä»£ç æ±  ###
$ ceph osd erasure-code-profile ls
# æŸ¥çœ‹çº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile get <profile-name>
# æŸ¥çœ‹æŒ‡å®šçº åˆ ä»£ç æ± çš„ profile å…·ä½“ä¿¡æ¯
$ ceph osd erasure-code-profile set <profile-name> \
  k=<num> m=<num> crush-failure-domain=[osd|host|rack]
# åˆ›å»ºçº åˆ ä»£ç æ± çš„ profile
$ ceph osd erasure-code-profile rm <profile-name>
# åˆ é™¤æŒ‡å®šçš„çº åˆ ä»£ç æ± çš„ profile
$ ceph osd pool create <pool-name> \
  <pg-num> <pgp-num> erasure <profile-name> <crush-rule-set>
# åˆ›å»ºçº åˆ ä»£ç æ± 
```

å…³äºçº åˆ ä»£ç  profile çš„è®¾ç½® RedHat ç»™å‡ºä»¥ä¸‹æ¨èæ–¹å¼ï¼š

![](pictures/redhat-reasure-code-profile-recommanded.jpg)

## CephX è®¤è¯ä¸ç”¨æˆ·ç›¸å…³

CephX è®¤è¯æœºåˆ¶ï¼š

![](pictures/cephx-request-1.jpg)

Ceph å®¢æˆ·ç«¯ä½¿ç”¨ CephX åè®®å‘ monitor å‘é€ç”¨æˆ·åˆ›å»ºè¯·æ±‚ï¼Œå½“ monitor åˆ›å»ºç”¨æˆ·åå°†å­˜å‚¨ç”¨æˆ·åã€keyring ä¸ capability ç­‰ä¿¡æ¯ï¼Œå¹¶å°†ç›¸åŒçš„ keyring æ–‡ä»¶è¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå…¶ä¸­ keyring æ–‡ä»¶ä¸­çš„ key ä¸º `secret key`ï¼Œç”¨äºåŠ å¯†ä¸è§£å¯† monitor ç”Ÿæˆçš„ä¼šè¯å¯†é’¥ï¼ˆ`session key`ï¼‰ã€‚

![](pictures/cephx-request-2.jpg)

![](pictures/cephx-request-4.png)

æ¯ä¸ª monitor éƒ½å¯ä»¥å¯¹å®¢æˆ·ç«¯è¿›è¡Œèº«ä»½éªŒè¯å¹¶åˆ†å‘å¯†é’¥ï¼Œè¿™æ„å‘³ç€è®¤è¯ä¾é  monitor èŠ‚ç‚¹å®Œæˆï¼Œä¸ä¼šå­˜åœ¨å•ç‚¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚monitor ä¼šè¿”å›ç”¨äºèº«ä»½éªŒè¯çš„æ•°æ®ç»“æ„ï¼Œå…¶ä¸­åŒ…å«è·å– Ceph æœåŠ¡æ—¶ç”¨åˆ°çš„ session keyã€‚æ‰€è°“ session key å°±æ˜¯å®¢æˆ·ç«¯ç”¨æ¥å‘ monitor è¯·æ±‚æ‰€éœ€æœåŠ¡çš„å‡­è¯ï¼Œ`session key` æ˜¯é€šè¿‡å®¢æˆ·ç«¯çš„ `secret key` è¿›è¡ŒåŠ å¯†ä¼ è¾“ã€‚

ğŸš€ å½“ monitor æ”¶åˆ°å®¢æˆ·ç«¯è®¤è¯è¯·æ±‚åï¼Œé¦–å…ˆç”Ÿæˆ session keyï¼Œç„¶åç”¨å®¢æˆ·ç«¯çš„ secret key åŠ å¯†session keyï¼Œå†å‘é€ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯ç”¨è‡ªèº«çš„ secret key å°†å…¶è§£å¯†ï¼Œæ‹¿åˆ° session keyã€‚å®¢æˆ·ç«¯è·å– session key ä¹‹åï¼Œå®ƒå°±å¯ä»¥ç”¨æ­¤ session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œmonitor æ”¶åˆ°å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼ˆæºå¸¦ session keyï¼‰ï¼Œæ­¤æ—¶ monitor ä¼šå‘å®¢æˆ·ç«¯æä¾›ä¸€ä¸ª `ticket`ï¼ˆç¥¨æ®ï¼‰ï¼Œç„¶åä½¿ç”¨ session key åŠ å¯†åå‘é€ç»™å®¢æˆ·ç«¯ã€‚éšåå®¢æˆ·ç«¯ä½¿ç”¨ session key è§£å¯†è¯¥ç¥¨æ®ï¼Œä½¿ç”¨æ­¤ç¥¨æ®åˆ°å¯¹åº” OSD å®Œæˆè®¤è¯ã€‚

ä»¥ä¸Šè¿‡ç¨‹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¦–å…ˆï¼Œå®¢æˆ·ç«¯çš„ secret key æ˜¯é€šè¿‡ monitor èŠ‚ç‚¹åœ¨åˆ›å»ºç”¨æˆ·å¸å·æ—¶å°±ç”Ÿæˆï¼Œæ‰€ä»¥ monitor èŠ‚ç‚¹æœ‰å¯¹åº”å®¢æˆ·ç«¯çš„ secret keyï¼Œé€šè¿‡å®¢æˆ·ç«¯çš„ secret key åŠ å¯†ï¼Œå®¢æˆ·ç«¯å¯ä»¥ç”¨è‡ªèº«çš„ secret key è§£å¯†ã€‚å…¶æ¬¡ï¼Œmonitor èŠ‚ç‚¹ç”Ÿæˆçš„ session key æ˜¯æœ‰è®°å½•çš„ï¼Œæ‰€ä»¥å¯¹äºä¸åŒå®¢æˆ·ç«¯æ¥è¯´ï¼Œéƒ½æœ‰ä¸åŒçš„è®°å½•ï¼Œå¹¶ä¸”è¯¥ session key æ˜¯æœ‰æ—¶é—´é™åˆ¶çš„ï¼Œè¿‡æœŸå³ä¾¿æ˜¯å¯¹åº”å®¢æˆ·ç«¯ï¼Œä¹Ÿæ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚æ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨å¯¹åº” session key å‘ monitor è¯·æ±‚æœåŠ¡ï¼Œå¯¹åº” monitor éƒ½æ˜¯è®¤å¯çš„ï¼Œmonitor ä¼šå‘å…¶å‘æ”¾ ticketã€‚æœ€åï¼Œmonitor å’Œ OSD éƒ½å…±äº«å®¢æˆ·ç«¯çš„ secret key å’Œ session keyï¼Œä»¥åŠ monitor å‘æ”¾çš„ ticketï¼Œæ‰€ä»¥å®¢æˆ·ç«¯ä½¿ç”¨ monitor å‘æ”¾çš„ ticketï¼Œå¯¹åº” OSD æ˜¯è®¤å¯çš„ã€‚è¿™ä¹Ÿæ„å‘³ç€ä¸ç®¡æ˜¯å“ªä¸ª monitor èŠ‚ç‚¹å‘æ”¾çš„ ticketï¼Œå¯¹åº”æ‰€æœ‰ monitor èŠ‚ç‚¹å’Œ OSD éƒ½å¯è®¤è¯ã€‚

![](pictures/cephx-request-3.png)

ğŸ· å…³äº CephX è®¤è¯æœºåˆ¶æ›´å¤šå¯å‚è€ƒ [HIGH AVAILABILITY AUTHENTICATION](https://docs.ceph.com/en/latest/architecture/#high-availabilityauthentication) ä¸­çš„è¯´æ˜ã€‚

```bash
$ ceph auth get-or-create [--id <name>|--name client.<name>] client.<name> \
  mon 'allow r' osd 'allow rw' \
  -o /path/to/ceph.client.<name>.keyring
# ä½¿ç”¨æŒ‡å®šçš„ Ceph ç”¨æˆ·åˆ›å»ºæ–°ç”¨æˆ·å¹¶å°†å…¶ secret key æ³¨å…¥ keyring æ–‡ä»¶
# ä¹Ÿå¯ä½¿ç”¨ --keyring é€‰é¡¹æŒ‡å®š keyring æ–‡ä»¶ç”¨äºåˆ›å»ºæ–°ç”¨æˆ·

### é™åˆ¶ç”¨æˆ·çš„è®¿é—® ###
$ ceph auth get-or-create client.<name> \
  mon 'profile rbd' osd 'profile rbd pool=<pool-name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå­˜å‚¨æ± ä¸­çš„ RBD é•œåƒ
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw object_prefix <prefix-name>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®šå‰ç¼€çš„å¯¹è±¡
$ ceph auth get-or-create client.<name> \
  mon 'allow r' osd 'allow rw namespace=<namespace>'
# é™åˆ¶ç”¨æˆ·åªèƒ½è®¿é—®æŒ‡å®š namespace ä¸­çš„å¯¹è±¡
$ ceph fs authorize cephfs client.<name> /path/to/cephfs rw
# é™åˆ¶ç”¨æˆ·åªèƒ½è¯»å†™ CephFS ä¸­çš„æŒ‡å®šè·¯å¾„
$ ceph auth get-or-create client.<name> \
  mon 'allow r, allow command "auth get-or-create", allow command "auth list"'
# é€šè¿‡ monitor å‘½ä»¤é™åˆ¶ç”¨æˆ·åªèƒ½è¿è¡ŒæŒ‡å®šçš„ ceph å‘½ä»¤

$ ceph auth list
# åˆ—å‡ºæ‰€æœ‰çš„ Ceph ç”¨æˆ·
$ ceph auth get client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
$ ceph auth print-key client.<name>
# æ˜¾ç¤ºæŒ‡å®šç”¨æˆ·çš„ secret key
$ ceph auth export client.<name> > /path/to/keyring
# å¯¼å‡ºæŒ‡å®šç”¨æˆ·çš„ keyring è‡³æ–‡ä»¶
$ ceph auth import -i /path/to/keyring
# æŒ‡å®šç”¨æˆ·çš„ keyring å¯¼å…¥é›†ç¾¤

$ ceph auth caps client.<name> mon 'allow r' osd 'allow rw'
# è¦†ç›–æ›´æ–°å½“å‰æŒ‡å®šç”¨æˆ·çš„ç°æœ‰åŠŸèƒ½
$ ceph auth caps client.<name> osd ''
# ä½¿ç”¨ç©ºå­—ç¬¦åˆ é™¤æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰åŠŸèƒ½

$ ceph auth del client.<name>
# åˆ é™¤é›†ä¸­é…ç½®æ•°æ®åº“ä¸­çš„æŒ‡å®šç”¨æˆ·
$ rm -f /path/to/keyring
# åˆ é™¤æŒ‡å®šç”¨æˆ·çš„ keyring æ–‡ä»¶ï¼ˆè‹¥å­˜åœ¨çš„è¯ï¼‰
```

## Ceph RBD é•œåƒ

- RBD é•œåƒçš„ç‰¹æ€§ï¼ˆfeatureï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š
  
  - `layering`ï¼šæ˜¯å¦æ”¯æŒé•œåƒåˆ†å±‚ã€å…‹éš†ï¼ˆBIT ç ä¸º 1ï¼‰
  
  - `exclusive-lock`ï¼šæ˜¯å¦æ”¯æŒåˆ†å¸ƒå¼ç‹¬å é”æœºåˆ¶ä»¥é™åˆ¶åŒæ—¶ä»…èƒ½ä¸€ä¸ªå®¢æˆ·ç«¯è®¿é—®å½“å‰é•œåƒï¼ˆBIT ç ä¸º 4ï¼‰
  
  - `object-map`ï¼šæ˜¯å¦æ”¯æŒå¯¹è±¡æ˜ å°„ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿå¯¼å…¥ã€å¯¼å‡ºåŠå·²ç”¨å®¹é‡ç»Ÿè®¡ç­‰æ“ä½œï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 8ï¼‰ã€‚
  
  - `fast-diff`ï¼šæ˜¯å¦æ”¯æŒå¿«ç…§é—´çš„å¿«é€Ÿæ¯”è¾ƒæ“ä½œï¼Œä¾èµ–äº object-map ç‰¹æ€§ï¼ˆBIT ç ä¸º 16ï¼‰ã€‚
  
  - `deep-flatten`ï¼šæ˜¯å¦æ”¯æŒå…‹éš†åˆ†ç¦»æ—¶è§£é™¤åœ¨å…‹éš†é•œåƒæ—¶åˆ›å»ºçš„å¿«ç…§ä¸å…¶çˆ¶é•œåƒä¹‹é—´çš„å…³è”ï¼Œå³å¿«ç…§æ‰å¹³åŒ–æ“ä½œï¼ˆBIT ç ä¸º 32ï¼‰ã€‚
  
  - `journaling`ï¼šæ˜¯å¦æ”¯æŒæ—¥å¿— I/O æ“ä½œï¼Œå³æ˜¯å¦æ”¯æŒè®°å½•é•œåƒçš„ä¿®æ”¹æ“ä½œè‡³æ—¥å¿—å¯¹è±¡ï¼Œä¾èµ–äº exclusive-lock ç‰¹æ€§ï¼ˆBIT ç ä¸º 64ï¼‰ã€‚
  
  - `data-pool`ï¼šæ˜¯å¦æ”¯æŒå°†é•œåƒçš„æ•°æ®å¯¹è±¡å­˜å‚¨äºçº åˆ ç å­˜å‚¨æ± ï¼Œä¸»è¦ç”¨äºå°†é•œåƒçš„å…ƒæ•°æ®ä¸æ•°æ®æ”¾ç½®äºä¸åŒçš„å­˜å‚¨æ± ã€‚
  
  - `striping`: æ˜¯å¦æ”¯æŒæ•°æ®å¯¹è±¡é—´çš„æ•°æ®æ¡å¸¦åŒ–ï¼ˆBIT ç ä¸º 2ï¼‰

- å¸¸ç”¨ RBD é•œåƒç›¸å…³å‘½ä»¤ï¼š
  
  ```bash
  $ rbd pool init <pool-name>
  # åˆå§‹åŒ–æŒ‡å®šå­˜å‚¨æ± å­˜å‚¨ RBD é•œåƒ
  $ rbd create [--id <name>] \
    --size <num>[M|G] [--object-size <num>[M|G]] \
    --image-feature=<feature1>,<feature2>,... \
    #--image-feature=exclusive-lock,journaling \  #ç¤ºä¾‹
    <pool-name>/<rbd-image-name>
  # æŒ‡å®š RBD é•œåƒå¤§å°ï¼ˆé»˜è®¤å•ä½ MiBï¼‰ã€chunk çš„å¯¹è±¡å¤§å°ä»¥åŠæ”¯æŒçš„é•œåƒç‰¹æ€§ä»¥åˆ›å»º RBD é•œåƒ
  # æ³¨æ„ï¼š
  #   1. å…¶ä¸­ layering, exclusive-lock, object-map, fast-diff, deep-flatten ä¸º
  #      é»˜è®¤æ”¯æŒçš„é•œåƒç‰¹æ€§
  #   2. è‹¥åªéœ€å¯ç”¨ç‰¹å®šçš„é•œåƒç‰¹æ€§å¯ä½¿ç”¨ --image-feature é€‰é¡¹æŒ‡å®šå³å¯
  #   3. ä»¥ä¸Šç¤ºä¾‹ä¸­çš„ exclusive-lock ä¸ journaling ç‰¹æ€§åœ¨ RBD Mirror ä¸­å¯ç”¨
  
  $ rbd rm [--id <name>] <pool-name>/<rbd-image-name>
  $ rbd ls [--id <name>] --pool <pool-name>
  $ rbd info [--id <name>] <pool-name>/<rbd-image-name>
  $ rbd resize --size <number>[M|G] <pool-name>/<rbd-image-name>
  
  $ rbd map [--id <name>] <pool-name>/<rbd-image-name>
  $ rbd unmap /dev/rbdX
  $ rbd showmapped
  # æ³¨æ„ï¼š
  #   1. RBD é•œåƒçš„æ˜ å°„äºå®¢æˆ·ç«¯ä¸Šæ‰§è¡Œ
  #   2. å®¢æˆ·ç«¯éœ€å®‰è£… ceph-common è½¯ä»¶åŒ…å¹¶åŠ è½½ rbd å†…æ ¸æ¨¡å—
  
  $ fsfreeze --freeze /path/to/<mount-point>
  $ fsfreeze --unfreeze /path/to/<mount-point>
  $ rbd snap create [--id <name>] <pool-name>/<rbd-image-name>@<snap-name>
  $ rbd snap remove [--id <name>] <pool-name>/<rbd-image-name>@<snap-name>
  $ rbd snap list [--id <name>] <pool-name>/<rbd-image-name>
  $ rbd snap protect [--id <name>] <pool-name>/<rbd-image-name>@<snap-name>
  $ rbd clone [--id <name>] \
    <pool-name>/<rbd-image-name>@<snap-name> <pool-name>/<clone-name>
  # åˆ›å»ºåŸºäº RBD é•œåƒå¿«ç…§çš„å…‹éš†
  $ rbd flatten <rbd-image-clone-name>
  #åˆ›å»ºæ‰å¹³åŒ–å…‹éš†
  
  $ blockdev --getro /path/to/device
  ```

- Ceph RBD Mirror ç›¸å…³å‘½ä»¤ï¼š
  
  RBD Mirror çš„ä¸¤ç§æ¨¡å¼ï¼ŒåŒ…æ‹¬ `RBD one-way mirroring` æ¨¡å¼ï¼ˆ`active-backup mode`ï¼‰ã€`RBD two-way mirroring` æ¨¡å¼ï¼ˆ`active-active mode`ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ![](pictures/rbd-one-way-mirroring.jpg)
  
  ![](pictures/rbd-two-way-mirroring.jpg)
  
  ğŸ§ª ç¤ºä¾‹ï¼šå®ç° RBD one-way mirroring çš„æ ¸å¿ƒå‘½ä»¤
  
  ```bash
  ### RBD Mirror Primary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰###
  $ rbd mirror pool enable <pool-name> <mirror-mode>
  # æŒ‡å®šå­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„æŒ‡å®šæ¨¡å¼ï¼ˆpool æ¨¡å¼ã€image æ¨¡å¼ï¼‰
  $ rbd mirror pool enable rbd pool  #ç¤ºä¾‹
  # å¯ç”¨ rbd å­˜å‚¨æ± å¯ç”¨ RBD Mirror çš„ pool æ¨¡å¼ï¼ˆæ± æ¨¡å¼ï¼‰
  $ rbd info rbd/image1  #ç¤ºä¾‹
    rbd image 'image1':
          ...
          features: exclusive-lock, journaling
          ...
          mirroring state: enabled
          mirroring mode: journal
          mirroring global id: cccdeb7c-1ce5-4ca4-9498-1400f228bf72
          mirroring primary: true
  # è¯¥ç¤ºä¾‹æ˜¾ç¤º rbd å­˜å‚¨æ± å·²å¯ç”¨ pool æ¨¡å¼ï¼Œå¹¶ä¸”å…¶ä¸­çš„ image1 é•œåƒå¯è¿›è¡ŒåŒæ­¥ã€‚
  
  $ rbd mirror pool peer bootstrap create \
    --site-name <site-name> <pool-name> > /path/to/mirror-bootstrap-token
  # åˆ›å»ºå¼•å¯¼å¯¹ç­‰å­˜å‚¨é›†ç¾¤çš„ bootstrap tokenï¼ˆactive-passive é›†ç¾¤æ¨¡å¼ï¼‰
  
  ### RBD Mirror Secondary é›†ç¾¤ï¼ˆpool æ¨¡å¼ï¼‰###
  $ ceph orch apply rbd-mirror --placement=<fqdn>
  # æŒ‡å®šèŠ‚ç‚¹å¯ç”¨ rbd-mirror å®ˆæŠ¤è¿›ç¨‹
  $ ceph orch apply rbd-mirror --placement=serverf.lab.example.com  #ç¤ºä¾‹
  $ ceph orch ls  #ç¤ºä¾‹
    ...
    rbd-mirror                   1/1  8m ago     2h   serverf.lab.example.com
    ...
  $ rbd mirror pool peer bootstrap import \
    --site-name <site-name> \
    --direction rx-only <pool-name> \
    /path/to/mirror-bootstrap-token
  # å¯¼å…¥ primary é›†ç¾¤æä¾›çš„å¯¹ç­‰é›†ç¾¤å¼•å¯¼ tokenï¼Œå°†è®¾ç½®ä¸º secondary é›†ç¾¤ã€‚
  # è¯¥é›†ç¾¤åªèƒ½ä»¥æ¥æ”¶æ–¹å¼ï¼ˆrx-onlyï¼‰å¤‡ä»½åŒæ­¥æŒ‡å®šå­˜å‚¨æ± ä¸­çš„é•œåƒ
  
  $ rbd mirror pool info <pool-name>
  # æŸ¥çœ‹ RBD Mirror æŒ‡å®šå­˜å‚¨æ± çš„å¯¹ç­‰ä¿¡æ¯
  $ rbd mirror pool info rbd  #ç¤ºä¾‹
    Mode: pool
    Site Name: bup
  
    Peer Sites: 
  
    UUID: 763de4dc-ba66-4672-aaec-582b68cf9cf1
    Name: prod
    Direction: rx-only
    Client: client.rbd-mirror-peer
  $ rbd mirror pool status
    health: OK
    daemon health: OK
    image health: OK
    images: 1 total
        1 replaying
  # æŸ¥çœ‹ RBD Mirror çš„å¯¹ç­‰çŠ¶æ€
  # æ³¨æ„ï¼šprimary é›†ç¾¤ä¸­æ— æ³•æŸ¥è¯¢ RBD é•œåƒåŒæ­¥çŠ¶æ€
  
  ### æŠ¥é”™ç¤ºä¾‹ ###
  # RBD Mirror Secondary é›†ç¾¤
  $ rbd rm rbd/image1
  2023-10-07T05:14:13.113+0000 7f6c226f3700 -1 librbd::image::PreRemoveRequest: 0x564199439410 handle_exclusive_lock:
  cannot obtain exclusive lock - not removing
  Removing image: 0% complete...failed.
  rbd: error: image still has watchers
  This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 
  30s for the crashed client to timeout.
  # ç”±äº primary é›†ç¾¤å­˜å‚¨æ± ä¸­é•œåƒè®¾ç½®äº†åˆ†å¸ƒå¼é”ï¼ˆexclusive-lockï¼‰ç‰¹æ€§ï¼Œ
  # å› æ­¤ï¼Œsecondary é›†ç¾¤ä¸­æ— æ³•åˆ é™¤é•œåƒï¼Œåªèƒ½ä» primary é›†ç¾¤ä¸­åˆ é™¤ã€‚
  ```
  
  ğŸ§ª ç¤ºä¾‹ï¼š`rbd-nbd` æ˜ å°„ä½¿ç”¨å·² mirroring çš„ RBD é•œåƒ
  
  ç¬”è€…ç¯å¢ƒä¸­å·²éƒ¨ç½² RBD Mirror Primary ä¸ Secondary é›†ç¾¤ï¼Œä¸¤å¥—é›†ç¾¤ä¸­å‡å·²åˆ›å»º rbd å­˜å‚¨æ± å¹¶å¯ç”¨ pool æ¨¡å¼çš„ RBD Mirrorã€‚primary é›†ç¾¤çš„ rbd å­˜å‚¨æ± ä¸­å·²åˆ›å»º image1 é•œåƒï¼Œè¯¥é•œåƒå·²å¯ç”¨ `exclusive-lock` ä¸ `journaling` ç‰¹æ€§ã€‚ç°å°†è¯¥é•œåƒæ˜ å°„ç»™å®¢æˆ·ç«¯è™šæ‹Ÿæœºä½œè™šæ‹Ÿç£ç›˜ä½¿ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ```bash
  ### å®¢æˆ·ç«¯è™šæ‹Ÿæœº ###
  $ sudo dnf install -y ceph-common
  # å®‰è£… rbd å‘½ä»¤
  $ sudo rbd map rbd/image1
    rbd: sysfs write failed
    RBD image feature set mismatch. You can disable features unsupported by 
    the kernel with "rbd feature disable image1 journaling"
    In some cases useful info is found in syslog - try "dmesg | tail".
    rbd: map failed: (6) No such device or address
  # rbd æ˜ å°„ rbd/image1 é•œåƒå†…æ ¸æ¨¡å—æŠ¥é”™
  # æ³¨æ„ï¼š
  #   1. ä½¿ç”¨è¯¥å‘½ä»¤å‰éœ€åŒæ­¥ primary é›†ç¾¤çš„ /etc/ceph/{ceph.conf,
  #      ceph.client.admin.keyring} æ–‡ä»¶è‡³æœ¬åœ° /etc/ceph/ ç›®å½•ä¸­
  #   2. rbd å†…æ ¸æ¨¡å—æŠ¥é”™æ— æ³•æ˜ å°„æ˜¯ç”±äºå½“å‰å†…æ ¸ä¸æ”¯æŒ journaling ç‰¹æ€§ã€‚
  #      ä½†æ˜¯ï¼Œmirroring å¿…é¡»ä¾èµ–è¯¥ç‰¹æ€§ï¼Œå› æ­¤ä½¿ç”¨ rbd æ˜ å°„ä¸ journaling ä¹‹é—´å­˜åœ¨çŸ›ç›¾ã€‚
  #      ç›®å‰ç°æœ‰çš„å†…æ ¸ç‰ˆæœ¬å‡ä¸æ”¯æŒè¯¥ç‰¹æ€§ï¼
  #   3. å¯ä½¿ç”¨ rbd-nbd æ›¿ä»£ rbd ä»¥å®Œæˆé•œåƒçš„æ˜ å°„ã€‚å¯å‚çœ‹æ–‡æœ«å‚è€ƒé“¾æ¥ã€‚
  $ sudo uname -r
    4.18.0-305.el8.x86_64
  
  $ sudo dnf install -y rbd-nbd
  # å®‰è£… rbd-nbd è½¯ä»¶åŒ…
  $ sudo dnf info rbd-nbd
    ...
    Name         : rbd-nbd
    Epoch        : 2
    Version      : 16.2.0
    Release      : 117.el8cp
    Architecture : x86_64
    Size         : 511 k
    Source       : ceph-16.2.0-117.el8cp.src.rpm
    Repository   : @System
    From repo    : rhceph-5-tools-for-rhel-8-x86_64-rpms
    Summary      : Ceph RBD client base on NBD
    URL          : http://ceph.com/
    License      : LGPL-2.1 and LGPL-3.0 and CC-BY-SA-3.0 and GPL-2.0 and 
                   BSL-1.0 and BSD-3-Clause and MIT
    Description  : NBD based client to map Ceph rbd images to local device
  $ sudo lsmod | egrep 'rbd|nbd'
    nbd                    49152  2
    rbd                   110592  0
    libceph               454656  1 rbd
  # ç¡®è®¤ rbd ä¸ nbd å†…æ ¸æ¨¡å—æ˜¯å¦åŠ è½½
  $ sudo rbd-nbd map rbd/image1
    /dev/nbd1
  # æ˜ å°„ rbd/image1 é•œåƒä¸º /dev/nbd1
  $ sudo mkfs -t ext4 /dev/nbd1
  $ sudo mkdir /mnt/rbd
  $ sudo mount -t ext4 /dev/nbd1 /mnt/rbd
  $ sudo dd if=/dev/zero of=/mnt/rbd/test-data1 oflag=direct bs=4M count=10
    10+0 records in
    10+0 records out
    41943040 bytes (42 MB, 40 MiB) copied, 0.747552 s, 56.1 MB/s
  $ sudo ls -lh /mnt/rbd/test-data1 
    -rw-r--r--. 1 root root 40M Oct  7 23:05 /mnt/rbd/test-data1
  # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
  
  ### RBD Mirror Primary/Secondary é›†ç¾¤ ###
  $ rbd info rbd/image1
    rbd image 'image1':
          size 1 GiB in 256 objects
          order 22 (4 MiB objects)
          snapshot_count: 0
          id: 8569a03bc09a
          block_name_prefix: rbd_data.8569a03bc09a
          format: 2
          features: exclusive-lock, journaling
          op_features: 
          flags: 
          create_timestamp: Sat Oct  7 17:41:54 2023
          access_timestamp: Sun Oct  8 03:05:02 2023
          modify_timestamp: Sun Oct  8 03:05:02 2023  # é•œåƒä¸­å·²å†™å…¥æ•°æ®
          journal: 8569a03bc09a
          mirroring state: enabled
          mirroring mode: journal
          mirroring global id: c28aca80-f176-49d0-9a7c-0f34f2380f51
          mirroring primary: true
  # ä»ä»¥ä¸Š primary ä¸ secondary é›†ç¾¤çš„é•œåƒä¿¡æ¯å¯è§æ•°æ®å·²å†™å…¥å¹¶åŒæ­¥
  ```
  
  ğŸ· å…¶ä»– RBD Mirror ç›¸å…³å‘½ä»¤ï¼š
  
  ```bash
  $ rbd mirror pool disable <pool-name>
  # æŒ‡å®šå­˜å‚¨æ± ç¦ç”¨ pool æ¨¡å¼çš„ RBD Mirror
  
  $ rbd feature enable <pool-name>/<rbd-image-name> <feature-name>
  # å¯ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
  $ rbd feature disable <pool-name>/<rbd-image-name> <feature-name>
  # ç¦ç”¨ RBD é•œåƒçš„æŒ‡å®šç‰¹æ€§
  ```
  
  ![](pictures/rbd-mirror-other-cmds.png)
  
  âœ¨ RBD Mirror çš„æ•…éšœè½¬ç§»ï¼š
  
  å½“ primary é›†ç¾¤ä¸­çš„ RBD é•œåƒå˜ä¸ºä¸å¯ç”¨æ—¶ï¼ˆéé›†ç¾¤ä¸å¯ç”¨ï¼‰ï¼Œå¯å¯¹ primary é›†ç¾¤æ‰§è¡Œé•œåƒçš„é™çº§ï¼ˆdemoteï¼‰æ“ä½œï¼Œè€Œå¯¹ secondary é›†ç¾¤æ‰§è¡Œé•œåƒçš„å‡çº§ï¼ˆpromoteï¼‰æ“ä½œã€‚
  
  ```bash
  $ rbd mirror image demote <pool-name>/<rbd-image-name>
    Image demoted to non-primary
  # primary é›†ç¾¤èŠ‚ç‚¹ï¼šé™çº§æ‰§è¡ŒæŒ‡å®šçš„ RBD é•œåƒ
  $ rbd mirror image promote <pool-name>/<rbd-image-name>
    Image promoted to primary
  # secondary é›†ç¾¤èŠ‚ç‚¹ï¼šå‡çº§æŒ‡å®šçš„ RBD é•œåƒ
  ```

- Ceph iSCSI Gateway ç›¸å…³å‘½ä»¤ï¼š

## CephFS æ–‡ä»¶ç³»ç»Ÿ

```bash
$ ceph fs status
```

- `dirty_ratio`ï¼šè„é¡µæ¯”ç‡
  
  ç‰©ç†å†…å­˜çš„ç¼“å­˜æ•°æ®ä¸èƒ½è¶…è¿‡ç‰©ç†å†…å­˜çš„ `30%`ï¼Œè¶…è¿‡åå°†å†™å…¥ç‰©ç†ç£ç›˜ï¼Œæé«˜ç£ç›˜æ€§èƒ½ã€‚

- `dirty_backgroud_ratio`ï¼š
  
  ç‰©ç†å†…å­˜ä½¿ç”¨è¶…è¿‡ `10%` æ—¶ï¼Œå°†åœ¨åå°å°†æ•°æ®å†™å…¥ç£ç›˜ã€‚

## å‚è€ƒé“¾æ¥

- [Product Documentation for Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5)

- â¤ [5.4.Â ä½¿ç”¨ Ceph Manager è´Ÿè½½å‡è¡¡å™¨æ¨¡å— Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5/html/operations_guide/using-the-ceph-manager-balancer-module_ops)

- [SERVICE MANAGEMENT | Ceph Docs](https://docs.ceph.com/en/latest/cephadm/services/?highlight=service_id#)

- [ä½¿ç”¨ cephadm æ­å»º cephï¼ˆoctopusï¼‰è¿‡ç¨‹](https://juejin.cn/post/7160585472538837006)

- [ChapterÂ 2.Â Management of services using the Ceph Orchestrator Red Hat Ceph Storage 5 | Red Hat Customer Portal](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/operations_guide/management-of-services-using-the-ceph-orchestrator#deploying-the-ceph-daemons-using-the-service-specification_ops)

- [Stray daemon tcmu-runner is reported not managed by cephadm - Red Hat Customer Portal](https://access.redhat.com/solutions/6472281)

- [Ceph - mapping rbd image is failing with RBD image feature set mismatch or image uses unsupported features - Red Hat Customer Portal](https://access.redhat.com/solutions/4270092)

- [maillist - rbd map image with journaling](https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/377S7XFN74MUYKVSXXRAN534FNZTDICK/)

- [ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢æ›´æ”¹ Ceph ä»ªè¡¨æ¿å¯†ç ](https://www.ibm.com/docs/zh/storage-ceph/6?topic=ia-changing-ceph-dashboard-password-using-command-line-interface)
