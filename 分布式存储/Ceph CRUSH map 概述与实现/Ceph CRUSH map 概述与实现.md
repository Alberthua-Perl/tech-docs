# ğŸš€ Ceph CRUSH map æ¦‚è¿°ä¸å®ç°

## æ–‡æ¡£è¯´æ˜

- è¯¥æ–‡æ¡£ä¸­çš„å‘½ä»¤ä¸é…ç½®å‡åœ¨ `Red Hat Ceph Storage v3.0` é›†ç¾¤ä¸­éªŒè¯ã€‚
- Red Hat Ceph Storage v3.0 å¯¹åº” `Ceph Luminous (12.2.13)` å¼€æºç‰ˆæœ¬ã€‚
- è¯¥æ–‡æ¡£ä¸­çš„ç›¸å…³æ¦‚å¿µã€ç¤ºä¾‹ä¸æ–¹æ³•åœ¨é«˜ç‰ˆæœ¬ Ceph é›†ç¾¤ä¸­å¤§éƒ¨åˆ†ä¾ç„¶å¯ç”¨ï¼Œä½†ç‰ˆæœ¬é—´çš„å·®å¼‚éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè¿›è¡Œæµ‹è¯•åŠè°ƒæ•´ï¼Œæ­¤å¤„æä¾›æ€è·¯ä¸æ–¹æ³•ã€‚

## æ–‡æ¡£ç›®å½•

- [CRUSH ä¸å¯¹è±¡æ”¾ç½®ç­–ç•¥](#crush-ä¸å¯¹è±¡æ”¾ç½®ç­–ç•¥)
- [CRUSH é…ç½®](#crush-é…ç½®)
- [CRUSH å¯è°ƒé¡¹ï¼ˆtunableï¼‰](#crush-å¯è°ƒé¡¹tunable)
- [CRUSH ä¸­çš„ OSD](#crush-ä¸­çš„-osd)
- [CRUSH ä¸­çš„ bucket ç±»å‹](#crush-ä¸­çš„-bucket-ç±»å‹)
- [Lab: CRUSH è§„åˆ™ï¼ˆruleï¼‰](#lab-crush-è§„åˆ™rule)
- [CRUSH ç®—æ³•æµç¨‹](#crush-ç®—æ³•æµç¨‹)
- [ç®¡ç† CRUSH map](#ç®¡ç†-crush-map)
- [Lab: CRUSH map è‡ªå®šä¹‰å±‚æ¬¡ç»“æ„ä¸è§„åˆ™](#lab-crush-map-è‡ªå®šä¹‰å±‚æ¬¡ç»“æ„ä¸è§„åˆ™)
- [è®¾ç½® OSD çš„ä½ç½®](#è®¾ç½®-osd-çš„ä½ç½®)
- [crushtool å‘½ä»¤ä½¿ç”¨ç¤ºæ„](#crushtool-å‘½ä»¤ä½¿ç”¨ç¤ºæ„)

## CRUSH ä¸å¯¹è±¡æ”¾ç½®ç­–ç•¥

- Ceph ä½¿ç”¨ `CRUSH`ï¼ˆ`Controlled Replication Under Scalable Hashing`ï¼Œå¯æ‰©å±•å“ˆå¸Œä¸‹çš„å—æ§å¤åˆ¶ï¼‰çš„æ”¾ç½®ç®—æ³•æ¥è®¡ç®—å“ªäº› OSD å­˜æ”¾å“ªäº›å¯¹è±¡ã€‚
- å¯¹è±¡åˆ†é…åˆ°æ”¾ç½®ç»„ï¼ˆPGï¼‰ä¸­ï¼ŒCRUSH å†³å®šè¿™äº› PG åº”ä½¿ç”¨å“ªäº› OSD æ¥å­˜å‚¨å…¶å¯¹è±¡ã€‚
- ç†æƒ³æƒ…å†µä¸‹ï¼Œç®—æ³•éœ€è¦å°†æ•°æ®å‡åŒ€åˆ†å¸ƒåˆ°å¯¹è±¡å­˜å‚¨ä¸­ã€‚
- ğŸ¤˜ å½“æ·»åŠ äº†æ–°çš„ OSD æˆ–ç°æœ‰ OSD ä¸»æœºå‡ºç°æ•…éšœæ—¶ï¼ŒCeph èƒ½å¤Ÿä½¿ç”¨ CRUSH åœ¨æ´»è·ƒçš„ OSD é—´é‡æ–°å¹³è¡¡ï¼ˆrebalanceï¼‰é›†ç¾¤ä¸­çš„å¯¹è±¡ã€‚

> âœ… æ³¨æ„ï¼š
>
> OSD ä¸»æœºæˆ–éƒ¨åˆ† OSD æ•…éšœï¼šé›†ç¾¤é€šè¿‡ cluster network å®ç°å‰¯æœ¬æ¢å¤ä¸æ•°æ®é‡å¹³è¡¡ã€‚
>
> OSD æ¢å¤æˆ–æ·»åŠ æ–°çš„ OSDï¼šé›†ç¾¤é€šè¿‡ cluster network å®ç°æ•°æ®å›å¡«ä¸æ•°æ®é‡å¹³è¡¡ã€‚

- å¯è°ƒä¼˜æ”¾ç½®ç®—æ³•ï¼Œä»¥ç¡®ä¿ CRUSH è¿è¡Œé¡ºç•…ã€‚
- âœ¨ `CRUSH map` æ˜¯ CRUSH ç®—æ³•çš„ä¸­å¤®é…ç½®æœºåˆ¶ã€‚
- é»˜è®¤æƒ…å†µä¸‹ï¼ŒCRUSH ç®—æ³•å°†å¤åˆ¶çš„å¯¹è±¡æ”¾ç½®åˆ°ä¸åŒä¸»æœºä¸Šçš„ OSD ä¸­ã€‚
  
  ![object-pg-crush-osd-mapping](images/object-pg-crush-osd-mapping.png)

- ğŸ‘‰ å¯ä»¥é…ç½® `CRUSH map`Â ä¸Â `CRUSH rules`ï¼Œä½¿å¾—å¯¹è±¡å¤åˆ¶åˆ°ä½äºä¸åŒæˆ¿é—´æˆ–ä¸åŒ PDUï¼ˆé…ç”µè£…ç½®ï¼‰ä¾›ç”µçš„ä¸»æœºä¸Šçš„ OSDã€‚ä¾‹å¦‚ï¼Œå°†å¸¦æœ‰ `SSD` é©±åŠ¨å™¨çš„ OSD åˆ†é…ç»™éœ€è¦æå¿«å­˜å‚¨çš„åº”ç”¨ä½¿ç”¨çš„æ± ï¼Œå¹¶å°†å¸¦æœ‰ä¼ ç»Ÿ `SATA` ç¡¬ç›˜é©±åŠ¨å™¨çš„ OSD åˆ†é…ç»™æ”¯æŒè¾ƒä½éœ€æ±‚å·¥ä½œè´Ÿè½½çš„æ± ã€‚
- âœ¨ `CRUSH map` åŒ…å«ä¸¤å¤§ç»„æˆéƒ¨åˆ†ï¼š
  - CRUSH å±‚æ¬¡ç»“æ„ï¼ˆhierarchyï¼‰ï¼š
    - å®ƒåˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ OSD å¹¶å°†å®ƒä»¬æ•´ç†åˆ° `bucket` æ ‘å½¢ç»“æ„ä¸­ã€‚
    - CRUSH å±‚æ¬¡ç»“æ„é€šå¸¸ç”¨äºä»£è¡¨ OSD æ‰€å¤„çš„ä½ç½®ã€‚
    - é»˜è®¤æƒ…å†µä¸‹ï¼Œæœ‰ä¸€ä¸ªè¡¨ç¤ºæ•´ä¸ªå±‚æ¬¡ç»“æ„çš„æ ¹ï¼ˆrootï¼‰bucketï¼Œå«æœ‰æ¯ä¸ª OSD ä¸»æœºçš„ä¸»æœºbucketï¼ŒOSD åˆ™æ˜¯æ ‘ä¸Šçš„æ ‘å¶ï¼ˆleafï¼‰ã€‚
    - é»˜è®¤æƒ…å†µä¸‹ï¼ŒåŒä¸€ OSD ä¸»æœºä¸Šçš„æ‰€æœ‰ OSD éƒ½æ”¾åœ¨è¯¥ä¸»æœº bucket ä¸­ã€‚
    - å¯è‡ªå®šä¹‰æ ‘ç»“æ„ï¼Œå¯¹å®ƒé‡æ–°æ’åˆ—æˆ–æ·»åŠ æ›´å¤šå±‚çº§ï¼Œä»¥å®ç°ä¸€äº›ç›®çš„ï¼Œå¦‚å°† OSD ä¸»æœºåˆ†ç»„åˆ°ä»£è¡¨å®ƒä»¬åœ¨ä¸åŒæœºæ¶ï¼ˆ`rack`ï¼‰æˆ–æ•°æ®ä¸­å¿ƒï¼ˆ`datacenter`ï¼‰ä¸­æ‰€å¤„ä½ç½®çš„ bucket ä¸­ã€‚
  - è‡³å°‘ä¸€æ¡ CRUSH è§„åˆ™ï¼ˆruleï¼‰ï¼š
    - å®ƒå†³å®šäº†å¦‚ä½•ä»è¿™äº› bucket ä¸­åˆ†é… OSD ç»™ PGï¼Œå†³å®šäº†åœ¨å“ªé‡Œå­˜å‚¨è¿™äº› PG çš„å¯¹è±¡ã€‚
    - ä¸åŒçš„æ± å¯èƒ½ä½¿ç”¨æ¥è‡ª CRUSH map çš„ä¸åŒ CRUSH è§„åˆ™ã€‚

## CRUSH é…ç½®

- CRUSH map çš„å®šä¹‰é‡ŒåŒ…å«ï¼š
  - æ‰€æœ‰å¯ç”¨ç‰©ç†å­˜å‚¨è®¾å¤‡åˆ—è¡¨
  - åŒ…å«æ‰€æœ‰åŸºç¡€æ¶æ„ bucket ä»¥åŠå„è‡ªå«æœ‰çš„ OSD å­˜å‚¨è®¾å¤‡æˆ–å…¶ä»– bucket ID çš„åˆ—è¡¨
  - åŒ…å« PG ä¸ OSD map çš„ `CRUSH rule` åˆ—è¡¨
  - å…¶ä»– CRUSH å¯è°ƒé¡¹åŠå…¶è®¾ç½®çš„åˆ—è¡¨
- Ceph é›†ç¾¤ä½¿ç”¨é»˜è®¤éƒ¨ç½²ï¼Œå°†é…ç½®é»˜è®¤çš„ CRUSH mapã€‚
- æŸ¥çœ‹ CRUSH map çš„ç›¸å…³å‘½ä»¤ï¼š
  
  ```bash
  $ ceph osd crush dump
  # ä»¥ JSON æ ¼å¼æŸ¥çœ‹ CRUSH map çš„è¯¦ç»†ä¿¡æ¯
  $ ceph osd getcrushmap -o <binfile>
  # å¯¼å‡º CRUSH map çš„äºŒè¿›åˆ¶æ–‡ä»¶
  $ crushtool -d <binfile> -o <textfile>
  # å°† CRUSH map çš„äºŒè¿›åˆ¶æ–‡ä»¶åç¼–è¯‘ä¸ºæ–‡æœ¬æ–‡ä»¶
  # ä»¥ä¸Šå‘½ä»¤åˆ†åˆ«ä»¥ä¸åŒçš„å½¢å¼å¯¼å‡º CRUSH map
  ```

## CRUSH å¯è°ƒé¡¹ï¼ˆtunableï¼‰

- å¯ä»¥ä½¿ç”¨é€‰é¡¹æ¥è°ƒæ•´ä¸ç¦ç”¨æˆ–å¯ç”¨ CRUSH ç®—æ³•çš„åŠŸèƒ½ã€‚
- è¾ƒæ–°ç‰ˆæœ¬çš„ Ceph å¢å¼ºå¹¶ä¸”ä¼˜åŒ–äº† CRUSHï¼Œå¯æä¾›æ›´å¥½ã€æ›´å¿«åœ°æ˜ å°„ï¼Œä»¥åŠæ›´å‡åŒ€çš„ PG åˆ†å¸ƒã€‚
- CRUSH map çš„å¼€å¤´å®šä¹‰å¯è°ƒé¡¹ã€‚
- è°ƒæ•´ CRUSH å¯è°ƒé¡¹æˆ–è®¸ä¼šæ”¹å˜ CRUSH å°† PG æ˜ å°„åˆ° OSD çš„æ–¹å¼ã€‚
- å‘ç”Ÿè¿™ç§æƒ…å†µæ—¶ï¼Œé›†ç¾¤å°†éœ€è¦æŠŠå¯¹è±¡ç§»åˆ°é›†ç¾¤ä¸­çš„ä¸åŒ OSD ä»¥æ›´æ­£å¯¹è±¡æ”¾ç½®ï¼Œæ¥åæ˜ é‡æ–°è®¡ç®—åçš„æ˜ å°„ã€‚
- CRUSH å¯è°ƒé¡¹çš„ç›¸å…³å‘½ä»¤ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ```bash
  $ ceph osd crush show-tunables
  {
      "choose_local_tries": 0,
      "choose_local_fallback_tries": 0,
      "choose_total_tries": 50,
      "chooseleaf_descend_once": 1,
      "chooseleaf_vary_r": 1,
      "chooseleaf_stable": 1,
      "straw_calc_version": 1,
      "allowed_bucket_algs": 54,
      "profile": "jewel",
      "optimal_tunables": 1,
      "legacy_tunables": 0,
      "minimum_required_version": "jewel",
      "require_feature_tunables": 1,
      "require_feature_tunables2": 1,
      "has_v2_rules": 1,
      "require_feature_tunables3": 1,
      "has_v3_rules": 0,
      "has_v4_buckets": 1,
      "require_feature_tunables5": 1,
      "has_v5_rules": 0
  }
  # æŸ¥çœ‹ CRUSH çš„å¯è°ƒé¡¹
  
  $ ceph osd crush set-tunable <tunable_option> <value>
  # è®¾ç½®æŒ‡å®šçš„å¯è°ƒé¡¹
  $ ceph osd crush tunables <profile>
  # è®¾ç½®é¢„å®šä¹‰çš„å¯è°ƒé¡¹ profile
  ```

- ä¸€äº› profile éœ€è¦ Ceph æˆ– Ceph å®¢æˆ·ç«¯çš„æœ€ä½ç‰ˆæœ¬ã€‚
- âœ… Red Hat å»ºè®®æ‰€æœ‰é›†ç¾¤å®ˆæŠ¤è¿›ç¨‹ä¸å®¢æˆ·ç«¯ä½¿ç”¨ç›¸åŒçš„å‘è¡Œç‰ˆæœ¬ã€‚
- `profile` çš„å€¼å¯ä»¥æ˜¯å¦‚ä¸‹ä»»ä¸€ä¸ªï¼š
  - `legacy`ï¼šCeph ç¤¾åŒºå‘è¡Œç‰ˆ 0.47ï¼ˆArgonaut å‰ï¼‰ä¸æ›´æ—©ç‰ˆæœ¬
  - `argonaut`ï¼šç¤¾åŒºå‘è¡Œç‰ˆ Argonautï¼ˆCRUSH V1ï¼‰æ‰€æ”¯æŒçš„å€¼
  - `bobtail`ï¼šç¤¾åŒºå‘è¡Œç‰ˆ Bobtail ä¸ Dumplingï¼ˆCRUSH V2ï¼‰æ‰€æ”¯æŒçš„å€¼
  - `firefly`ï¼šç¤¾åŒºå‘è¡Œç‰ˆ Firefly ä¸ RHCS v1.2.3ï¼ˆCRUSH V3ï¼‰æ‰€æ”¯æŒçš„å€¼
  - `hammer`ï¼šç¤¾åŒºå‘è¡Œç‰ˆ Hammer ä¸ RHCS v1.3ï¼ˆCRUSH V4ï¼‰æ‰€æ”¯æŒçš„å€¼
  - ğŸ‘‰ `jewel`ï¼šç¤¾åŒºå‘è¡Œç‰ˆ Jewel ä¸ RHCS v2ï¼ˆéš RHEL 7.3 å‘å¸ƒï¼‰åŠæ›´é«˜ç‰ˆæœ¬ï¼ˆCRUSH V5ï¼‰æ‰€æ”¯æŒçš„å€¼ã€‚
  - ğŸ‘‰ `optimal`ï¼šå½“å‰ç‰ˆæœ¬çš„ RHCS çš„æœ€ä½³ï¼ˆæœ€ä¼˜ï¼‰å€¼

## CRUSH ä¸­çš„ OSD

- CRUSH map åŒ…å« Ceph ä¸­æ‰€æœ‰å­˜å‚¨è®¾å¤‡çš„åˆ—è¡¨ã€‚
- æ¯ä¸€å­˜å‚¨è®¾å¤‡éƒ½æä¾›æœ‰ä»¥ä¸‹ä¿¡æ¯ï¼š
  - å­˜å‚¨è®¾å¤‡çš„ ID
  - å­˜å‚¨è®¾å¤‡çš„åç§°ï¼ˆnameï¼‰
  - ğŸ¤˜ å­˜å‚¨è®¾å¤‡çš„æƒé‡ï¼ˆweightï¼‰ï¼š
    - é€šå¸¸åŸºäºä»¥ `TB` ä¸ºå•ä½çš„å®¹é‡
    - å¦‚ï¼Œ`4TB` æƒé‡å¤§çº¦ä¸º `4.0`ï¼Œè¿™æ˜¯è®¾å¤‡å¯ä»¥å­˜å‚¨çš„ç›¸å¯¹æ•°æ®é‡ï¼ŒCRUSH ç®—æ³•ç”¨å®ƒæ¥å¸®åŠ©ç¡®ä¿å¯¹è±¡å‡åŒ€åˆ†å¸ƒã€‚
    - `ceph-ansible` å®‰è£…ç¨‹åºä¼šè‡ªåŠ¨è®¾ç½®æƒé‡ã€‚
  - å­˜å‚¨è®¾å¤‡çš„ç±»å‹ï¼ˆclassï¼‰ï¼š
    - åŒ…æ‹¬ï¼š`HDD (hdd)`ã€`SSD (ssd)`ã€`NVMe (nvme)`
    - OSD è‡ªåŠ¨æ£€æµ‹ä¸è®¾ç½®å…¶è®¾å¤‡ç±»å‹
  
  > ä»¥ä¸Š osd çš„è®¾å¤‡ç±»å‹å°†åœ¨ä¸‹æ–‡çš„ç¤ºä¾‹ä¸­ä½¿ç”¨ã€‚

## CRUSH ä¸­çš„ bucket ç±»å‹

> ğŸ’¥ CRUSH ä¸­çš„ bucket ä¸ radosgw ä¸­çš„ bucket ä¸åŒï¼

- CRUSH å±‚æ¬¡ç»“æ„å°† OSD æ•´ç†åˆ°ç”±ä¸åŒæ•…éšœåŸŸç»„æˆçš„æ ‘ä¸­ï¼Œè¿™äº›æ•…éšœåŸŸç§°ä¸º `bucket`ã€‚
- bucket æ˜¯æ•…éšœåŸŸï¼Œæˆ–æ˜¯ CRUSH å±‚æ¬¡ç»“æ„ä¸­çš„æ ‘æï¼ˆ`branch`ï¼‰ã€‚
- device æ˜¯ OSDï¼Œæˆ–æ˜¯ CRUSH å±‚æ¬¡ç»“æ„ä¸­çš„æ ‘å¶ï¼ˆ`leaf`ï¼‰ã€‚
- ğŸš€ é€šè¿‡åˆ›å»º CRUSH map è§„åˆ™ï¼Œæ—¢å¯ä»¥ä½¿ Ceph å°†å¯¹è±¡çš„å‰¯æœ¬æ”¾åœ¨ä¸åŒæœåŠ¡å™¨ä¸Šçš„ OSD ä¸­ï¼Œä¹Ÿå¯ä»¥ä½¿å®ƒæ”¾åœ¨ä½äºä¸åŒçš„æœºæ¶æˆ–ä¸åŒæ•°æ®ä¸­å¿ƒä¸­çš„ OSD ä¸­ã€‚
- å¯¹äº Ceph é›†ç¾¤çš„å¤§å‹éƒ¨ç½²ï¼Œå¯åˆ›å»ºå¦‚ä¸‹æ‰€ç¤ºçš„å…·ä½“å±‚æ¬¡ç»“æ„ï¼š
  
  ![crush-map-bucket-1](images/crush-map-bucket-1.jpg)

- é‡è¦çš„ bucket å±æ€§ï¼š
  - bucket çš„ IDï¼šè¯¥ ID ä¸ºè´Ÿæ•°ï¼Œä»¥ä¾¿ä¸å­˜å‚¨è®¾å¤‡ï¼ˆOSDï¼‰ID åŒºåˆ†ã€‚
  - bucket çš„åç§°ï¼ˆnameï¼‰
  - bucket çš„ç±»å‹ï¼ˆtypeï¼‰ï¼š
    - é»˜è®¤ map å®šä¹‰äº†å‡ ç§ç±»å‹ï¼Œå¯é€šè¿‡ `ceph osd crush dump` å‘½ä»¤æ¥æ£€ç´¢ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
      - `root`ï¼šé»˜è®¤æœ€é¡¶çº§ bucket
      - `region`ï¼šåŒºåŸŸï¼ˆåœ°ç†èŒƒç•´ï¼‰
      - `datacenter`ï¼šæ•°æ®ä¸­å¿ƒï¼ˆå¯èƒ½ç”±å¤šä¸ªæœºæˆ¿ç»„æˆæ•°æ®ä¸­å¿ƒï¼‰
      - `room`ï¼šæœºæˆ¿
      - `pod`ï¼šå¤šä¸ªæœºæ¶åˆ—
      - `pdu`ï¼šé…ç”µå•å…ƒï¼ˆå¯èƒ½å¤šä¸ªæœºæ¶åˆ—å…±ç”¨ä¸€ä¸ªé…ç”µå•å…ƒï¼‰
      - `row`ï¼šæœºæ¶åˆ—
      - `rack`ï¼šæœºæ¶ï¼ˆä¸€ä¸ªæœºæ¶å¯åŒ…å«å¤šä¸ªæœºç®±ï¼‰
      - `chassis`ï¼šæœºç®±
      - `host`ï¼šæœåŠ¡å™¨
    - é™¤äº†ä»¥ä¸Šçš„é»˜è®¤ bucket ç±»å‹å¤–ï¼Œè¿˜å¯è‡ªå®šä¹‰ bucket ç±»å‹ã€‚
    - ä»¥ä¸Š bucket ç±»å‹åœ¨è‡ªå®šä¹‰çš„ CRUSH å±‚æ¬¡ç»“æ„ä¸­å¯æ— éœ€å…¨éƒ¨ä½¿ç”¨ï¼Œå¯åªä½¿ç”¨å…¶ä¸­ä¸€éƒ¨åˆ† bucket ç±»å‹ã€‚
    - ğŸ¤˜ Ceph åœ¨æ˜ å°„ PG å‰¯æœ¬åˆ° OSD æ—¶ä½¿ç”¨ `straw2` æ•…éšœåŸŸç®—æ³•ã€‚
- CRUSH map å¯ä»¥åŒ…å«å¤šä¸ªå±‚æ¬¡ç»“æ„ï¼Œå¯é€šè¿‡ä¸åŒ CRUSH è§„åˆ™åŠ ä»¥é€‰æ‹©ã€‚
- Ceph éƒ¨ç½²æ—¶ä»…åˆ›å»ºä¸¤ä¸ªå±‚çº§çš„å±‚æ¬¡ç»“æ„ï¼Œå³ `host bucket` ä¸Â `device leaf`ã€‚
- CRUSH map é»˜è®¤å±‚æ¬¡ç»“æ„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ![crush-map-bucket-2](images/crush-map-bucket-2.jpg)

## Lab: CRUSH è§„åˆ™ï¼ˆruleï¼‰

- CRUSH map ä¹ŸåŒ…å«æ•°æ®æ”¾ç½®è§„åˆ™ï¼Œå®ƒä»¬å†³å®šäº† PG å¦‚ä½• map åˆ° OSD æ¥å­˜å‚¨å¯¹è±¡ã€‚
- é»˜è®¤æœ‰ä¸¤ç§è§„åˆ™ï¼š`replicated_rule`ã€`erasure-code`
- ä»¥ä¸Šè¿™äº›é»˜è®¤è§„åˆ™å°†æ¯ä¸€ PG æ˜ å°„åˆ°ä¸åŒä¸»æœºä¸Šçš„ OSD ä¸Šã€‚
- æŸ¥çœ‹ CRUSH è§„åˆ™ï¼š
  
  ```bash
  $ ceph osd crush rule ls
  # åˆ—å‡ºç°æœ‰çš„ CRUSH è§„åˆ™
  $ ceph osd crush rule dump <rule_name>
  # æŸ¥çœ‹ CRUSH è§„åˆ™çš„è¯¦ç»†ä¿¡æ¯
  ```

- CRUSH è§„åˆ™ç¤ºä¾‹ï¼šä½¿ç”¨åç¼–è¯‘åçš„ CRUSH map ä¹ŸåŒ…å«è§„åˆ™ä¸”æ›´å®¹æ˜“é˜…è¯»
  
  ```bash
  $ ceph osd getcrushmap -o ./map.bin
  $ crushtool -d ./map.bin -o ./map.txt
  $ cat ./map.txt
    ...
    # rules
    rule replicated_rule {  # 1ï¸âƒ£
        id 0  # 2ï¸âƒ£
        type replicated
        min_size 1  # 3ï¸âƒ£
        max_size 10 # 4ï¸âƒ£
        step take default  # 5ï¸âƒ£
        step chooseleaf firstn 0 type host  # 6ï¸âƒ£
        step emit  # 7ï¸âƒ£
    }
    ...
  # æŸ¥çœ‹ CRUSH è§„åˆ™çš„è¯¦ç»†ä¿¡æ¯
  ```
  
  - 1ï¸âƒ£ è§„åˆ™çš„åç§°ï¼šåœ¨é€šè¿‡ ceph osd pool create å‘½ä»¤åˆ›å»ºæ± æ—¶ï¼Œå¯ä½¿ç”¨è¯¥åç§°æ¥é€‰æ‹©è§„åˆ™ã€‚
  - 2ï¸âƒ£ è§„åˆ™çš„ IDï¼šä¸€äº›å‘½ä»¤ä½¿ç”¨è§„åˆ™ IDï¼Œè€Œä¸ä½¿ç”¨è§„åˆ™åç§°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

    ```bash
    $ ceph osd pool set <pool_name> crush_ruleset <ID>
    # ä¸ºç°æœ‰çš„æŒ‡å®šçš„æ± è®¾ç½®è§„åˆ™
    ```
  
  - 3ï¸âƒ£ æœ€å°‘çš„å¯¹è±¡å‰¯æœ¬æ•°ï¼šè‹¥æ± å½¢æˆçš„å‰¯æœ¬æ•°å°‘äºè¯¥æ•°ç›®ï¼ŒCRUSH ä¸ä¼šé€‰æ‹©è¯¥è§„åˆ™ã€‚
  - 4ï¸âƒ£ æœ€å¤šçš„å¯¹è±¡å‰¯æœ¬æ•°ï¼šè‹¥æ± å½¢æˆçš„å‰¯æœ¬æ•°å¤šäºè¯¥æ•°ç›®ï¼ŒCRUSH ä¸ä¼šé€‰æ‹©è¯¥è§„åˆ™ã€‚
  - 5ï¸âƒ£ å– `bucket` åç§°ï¼Œå†æ²¿ç€æ ‘ç»“æ„å¾€ä¸‹è¿­ä»£ã€‚
    - ğŸ¤˜ è¯¥ step å®šä¹‰ PG æ˜ å°„çš„ OSD çš„åˆ†å¸ƒèŒƒå›´ã€‚
    - å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿­ä»£å§‹äºåä¸º default çš„ bucketï¼Œå³é»˜è®¤ CRUSH å±‚æ¬¡ç»“æ„çš„æ ¹éƒ¨ã€‚
    - å¯¹äºç”±å¤šä¸ªæ•°æ®ä¸­å¿ƒï¼ˆdatacenterï¼‰ç»„æˆçš„å¤æ‚å±‚æ¬¡ç»“æ„ï¼Œå¯ä»¥ä¸ºä¸€ä¸ªæ•°æ®ä¸­å¿ƒåˆ›å»ºä¸€æ¡è§„åˆ™ï¼Œä½¿å¾—ç‰¹å®šæ± ä¸­çš„å¯¹è±¡å¼ºåˆ¶å­˜å‚¨åˆ°è¯¥æ•°æ®ä¸­å¿ƒå†…çš„ OSD ä¸­ï¼Œåœ¨è¿™ç§æƒ…å½¢ä¸­ï¼Œæ­¤ stepå¯ä»¥åœ¨ `datacenter` bucket å¤„å¼€å§‹è¿­ä»£ã€‚
  - 6ï¸âƒ£ é€‰æ‹©ç»™å®š bucket ç±»å‹çš„ bucket é›†åˆï¼Œå¹¶ä¸”ä»è¯¥é›†åˆä¸­å„ä¸ª bucket çš„å­æ ‘ä¸­é€‰æ‹©æ ‘å¶ï¼ˆOSDï¼‰ã€‚
    - åœ¨æœ¬ä¾‹ä¸­ï¼Œè§„åˆ™ä»é›†åˆçš„æ¯ä¸€ host bucket ä¸Šé€‰æ‹©ä¸€ä¸ª OSDï¼Œç¡®ä¿ OSD æ¥è‡ªä¸åŒçš„ä¸»æœºã€‚

    > ğŸ“ æ± ä¸­å‰¯æœ¬æ•°é‡ç”±é›†ç¾¤éƒ¨ç½²æ—¶å®šä¹‰çš„ `osd_pool_default_size` å‚æ•°å®šä¹‰ã€‚

    - ğŸ‘‰ è‹¥Â `firstn` åé¢çš„æ•°å­—ä¸º `0`ï¼Œåˆ™é€‰æ‹©çš„ bucket ä¸æ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ç›¸åŒï¼ˆé»˜è®¤æƒ…å†µï¼‰ã€‚
    - ğŸ‘‰ è‹¥æ•°å­—å¤§äºé›¶ï¼Œä½†å°äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ï¼Œåˆ™é€‰æ‹©è¯¥æ•°é‡çš„ bucketã€‚
      - è¿™æ—¶ï¼Œè§„åˆ™éœ€è¦å¦ä¸€ä¸ª step ä¸ºå‰©ä½™çš„å‰¯æœ¬æŠ½å– bucketï¼Œå¯ä»¥è¿ç”¨è¿™ç§æœºåˆ¶æ¥å¼ºåˆ¶å®æ–½å¯¹è±¡å‰¯æœ¬æŸä¸€å­é›†çš„ä½ç½®ã€‚

      > ğŸ’¥ è‹¥æ— å¦ä¸€ä¸ª step ä¸ºå‰©ä½™çš„å‰¯æœ¬æŠ½å– bucketï¼Œé‚£ä¹ˆå‰©ä½™çš„å‰¯æœ¬ä¸å†è¿›è¡Œ pg è‡³ osd çš„æ˜ å°„ï¼Œå®é™…çš„å‰¯æœ¬æ•°å°äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ï¼

      - å¦‚ä¸‹ CRUSH è§„åˆ™æ‰€ç¤ºï¼š

        ```bash
        $ cat ./cm-firstn-1.txt
          ...
          rule crush_test_replicated {
              id 2
              type replicated
              min_size 1
              max_size 10
              step take default
              step chooseleaf firstn 1 type host
              step emit
          }
          ...
        # è‹¥ä½¿ç”¨è¯¥è§„åˆ™åˆ›å»ºå®šä¹‰ 2 å‰¯æœ¬çš„æ± ï¼Œè¯¥è§„åˆ™å®šä¹‰ä» 1 ä¸ª host bucket ä¸­é€‰æ‹© 1 ä¸ª
        # osd è¿›è¡Œ pg è‡³ osd çš„æ˜ å°„ã€‚
        # å› æ­¤ï¼Œè‹¥ä½¿ç”¨è¯¥è§„åˆ™åˆ›å»ºæ± ï¼Œpg åªè¢«æ˜ å°„è‡³ 1 ä¸ª osd ä¸­ã€‚ 
        $ crushtool -c ./cm-firstn-1.txt -o ./cm-firstn-1.bin
        # å°†æ–‡æœ¬å½¢å¼çš„ CRUSH map ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶å½¢å¼
        $ ceph osd setcrushmap -i ./cm-firstn-1.bin
        # å°†æ–°ç”Ÿæˆçš„äºŒè¿›åˆ¶å½¢å¼çš„ CRUSH map å¯¼å…¥è‡³é›†ç¾¤ä¸­
        $ ceph osd crush rule ls
          replicated_rule
          myerasurepool
          crush_test_replicated
        # æŸ¥çœ‹å½“å‰é›†ç¾¤æœ€æ–°ç‰ˆæœ¬çš„ CRUSH è§„åˆ™
        $ ceph osd pool create tc-pool-firstn-1 8 8 crush_test_replicated
          pool 'tc-pool-firstn-1' created
        # ä½¿ç”¨ crush_test_replicated è§„åˆ™åˆ›å»º tc-pool-firstn-1 å­˜å‚¨æ± 
        ```

        ![ceph-crush-rule-firstn-1](images/ceph-crush-rule-firstn-1.jpg)

        å¦‚ä¸Šæ‰€ç¤ºï¼Œåˆ›å»ºçš„æ± ä¸­ pg çŠ¶æ€å§‹ç»ˆä¸º `active+undersized+degraded`ï¼Œæç¤º pg æ•°é‡å°äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°é‡ï¼Œè¿™æ˜¯ç”±äºè‡ªå®šä¹‰çš„ CRUSH è§„åˆ™ä¸­å®šä¹‰ä» 1 ä¸ª host bucket ä¸­é€‰å– 1 ä¸ª osd è¿›è¡Œ pg çš„æ˜ å°„ï¼Œå› æ­¤åªæœ‰ 1 ä¸ªå‰¯æœ¬å­˜å‚¨äº osd ä¸­ï¼Œè°ƒæ•´æ± çš„å‰¯æœ¬æ•° 2 ä¸º 1 å³å¯æ¢å¤ pg çŠ¶æ€ã€‚

    - ğŸ‘‰ è‹¥æ•°å­—å°äºé›¶ï¼Œåˆ™ä»æ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°å‡å»å…¶ç»å¯¹å€¼ï¼Œå†é€‰æ‹©è¿™ä¸ªæ•°é‡çš„ bucketã€‚å¦‚ä¸‹ CRUSH è§„åˆ™æ‰€ç¤ºï¼š

      ```bash
      $ cat ./cm-firstn--1.txt
        ...
        rule crush_test_firstn {
            id 3
            type replicated
            min_size 1
            max_size 10
            step take default
            step chooseleaf firstn -1 type host
            step emit
        }
        ...
      # è‹¥ä½¿ç”¨è¯¥è§„åˆ™åˆ›å»ºå®šä¹‰ 3 å‰¯æœ¬çš„æ± ï¼Œè¯¥è§„åˆ™å®šä¹‰ä» 2ï¼ˆ3-|-1|ï¼‰ä¸ª host bucket ä¸­
      # åˆ†åˆ«é€‰æ‹© 1 ä¸ª osd è¿›è¡Œ pg è‡³ osd çš„æ˜ å°„ã€‚
      # å› æ­¤ï¼Œè‹¥ä½¿ç”¨è¯¥è§„åˆ™åˆ›å»ºæ± ï¼Œpg å°†è¢«æ˜ å°„è‡³ 2 ä¸ª osd ä¸­ã€‚
      # ç”±äºå®é™…çš„å‰¯æœ¬æ•°ï¼ˆ2 ä¸ªï¼‰å°äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ï¼ˆ3 ä¸ªï¼‰ï¼Œpg çŠ¶æ€å°†æ˜¾ç¤ºä¸º 
      # active+undersized+degraded
      
      ### ç¼–è¯‘ä¸å¯¼å…¥ CRUSH map çš„è¿‡ç¨‹æ­¤å¤„çœç•¥
      $ ceph osd pool create tc-pool-firstn--1 8 8 crush_test_firstn
        pool 'tc-pool-firstn--1' created
      $ ceph -w
        cluster:
          id:     fa354f06-1bcc-406e-9f1e-a34222f8a155
          health: HEALTH_WARN
                  Degraded data redundancy: 
                  8 pgs unclean, 8 pgs degraded, 8 pgs undersized
      
        services:
          mon: 3 daemons, quorum serverc,serverd,servere
          mgr: servere(active), standbys: serverd, serverc
          mds: cephfs-1/1/1 up  {0=servera=up:active}
          osd: 9 osds: 9 up, 9 in
          rgw: 1 daemon active
      
        data:
          pools:   13 pools, 176 pgs
          objects: 411 objects, 286 MB
          usage:   2035 MB used, 168 GB / 170 GB avail
          pgs:     168 active+clean
                   8   active+undersized+degraded
      
        2022-06-06 11:08:58.246678 mon.serverc [WRN] overall HEALTH_WARN Degra
            ded data redundancy: 8 pgs unclean, 8 pgs degraded, 8 pgs undersized
      
      $ ceph pg dump pgs_brief | grep ^22
        dumped pgs_brief
        22.5    active+undersized+degraded [6,3]    6  [6,3]              6 
        22.4    active+undersized+degraded [3,5]    3  [3,5]              3 
        22.1    active+undersized+degraded [2,3]    2  [2,3]              2 
        22.2    active+undersized+degraded [7,6]    7  [7,6]              7 
        22.3    active+undersized+degraded [1,8]    1  [1,8]              1 
        22.0    active+undersized+degraded [1,0]    1  [1,0]              1 
        22.6    active+undersized+degraded [2,5]    2  [2,5]              2 
        22.7    active+undersized+degraded [3,2]    3  [3,2]              3
      # åˆ›å»ºçš„æ± ä¸­ pg å°†å¤„äº active+undersized+degraded çŠ¶æ€
      ```

      ![ceph-crush-rule-firstn--1-1](images/ceph-crush-rule-firstn--1-1.jpg)

      ![images/ceph-crush-rule-firstn--1-2](images/ceph-crush-rule-firstn--1-2.jpg)

      é‡æ–° `peering` ä¸ `remapped` çš„ pg ä¾ç„¶å¯è¢« Ceph å®¢æˆ·ç«¯è¯»å†™ã€‚

  - 7ï¸âƒ£ è¾“å‡ºè§„åˆ™çš„ç»“æœ

- ç¤ºä¾‹ï¼š
  
  ```bash
  rule myrackruleinDC1 {
      id 2
      type replicated
      min_size 1
      max_size 10
      step take DC1
      step chooseleaf firstn 0 type rack
      step emit
  }
  # è‹¥ä½¿ç”¨è¯¥è§„åˆ™åˆ›å»ºå®šä¹‰ 3 å‰¯æœ¬çš„æ± ï¼Œä»…ä» DC1 æ•°æ®ä¸­å¿ƒ bucket å¼€å§‹é€‰æ‹©ï¼Œå¹¶ä» 3 ä¸ª
  # æœºæ¶ï¼ˆrackï¼‰ä¸­é€‰æ‹©ä»»æ„ 1 ä¸ª osdï¼ˆå…± 3 ä¸ª osdï¼‰è¿›è¡Œ pg è‡³ osd çš„æ˜ å°„ã€‚
  ```

## CRUSH ç®—æ³•æµç¨‹

- `take`ï¼šä» step take é€‰æ‹©ä¸€ä¸ªæ ¹èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹ä¸ä¸€å®šæ˜¯ rootï¼Œè¯¥èŠ‚ç‚¹å¯ä»¥æ˜¯ä»»ä½•ä¸€ä¸ªæ•…éšœåŸŸï¼Œä»æŒ‡å®šé€‰æ‹©çš„èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œã€‚
- `select`ï¼šä» step chooseleaf å®šä¹‰çš„ bucket ç±»å‹ï¼ˆtypeï¼‰å¼€å§‹é€‰æ‹©åˆé€‚çš„ OSDï¼Œå¦‚æœæ˜¯å¤åˆ¶æ± é»˜è®¤ä½¿ç”¨ `firstn` ç®—æ³•ï¼Œå¦‚æœæ˜¯çº åˆ ä»£ç æ± é»˜è®¤ä½¿ç”¨ `indep` ç®—æ³•ã€‚
- `emit`ï¼šè¿”å›æœ€ç»ˆç»“æœã€‚

## ç®¡ç† CRUSH map

- ä¿®æ”¹ CRUSH map çš„ä¸¤ç§æ–¹å¼ï¼š
  - æ–¹å¼ 1ï¼šä½¿ç”¨ `ceph osd crush` å‘½ä»¤
  - æ–¹å¼ 2ï¼šæå–å¹¶åç¼–è¯‘äºŒè¿›åˆ¶ CRUSH map åˆ°çº¯æ–‡æœ¬ï¼Œç¼–è¾‘è¯¥çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œå†é‡æ–°ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œå†é‡æ–°å¯¼å…¥åˆ°é›†ç¾¤ä¸­ã€‚
- é€šå¸¸è€Œè¨€ï¼Œä½¿ç”¨ ceph osd crush å‘½ä»¤æ›´æ–° CRUSH map è¾ƒè½»æ¾ã€‚
- ä¸è¿‡ï¼Œæœ‰ä¸€äº›æ¯”è¾ƒä¸å¸¸è§çš„æƒ…æ™¯åªèƒ½ä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•æ¥å®ç°ã€‚

## Lab: CRUSH map è‡ªå®šä¹‰å±‚æ¬¡ç»“æ„ä¸è§„åˆ™

- ğŸš€ ç¤ºä¾‹ 1ï¼ˆå¯¹åº”å‰æ–‡æ–¹å¼ 1ï¼‰ï¼š
  
  ä½¿ç”¨ `ceph osd crush` å‘½ä»¤è®¾ç½®æŒ‡å®š osd çš„è®¾å¤‡ç±»å‹ï¼Œå¹¶åˆ›å»º CRUSH è§„åˆ™ï¼Œå› æ­¤ï¼Œåˆ›å»ºæ± æ—¶æŒ‡å®šè¯¥ CRUSH è§„åˆ™ï¼Œå¯é€šè¿‡è¯¥æ–¹æ³•å°†æ± ä¸­çš„æ‰€æœ‰å¯¹è±¡å‰¯æœ¬éƒ½å­˜å‚¨è‡³æŒ‡å®šè®¾å¤‡ç±»å‹çš„ osd ç£ç›˜ä¸Šã€‚è¯¥ç¤ºä¾‹ä¸­ `pg` å…¨éƒ¨æ˜ å°„è‡³åç«¯ä¸º 3 ä¸ª `ssd` è®¾å¤‡ç±»å‹çš„ osd ä¸Šï¼ŒåŒ…æ‹¬ `primary osd` ä¸å…¶ä½™ä¸¤ä¸ªå‰¯æœ¬ osdã€‚
  
  ```bash
  $ ceph osd crush rm-device-class 4
    done removing class of osd(s): 4
  $ ceph osd crush rm-device-class 6
    done removing class of osd(s): 6
  $ ceph osd crush rm-device-class 8
    done removing class of osd(s): 8
  # ç§»é™¤æŒ‡å®š osd çš„è®¾å¤‡ç±»å‹
  # æ³¨æ„ï¼š
  #   è‹¥åœ¨è®¾ç½®æŒ‡å®š osd è®¾å¤‡ç±»å‹å‰æœªç§»é™¤å·²æœ‰çš„è®¾å¤‡ç±»å‹å°†è¿”å›å¦‚ä¸‹æŠ¥é”™ï¼š
  #   Error EBUSY: osd.0 has already bound to class 'hdd', can not reset class to 
  #   'nvme'; use 'ceph osd crush rm-device-class <osd>' to remove old class first 
  
  $ ceph osd crush set-device-class ssd 4
    set osd(s) 4 to class 'ssd'
  $ ceph osd crush set-device-class ssd 6
    set osd(s) 6 to class 'ssd'
  $ ceph osd crush set-device-class ssd 8
    set osd(s) 8 to class 'ssd'
  # è®¾ç½® ssd è®¾å¤‡ç±»å‹çš„ osd
  $ ceph osd crush class ls
    [
        "hdd",
        "ssd"
    ]
  # æŸ¥çœ‹å½“å‰ CRUSH map ä¸­å·²è®¾ç½®çš„ osd è®¾å¤‡ç±»å‹
  
  $ ceph osd crush tree
      ID CLASS WEIGHT  TYPE NAME        
    -1       0.16498 root default     
    -3       0.05499     host serverc 
     0   hdd 0.01799         osd.0    
     3   hdd 0.01799         osd.3    
     4   ssd 0.01799         osd.4    
    -5       0.05499     host serverd 
     1   hdd 0.01799         osd.1    
     5   hdd 0.01799         osd.5    
     6   ssd 0.01799         osd.6    
    -7       0.05499     host servere 
     2   hdd 0.01799         osd.2    
     7   hdd 0.01799         osd.7    
     8   ssd 0.01799         osd.8
  
  $ ceph osd crush rule create-replicated onssd default host ssd
  # åˆ›å»ºåä¸º onssd çš„ CRUSH è§„åˆ™ï¼Œå¹¶æŒ‡å®š default æ ¹ bucketã€host æ•…éšœåŸŸä¸ ssd
  # è®¾å¤‡ç±»å‹çš„ osd
  # æ³¨æ„ï¼š
  #   $ ceph osd crush rule create create-replicated <name> <root> <type> {<class>}
  #   æŒ‡å®š CRUSH è§„åˆ™çš„åç§°ã€æ ¹ bucketï¼ˆå¯ä»¥æ˜¯é defaultï¼‰ã€æ•…éšœåŸŸç±»å‹ä¸ osd è®¾å¤‡ç±»å‹
  #   åˆ›å»º CRUSH è§„åˆ™
  
  $ ceph osd pool create myfast 32 32 onssd
  # ä½¿ç”¨ onssd çš„ CRUSH è§„åˆ™åˆ›å»ºåä¸º myfast çš„æ± 
  # æ³¨æ„ï¼š
  #   è‹¥åªå­˜åœ¨ 1 ä¸ª ssd è®¾å¤‡ç±»å‹çš„ osdï¼Œå†ä½¿ç”¨ onssd çš„ CRUSH è§„åˆ™åˆ›å»ºæ± åï¼Œç”±äº CRUSH 
  #   è§„åˆ™ä¸­çš„ firstn ä¸º 0ï¼Œå³å°†ä» 3 ä¸ª host bucket ä¸­é€‰å– 1 ä¸ª ssd è®¾å¤‡ç±»å‹çš„ osd è¿›è¡Œ
  #   pg è‡³ osd çš„æ˜ å°„ï¼Œç„¶è€Œæ­¤æ—¶ osd ä¸­ çš„å‰¯æœ¬æ•°ä¸º 1 ä¸ªå°äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•° 3 ä¸ªï¼Œpg çŠ¶æ€
  #   å°†ä¸º active+undersized+degradedã€‚

  $ ceph osd pool ls detail | grep myfast
    pool 23 'myfast' replicated size 3 min_size 1 crush_rule 4 object_hash rjenkins 
    pg_num 32 pgp_num 32 last_change 458 flags hashpspool stripe_width 0
  
  $ ceph pg dump pgs_brief | grep ^23
    dumped pgs_brief
    23.6             active+clean [6,8,4]          6 [6,8,4]              6 
    23.7             active+clean [8,4,6]          8 [8,4,6]              8 
    23.1             active+clean [4,6,8]          4 [4,6,8]              4 
    23.2             active+clean [8,4,6]          8 [8,4,6]              8 
    23.3             active+clean [8,4,6]          8 [8,4,6]              8 
    23.1c            active+clean [8,6,4]          8 [8,6,4]              8 
    23.1d            active+clean [4,8,6]          4 [4,8,6]              4 
    23.1e            active+clean [4,6,8]          4 [4,6,8]              4 
    ...
  # æ± ä¸­çš„ pg éƒ½æ˜ å°„è‡³ ssd è®¾å¤‡ç±»å‹çš„ osd 4,6,8 ä¸Š
  # è¯¥ç¤ºä¾‹çš„ CRUSH è§„åˆ™ç”Ÿæ•ˆ 
  ```

- âœ¨ ç¤ºä¾‹ 2ï¼ˆå¯¹åº”å‰æ–‡æ–¹å¼ 2ï¼‰ï¼š
  
  å¯¹åŸå§‹ Ceph é›†ç¾¤åˆ›å»ºè‡ªå®šä¹‰ CRUSH map å±‚æ¬¡ç»“æ„ä¸è‡ªå®šä¹‰ CRUSH è§„åˆ™ï¼Œè¯¥è§„åˆ™ä¼šå°†ä½äºåŸå§‹ osd ä¸­çš„ pg æ ¹æ®è‡ªå®šä¹‰ CRUSH è§„åˆ™é‡æ–°æ˜ å°„è‡³æ›´æ”¹çš„ osd ä¸­ï¼Œå…¶ä¸­å‰¯æœ¬æ‰€åœ¨ `primary osd` çš„åç«¯ç£ç›˜ç±»å‹ä¸º `ssd`ï¼Œè€Œå…¶ä½™å‰¯æœ¬çš„åç«¯ç£ç›˜ç±»å‹ä¸º `hdd`ã€‚
  
  ```bash
  $ ceph osd crush rm-device-class 2
  $ ceph osd crush rm-device-class 4
  $ ceph osd crush rm-device-class 8
  
  $ ceph osd crush set-device-class ssd 2
  $ ceph osd crush set-device-class ssd 4
  $ ceph osd crush set-device-class ssd 8
  # è®¾ç½® ssd è®¾å¤‡ç±»å‹çš„ osd
  
  $ ceph osd crush add-bucket East-DC1 root
  $ ceph osd crush add-bucket rackA1 rack
  $ ceph osd crush add-bucket hostc host
  $ ceph osd crush add-bucket rackB1 rack
  $ ceph osd crush add-bucket hostd host
  $ ceph osd crush add-bucket rackC1 rack
  $ ceph osd crush add-bucket hoste host
  # åˆ›å»ºè‡ªå®šä¹‰ root bucketã€rack bucket ä¸ host bucket
  
  $ ceph osd crush move rackA1 root=East-DC1
  $ ceph osd crush move hostc rack=rackA1
  $ ceph osd crush move rackB1 root=East-DC1
  $ ceph osd crush move hostd rack=rackB1
  $ ceph osd crush move rackC1 root=East-DC1
  $ ceph osd crush move hoste rack=rackC1
  # å°† rack bucket æ•´åˆè‡³ root bucket ä¸‹ï¼Œå¹¶å°† host bucket æ•´åˆè‡³ rack bucket ä¸‹ã€‚
  
  $ ceph osd crush tree
    ID  CLASS WEIGHT  TYPE NAME          
     -9             0 root East-DC1      
    -10             0     rack rackA1    
    -13             0         host hostc 
    -11             0     rack rackB1    
    -14             0         host hostd 
    -12             0     rack rackC1    
    -15             0         host hoste 
     -1       0.16644 root default       
     -3       0.05548     host serverc   
      0   hdd 0.01849         osd.0      
      1   hdd 0.01849         osd.1      
      2   ssd 0.01849         osd.2      
     -5       0.05548     host serverd   
      3   hdd 0.01849         osd.3      
      5   hdd 0.01849         osd.5      
      4   ssd 0.01849         osd.4      
     -7       0.05548     host servere   
      6   hdd 0.01849         osd.6      
      7   hdd 0.01849         osd.7      
      8   ssd 0.01849         osd.8 
  # æŸ¥çœ‹è°ƒæ•´åçš„ CRUSH map å±‚æ¬¡ç»“æ„ï¼Œæ­¤æ—¶è¿˜æœªå°† osd è®¾ç½®äºå„ä¸ª host bucket ä¸­ã€‚
  ```
  
  æ›´æ”¹ CRUSH map å±‚æ¬¡ç»“æ„åï¼Œéœ€å°† osd è®¾ç½®äºæŒ‡å®šçš„ host bucket ä¸­ã€‚
  
  è¯¥ç¤ºä¾‹ä¸­ä½¿ç”¨ç›´æ¥ç¼–è¾‘ `/etc/ceph/ceph.conf` é…ç½®æ–‡ä»¶çš„æ–¹å¼ä»¥å®šä¹‰ osd åœ¨å„ä¸ª host bucket ä¸­çš„ä½ç½®ï¼ˆè§åæ–‡ "è®¾ç½® OSD ä½ç½®"ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ```bash
  $ sudo vim /etc/ceph/ceph.conf
    ...
    ### customized crush osd location
    [osd.0]
    crush_location= root=East-DC1 rack=rackA1 host=hostc
  
    [osd.1]
    crush_location= root=East-DC1 rack=rackA1 host=hostc
  
    [osd.2]
    crush_location= root=East-DC1 rack=rackA1 host=hostc
  
    [osd.3]
    crush_location= root=East-DC1 rack=rackB1 host=hostd
  
    [osd.4]
    crush_location= root=East-DC1 rack=rackB1 host=hostd
  
    [osd.5]
    crush_location= root=East-DC1 rack=rackB1 host=hostd
  
    [osd.6]
    crush_location= root=East-DC1 rack=rackC1 host=hoste
  
    [osd.7]
    crush_location= root=East-DC1 rack=rackC1 host=hoste
  
    [osd.8]
    crush_location= root=East-DC1 rack=rackC1 host=hoste
  # åœ¨è¯¥é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸Šé…ç½®ä»¥è®¾ç½® osd è‡³ host bucket ä¸­çš„ä½ç½®
  ```
  
  ä»¥ä¸Š /etc/ceph/ceph.conf é…ç½®æ–‡ä»¶å¯å‚è€ƒ [è¯¥ GitHub é“¾æ¥](https://github.com/Alberthua-Perl/sc-col/blob/master/redhat-ceph-conf/v3.0/ceph.conf)ã€‚
  
  é€šè¿‡ä¸Šè¿°æ–¹æ³•å°†åŸå§‹ Ceph é›†ç¾¤çš„ CRUSH map å±‚æ¬¡ç»“æ„è¿›è¡Œé‡æ„ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä½¿ osd è®¾ç½®äºæ­£ç¡®çš„ host bucket ä¸­ï¼š
  
  ```bash
  ### root@serverc:
  $ scp /etc/ceph/ceph.conf root@serverd:/etc/ceph/ceph.conf
  $ scp /etc/ceph/ceph.conf root@servere:/etc/ceph/ceph.conf
  # å°† Ceph é›†ç¾¤é…ç½®æ–‡ä»¶åŒæ­¥è‡³å…¶ä½™ osd èŠ‚ç‚¹
  
  $ sudo systemctl restart ceph-osd.target
  $ ssh root@serverd systemctl restart ceph-osd.target
  $ ssh root@servere systemctl restart ceph-osd.target
  # é‡å¯æ‰€æœ‰èŠ‚ç‚¹çš„ ceph-osd æœåŠ¡ï¼Œä½¿é…ç½®ç”Ÿæ•ˆã€‚ 
  
  $ ceph osd crush tree
    ID  CLASS WEIGHT  TYPE NAME          
     -9       0.16498 root East-DC1      
    -10       0.05499     rack rackA1    
    -13             0         host hostc 
      0   hdd 0.01799         osd.0      
      1   hdd 0.01799         osd.1      
      2   ssd 0.01799         osd.2      
    -11       0.05499     rack rackB1    
    -14             0         host hostd 
      3   hdd 0.01799         osd.3      
      5   hdd 0.01799         osd.5      
      4   ssd 0.01799         osd.4      
    -12       0.05499     rack rackC1    
    -15             0         host hoste 
      6   hdd 0.01799         osd.6      
      7   hdd 0.01799         osd.7      
      8   ssd 0.01799         osd.8      
     -1             0 root default       
     -3             0     host serverc   
     -5             0     host serverd   
     -7             0     host servere
  # é‡æ„çš„ CRUSH map å±‚æ¬¡ç»“æ„
  ```
  
  é…ç½®å®Œ osd ä½ç½®åï¼Œéœ€åˆ›å»ºè‡ªå®šä¹‰ CRUSH è§„åˆ™ä»¥æ»¡è¶³å¯¹è±¡å­˜å‚¨çš„è¦æ±‚ï¼š
  
  ```bash
  $ ceph osd getcrushmap -o ./cm-org.bin
  $ crushtool -d ./cm-org.bin -o ./cm-org.txt
  # å¤‡ä»½åŸå§‹ Ceph é›†ç¾¤çš„ CRUSH map ä¸è§„åˆ™
  
  $ cp ./cm-org.txt ./cm-on-ssd-distributed.txt
  $ vim ./cm-on-ssd-distributed.txt
    ...
    # rules
    rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take East-DC1
        # æŒ‡å®šä»æ ¹ East-DC1 bucket å¼€å§‹é€‰å– osd
        step chooseleaf firstn 0 type host
        # ä»ä¸æ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ç›¸åŒæ•°é‡çš„ host bucket ä¸­éšæœºé€‰å– 1 ä¸ª osd è¿›è¡Œ pg çš„æ˜ å°„
        # è¯¥ç¤ºä¾‹ä¸­æ± çš„å‰¯æœ¬æ•°é»˜è®¤ä¸º 2ï¼Œä¹Ÿå°±éœ€è¦ 2 ä¸ª osdï¼Œå› æ­¤é»˜è®¤å°†ä» 2 ä¸ª host bucket ä¸­
        # éšæœºé€‰å– 1 ä¸ª osd è¿›è¡Œæ˜ å°„ã€‚
        step emit
    }
  # å¤åˆ¶æ± é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨çš„ CRUSH è§„åˆ™
  
    rule on-ssd-distributed {
        id 1
        type replicated
        min_size 1
        max_size 10
        step take East-DC1 class ssd
        # æŒ‡å®šä»æ ¹ East-DC1 bucket å¼€å§‹é€‰å– ssd è®¾å¤‡ç±»å‹çš„ osd
        step chooseleaf firstn 1 type rack
        # ä» 1 ä¸ª rack bucket ä¸­éšæœºé€‰å– ssd è®¾å¤‡ç±»å‹çš„ 1 ä¸ª osd è¿›è¡Œ pg çš„æ˜ å°„
        # æ­¤æ—¶åªæœ‰ 1 ä¸ª pg æ˜ å°„è‡³ ssd è®¾å¤‡ç±»å‹çš„ osd ä¸Š
        # è¯¥ç¤ºä¾‹ä¸­æ± çš„å‰¯æœ¬æ•°é»˜è®¤ä¸º 2ï¼Œpg æ•°é‡å°äºå‰¯æœ¬æ•°ï¼Œå› æ­¤éœ€å†å®šä¹‰é€‰å–è§„åˆ™ã€‚
        step emit
        step take East-DC1 class hdd
        # æŒ‡å®šä»æ ¹ East-DC1 bucket å¼€å§‹é€‰å– hdd è®¾å¤‡ç±»å‹çš„ osd 
        step chooseleaf firstn -1 type rack
        # ç”±äºæ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°é»˜è®¤ä¸º 2ï¼Œæ ¹æ®å‰é¢çš„å®šä¹‰ï¼Œè¿˜å‰©ä½™ 1 ä¸ªå‰¯æœ¬éœ€å­˜å‚¨äº osd ä¸­ï¼Œ
        # å› æ­¤å°† firstn è®¾ç½®ä¸º -1ï¼Œå³ 2-|-1| ä¸º 1ï¼Œæœ€ç»ˆå°†å‰©ä½™çš„ 1 ä¸ª pg æ˜ å°„è‡³ hdd 
        # è®¾å¤‡ç±»å‹çš„ osd ä¸Šï¼Œå®Œæˆ 2 ä¸ª pg çš„ osd æ˜ å°„ï¼Œæ»¡è¶³æ± ä¸­å®šä¹‰çš„å‰¯æœ¬æ•°ã€‚      
        step emit
    }
    ...
  # æ ¹æ®åŸå§‹ CRUSH map ä¸è§„åˆ™æ·»åŠ å¹¶æ›´æ–°è‡ªå®šä¹‰çš„è§„åˆ™
  ```
  
  ä»¥ä¸Š CRUSH map ä¸è§„åˆ™çš„çº¯æ–‡æœ¬æ–‡ä»¶å¯å‚è€ƒ [è¯¥ GitHub é“¾æ¥](https://github.com/Alberthua-Perl/sc-col/blob/master/redhat-ceph-conf/v3.0/cm-on-ssd-distributed.txt)ã€‚
  
  ```bash
  $ crushtool -c ./cm-on-ssd-distributed.txt -o ./cm-on-ssd-distributed.bin
  # ç¼–è¯‘è‡ªå®šä¹‰çš„ CRUSH map ä¸è§„åˆ™ä¸ºäºŒè¿›åˆ¶æ ¼å¼
  $ ceph osd setcrushmap -i ./cm-on-ssd-distributed.bin
  # å°†ç¼–è¯‘çš„äºŒè¿›åˆ¶æ ¼å¼ CRUSH map ä¸è§„åˆ™å¯¼å…¥è‡³ Ceph é›†ç¾¤ä¸­
  # å¯¼å…¥åå°†è¿”å› CRUSH map çš„ epoch ç‰ˆæœ¬å·

  $ ceph -s
    cluster:
      id:     d696618e-20ee-4169-a1da-edc7a33857be
      health: HEALTH_OK
  
    services:
      mon: 3 daemons, quorum serverc,serverd,servere
      mgr: serverc(active), standbys: servere, serverd
      mds: cephfs-1/1/1 up  {0=servera=up:active}
      osd: 9 osds: 9 up, 9 in
      rgw: 1 daemon active
  
    data:
      pools:   6 pools, 48 pgs
      objects: 240 objects, 3359 bytes
      usage:   1000 MB used, 169 GB / 170 GB avail
      pgs:     48 active+clean 
  
  $ ceph osd pool create myfast 8 8 on-ssd-distributed
  # ä½¿ç”¨è‡ªå®šä¹‰çš„ CRUSH è§„åˆ™åˆ›å»ºæ± 
  $ ceph osd pool ls detail | grep myfast
    pool 9 'myfast' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 
    last_change 192 flags hashpspool stripe_width
  
  $ ceph pg dump pgs_brief | grep ^9
  dumped pgs_brief
  9.7     active+clean [8,7]          8  [8,7]              8 
  9.6     active+clean [4,5]          4  [4,5]              4 
  9.5     active+clean [4,5]          4  [4,5]              4 
  9.4     active+clean [8,1]          8  [8,1]              8 
  9.3     active+clean [8,7]          8  [8,7]              8 
  9.2     active+clean [2,1]          2  [2,1]              2 
  9.1     active+clean [8,7]          8  [8,7]              8 
  9.0     active+clean [2,1]          2  [2,1]              2 
  # åˆ›å»ºåæ± ä¸­çš„æ‰€æœ‰ pg çŠ¶æ€å‡ä¸º active+cleanï¼Œå³å¯å¯¹å¤–æä¾›è¯»å†™ã€‚
  ```
  
  ğŸ’¥ è‹¥æœªæ ¹æ® CRUSH map å±‚æ¬¡ç»“æ„æ­£ç¡®é…ç½® CRUSH è§„åˆ™ï¼Œåœ¨å¯¼å…¥ç¼–è¯‘çš„äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶åï¼Œé€šè¿‡ `ceph -w` å‘½ä»¤å°†æŸ¥çœ‹åˆ°ä»¥ä¸‹ç±»ä¼¼çš„æŠ¥é”™ï¼š
  
  ```bash
  $ ceph -w
    cluster:
      id:     d696618e-20ee-4169-a1da-edc7a33857be
      health: HEALTH_WARN
              36/480 objects misplaced (7.500%)
              Reduced data availability: 16 pgs inactive, 22 pgs peering, 22 pgs stale
              Degraded data redundancy: 72/480 objects degraded (15.000%), 47 pgs unclean, 25 pgs degraded, 23 pgs undersized
  
    services:
      mon: 3 daemons, quorum serverc,serverd,servere
      mgr: serverc(active), standbys: servere, serverd
      mds: cephfs-1/1/1 up  {0=servera=up:active}
      osd: 9 osds: 9 up, 9 in; 2 remapped pgs
      rgw: 1 daemon active
  
    data:
      pools:   6 pools, 48 pgs
      objects: 240 objects, 3359 bytes
      usage:   997 MB used, 169 GB / 170 GB avail
      pgs:     45.833% pgs not active
               72/480 objects degraded (15.000%)
               36/480 objects misplaced (7.500%)
               19 peering
               17 stale+active+undersized+degraded
               6  active+undersized+degraded+remapped
               2  stale+remapped+peering
               1  stale+active+recovery_wait+degraded+remapped
               1  stale+active+recovering+degraded+remapped
               1  stale+peering
               1  active+clean+remapped
  
    io:
      recovery: 0 B/s, 2 objects/s
  ```

- ç¤ºä¾‹ 3ï¼š
  
  ä»¥ä¸Šç¤ºä¾‹ 1 ä¸ç¤ºä¾‹ 2 ä¸­ä»¥åˆ›å»ºå¤åˆ¶æ± ä¸ºä¾‹è¯´æ˜ `replicated` ç±»å‹çš„ CRUSH è§„åˆ™ï¼Œè€Œçº åˆ ä»£ç æ± ä¸ä½¿ç”¨è¯¥ç±»å‹çš„è§„åˆ™ï¼Œå¯ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•åˆ›å»º `erasure-code-profile` ä¸çº åˆ ä»£ç æ± ï¼š
  
  ```bash
  $ ceph osd erasure-code-profile set myprofile k=2 m=1 \
    crush-root=East-DC1 \
    crush-failure-domain=rack \
    crush-device-class=ssd
  # åˆ›å»ºåä¸º myprofile çš„ erasure-code-profile
  # è¯¥ profile å®šä¹‰ 2 ä¸ªæ•°æ®åŒºå—ä¸ 1 ä¸ªç¼–ç åŒºå—ï¼Œå°†ä»æ ¹ East-DC1 bucket ä¸­å¼€å§‹é€‰å– osdï¼Œ
  # åœ¨ 3 ä¸ª rack ç±»å‹çš„æ•…éšœåŸŸä¸­åˆ†åˆ«éšæœºé€‰å– 1 ä¸ª ssd è®¾å¤‡ç±»å‹çš„ osd è¿›è¡Œ pg è‡³ osd çš„
  # æ˜ å°„ï¼ˆå…± 3 ä¸ª osdï¼‰ã€‚
  
  $ ceph osd pool create myecpool 50 50 erasure myprofile
  # ä½¿ç”¨ä¸Šè¿° profile åˆ›å»ºåä¸º myecpool çš„çº åˆ ä»£ç æ±  
  ```

## è®¾ç½® OSD çš„ä½ç½®

- åœ¨åˆ›å»ºäº†è‡ªå®šä¹‰ CRUSH map å±‚æ¬¡ç»“æ„åï¼Œç”±äºæ¯ä¸€ OSD å…·æœ‰ä¸€ä¸ªä½ç½®ï¼Œå¯ä»¥å°† OSD æ”¾ç½®äº host bucket ä¸Šä½œä¸ºæ ‘ä¸Šçš„æ ‘å¶ã€‚
- å¦‚ä¸‹æ‰€ç¤ºï¼šé™„åŠ è‡³ rackA1 bucket çš„ OSD çš„ä½ç½®
  
  ```bash
  root=default datacenter=DC1 rack=rackA1
  ```

- è®¾ç½® OSD ä½ç½®çš„ 2 ç§æ–¹å¼ï¼š
  - æ–¹å¼ 1ï¼šè‡ªå®šä¹‰ `Shell` è„šæœ¬
    - å½“ Ceph å¯åŠ¨æ—¶ï¼Œå®ƒä½¿ç”¨ `ceph-crush-location` è„šæœ¬è‡ªåŠ¨éªŒè¯æ¯ä¸ª OSD åœ¨ä¸åœ¨æ­£ç¡®çš„ CRUSH ä½ç½®ä¸Šã€‚

      ```bash
      $ which ceph-crush-location
        /usr/bin/ceph-crush-location
      $ file /usr/bin/ceph-crush-location
        /usr/bin/ceph-crush-location: POSIX shell script, ASCII text executable
      $ rpm -qf /usr/bin/ceph-crush-location
        ceph-common-12.2.1-40.el7cp.x86_64
      ```

    - è‹¥ OSD ä¸åœ¨ CRUSH map ä¸­çš„é¢„æœŸä½ç½®ä¸Šï¼Œå®ƒä¼šè¢«è‡ªåŠ¨ç§»åŠ¨ã€‚
    - é»˜è®¤æƒ…å†µï¼š`root=default host=<hostname>`
    - å¯å°† ceph-crush-location è„šæœ¬æ›¿æ¢ä¸ºè‡ªå·±çš„è„šæœ¬ï¼Œä»¥æ›´æ”¹ OSD åœ¨ CRUSH map ä¸­ çš„æ”¾ç½®ä½ç½®ã€‚
    - ğŸ‘‰ å¯æŒ‡å®š `/etc/ceph/ceph.conf` é…ç½®æ–‡ä»¶ä¸­çš„ `crush_location_hook` å‚æ•°ã€‚

      ```ini
      [osd]
      crush_location_hook = /path/to/your/script
      ```

    - Ceph ä¼šä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨è„šæœ¬ï¼š`--cluster $cluster --id osd.$id --type osd`
    - è„šæœ¬å¿…é¡»åœ¨æ ‡å‡†è¾“å‡ºä¸­ä»¥å•è¡Œå½¢å¼æ‰“å°ä½ç½®ã€‚
    - Ceph æ–‡æ¡£æœ‰ä¸€ä¸ªè‡ªå®šä¹‰è„šæœ¬çš„ç®€å•ç¤ºä¾‹ï¼Œå…¶å‡å®šæ¯ä¸ªç³»ç»Ÿå…·æœ‰ `/etc/rack` æ–‡ä»¶ï¼Œå…¶ä¸­å«æœ‰å…¶æœºæ¶çš„åç§°ï¼š

      ```bash
      #!/bin/sh
      echo "root=default rack=$(cat /etc/rack) host=$(hostname -s)"
      ```
  
  - æ–¹å¼ 2ï¼šè®¾ç½® `crush_location` å‚æ•°
    - å¯ä»¥åœ¨ /etc/ceph/ceph.conf é…ç½®æ–‡ä»¶ä¸­è®¾ç½® crush_location å‚æ•°ï¼Œä»¥é‡æ–°å®šä¹‰ç‰¹å®š OSD çš„ä½ç½®ï¼ˆå‰æ–‡ç¤ºä¾‹ 2 ä¸­ä½¿ç”¨çš„æ–¹æ³•ï¼‰ã€‚
    - å¦‚ï¼Œè®¾ç½® osd.0Â ä¸Â osd.1 çš„ä½ç½®ï¼Œå¯ä»¥å°† crush_location å‚æ•°æ·»åŠ åˆ°å®ƒä»¬åœ¨è¯¥æ–‡ä»¶ä¸­çš„å¯¹åº”éƒ¨åˆ†ä¸­ï¼š

      ```ini
      [osd.0]
      crush_location = root=default datacenter=DC1 rack=rackA1 
      
      [osd.1]
      crush_location = root=default datacenter=DC1 rack=rackB1
      ```

## crushtool å‘½ä»¤ä½¿ç”¨ç¤ºæ„

- å¯ä»¥ä½¿ç”¨ `crushtool` å‘½ä»¤æ‰‹åŠ¨ç¼–è¾‘ä¸ç¼–è¯‘ CRUSH map ä¸è§„åˆ™çš„å‘½ä»¤ã€‚
- ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š
  
  ```bash
  $ ceph osd getcrushmap -o <binfile>
  # å°†å½“å‰çš„ CRUSH map ä¸è§„åˆ™å¯¼å‡ºè‡³æŒ‡å®šçš„äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶ä¸­
  
  $ crushtool -d <binfile> -o <textfile>
  # å°† CRUSH map ä¸è§„åˆ™çš„äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶åç¼–è¯‘ä¸ºçº¯æ–‡æœ¬æ ¼å¼æ–‡ä»¶
  
  $ crushtool -c <textfile> -o <binfile>
  # å°†ç¼–è¾‘å¥½çš„ CRUSH map ä¸è§„åˆ™çš„çº¯æ–‡æœ¬æ ¼å¼æ–‡ä»¶ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶
  
  $ ceph osd setcrushmap -i <binfile>
  # å°†ç¼–è¯‘å¥½çš„ CRUSH map ä¸è§„åˆ™çš„äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶å¯¼å…¥ Ceph é›†ç¾¤ä¸­ä»¥æ›´æ–° CRUSH map ä¸è§„åˆ™
  # æ³¨æ„ï¼šä»¥ä¸Šå‘½ä»¤å¯åœ¨å‰æ–‡ç¤ºä¾‹ä¸­æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•
  
  $ crushtool -i <binfile> --test --show-mappings \
    --rule=<rule_id> --num-rep <replica_num>
  # ä½¿ç”¨ç¼–è¯‘å¥½çš„ CRUSH map ä¸è§„åˆ™çš„äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶æ ¹æ® CRUSH è§„åˆ™çš„ id åŠå®šä¹‰çš„å‰¯æœ¬æ•°
  # è¿›è¡Œæ˜ å°„æµ‹è¯•ï¼ˆä¸è¿›è¡ŒçœŸæ­£çš„ pg æ˜ å°„ï¼‰ 
  ```
